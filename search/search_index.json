{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#nerc-technical-documentation","title":"NERC Technical Documentation","text":"<p>NERC welcomes your contributions</p> <p>These pages are hosted from a git repository and contributions are welcome!</p> <p>Fork this repo</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-nerc","title":"About NERC","text":"<p>We are currently in the pilot phase of the project and are focusing on developing the technology to make it easy for researchers to take advantage of a suite of services (IaaS, PaaS, SaaS) that are not readily available today. This includes:</p> <ol> <li> <p>The creation of the building blocks needed for production cloud services</p> </li> <li> <p>Begin collaboration with Systems Engineers from other institutions with well    established RC groups</p> </li> <li> <p>On-board select proof of concept use cases from institutions within the    MGHPCC consortium and other institutions    within Massachusetts</p> </li> </ol> <p>The longer term objectives will be centered around activities that will focus on:</p> <ol> <li> <p>Engaging with various OpenStack communities by sharing best practices and    setting standards for deployments</p> </li> <li> <p>Connecting regularly with the Mass Open Cloud    (MOC) leadership to understand when new technologies they are developing with    RedHat, Inc. \u2013 and as part of the new NSF funded Open Cloud Testbed \u2013 might be ready for    adoption into the production NERC environment</p> </li> <li> <p>Broadening the local deployment team of NERC to include partner universities    within the MGHPCC consortium.</p> </li> </ol> <p> Figure 1: NERC Overview</p> <p>NERC production services (red) stand on top of the existing NESE storage services (blue) that are built on the strong foundation of MGHPCC (green) that provides core facility and network access. The Innovation Hub (grey) enables new technologies to be rapidly adopted by the NERC or NESE services. On the far left (purple) are the Research and Learning communities which are the primary customers of NERC. As users proceed down the stack of production services from Web-apps, that require more technical skills, the Cloud Facilitators (orange) in the middle guide and educate users on how to best use the services.</p> <p>For more information, view NERC's concept document.</p>"},{"location":"get-started/","title":"Getting Started","text":""},{"location":"get-started/#nerc-getting-started-index","title":"NERC Getting Started Index","text":"<p>If you're just starting out, we recommend starting from User Onboarding Process Overview and going through the tutorial in order.</p> <p>If you just need to review a specific step, you can find the page you need in the list below.</p>"},{"location":"get-started/#cost-billing","title":"Cost &amp; Billing","text":"<ul> <li> <p>How does NERC pricing work? &lt;&lt;-- Start Here</p> </li> <li> <p>Pricing for Bare Metal Machines on NERC</p> </li> <li> <p>NERC Pricing Calculator</p> </li> </ul>"},{"location":"get-started/#billing-process","title":"Billing Process","text":"<ul> <li> <p>Billing Process for My Institution</p> </li> <li> <p>Billing Process for Harvard University</p> </li> <li> <p>Billing Process for Boston University</p> </li> <li> <p>Billing FAQs</p> </li> </ul>"},{"location":"get-started/#user-onboarding-steps","title":"User Onboarding Steps","text":"<ul> <li>User Onboarding Process</li> </ul>"},{"location":"get-started/#user-registration-using-mghpcc-shared-services-account-portal-aka-regapp","title":"User Registration using MGHPCC Shared Services Account Portal a.k.a. RegApp","text":"<ul> <li>How to Create a User Account</li> </ul>"},{"location":"get-started/#how-to-get-resource-allocations-using-coldfront","title":"How to Get Resource Allocations using ColdFront?","text":"<ul> <li> <p>What is NERC's ColdFront?</p> </li> <li> <p>A New Project Creation Process</p> </li> <li> <p>Managing Users in the Project</p> </li> <li> <p>How to request a new Resource Allocation</p> </li> <li> <p>Request change to Resource Allocation to an existing project</p> </li> <li> <p>Adding a new Resource Allocation to the project</p> </li> <li> <p>Managing Users in the Project Resource Allocation</p> </li> <li> <p>Project and Individual Allocation Annual Review Process</p> </li> <li> <p>Archiving an Existing Project</p> </li> <li> <p>Allocation details</p> </li> </ul>"},{"location":"get-started/#security-best-practices-for-the-nerc-users","title":"Security Best Practices for the NERC Users","text":"<ul> <li> <p>Quick Guide and Best Practices</p> </li> <li> <p>Best Practices for My Institution</p> </li> <li> <p>Best Practices for Harvard University</p> </li> <li> <p>Best Practices for Boston University</p> </li> </ul>"},{"location":"get-started/create-a-user-portal-account/","title":"How to Create a User Account","text":""},{"location":"get-started/create-a-user-portal-account/#user-account-types","title":"User Account Types","text":"<p>NERC offers two types of user accounts: a Principal Investigator (PI) Account and a General User Account. All General Users must be assigned to their project by an active NERC PI or by one of the delegated project manager(s), as described here. Then, those project users can be added to the resource allocation during a new allocation request or at a later time.</p> <p>Principal Investigator Eligibility Information</p> <ul> <li> <p>MGHPCC consortium members, whereby they enter into an service agreement with MGHPCC for the NERC services.</p> </li> <li> <p>Non-members of MGHPCC can also be PIs of NERC Services, but must also have an active non-member agreement with MGHPCC.</p> </li> <li> <p>External research focused institutions will be considered on a case-by-case basis and are subject to an external customer cost structure.</p> </li> </ul> <p>A PI account can request allocations of NERC resources, grant access to other general users enabling them to log into NERC's computational project space, and delegate its responsibilities to other collaborators from the same institutions or elsewhere as managers using NERC's ColdFront interface, as described here.</p>"},{"location":"get-started/create-a-user-portal-account/#getting-started","title":"Getting Started","text":"<p>Any faculty, staff, student, and external collaborator must request a user account through the MGHPCC Shared Services (MGHPCC-SS) Account Portal, also known as \"RegApp\". This is a web-based, single point-of-entry to the NERC system that displays a user welcome page. The welcome page of the account registration site displays instructions on how to register a General User account on NERC, as shown in the image below:</p> <p></p> <p>There are two options: either register for a new account or manage an existing one. If you are new to NERC and want to register as a new MGHPCC-SS user, click on the \"Register for an Account\" button. This will redirect you to a new web page which shows details about how to register for a new MGHPCC-SS user account. NERC uses CILogon that supports login either using your Institutional Identity Provider (IdP).</p> <p>Clicking the \"Begin MGHPCC-SS Account Creation Process\" button will initiate the account creation process. You will be redirected to a site managed by CILogon where you will select your institutional or commercial identity provider, as shown below:</p> <p></p> <p>If you are not able to see the CILogon landing page</p> <p>If the default CILogon login page does not appear, it may be because you are already logged in through another identity provider supported by CILogon. Since CILogon provides authentication and authorization using institutional credentials, this can sometimes cause session conflicts.</p> <p>To resolve this issue, you can reset your CILogon session by following these steps:</p> <ol> <li> <p>Visit the CILogon cookie management page: https://cilogon.org/me.</p> </li> <li> <p>Click the Delete ALL button to remove all CILogon related data from your current browser session.</p> </li> <li> <p>Once the CILogon data has been cleared, go back to the RegApp.</p> </li> <li> <p>Log in again using your institutional credentials.</p> </li> </ol> <p>If you continue to experience issues, try clearing your browser's cache and cookies, or use a different browser.</p> <p>Once selected, you will be redirected to your institutional or commercial identity provider, where you will log in, as shown here:</p> <p></p> <p>After a successful log on, your browser will be redirected back to the MGHPCC-SS Registration Page and ask for a review and confirmation of creating your account with fetched information to complete the account creation process.</p> <p></p> <p>Very Important</p> <p>If you don't click the \"Create MGHPCC-SS Account\" button, your account will not be created! So, this is a very important step. Review your information carefully and then click on the \"Create MGHPCC-SS Account\" button to save your information. Please review the information, make any corrections that you need and fill in any blank/ missing fields such as \"Research Domain\". Please read the End User Level Agreement (EULA) and accept the terms by checking the checkbox in this form.</p> <p>Once you have reviewed and verified that all your user information in this form is correct, only then click the \"Create MGHPCC-SS Account\" button. This will automatically send an email to your email address with a link to validate and confirm your account information.</p> <p></p> <p>Once you receive an \"MGHPCC-SS Account Creation Validation\" email, review your user account information to ensure it is correct. Then, click on the provided validation web link and enter the unique account creation Confirmation Code provided in the email as shown below:</p> <p></p> <p>Once validated, you need to ensure that your user account is created and valid by viewing the following page:</p> <p></p> <p>Important Note</p> <p>If you have an institutional identity, it's preferable to use that identity to create your MGHPCC-SS account. Institutional identities are vetted by identity management teams and provide a higher level of confidence to resource owners when granting access to resources. You can only link one university account to an MGHPCC-SS account; if you have multiple university accounts, you will only be able to link one of those accounts to your MGHPCC-SS account. If, at a later date, you want to change which account is connected to your MGHPCC-SS identity, you can do so by contacting help@mghpcc.org.</p>"},{"location":"get-started/create-a-user-portal-account/#how-to-update-and-modify-your-mghpcc-ss-account-information","title":"How to update and modify your MGHPCC-SS account information?","text":"<ol> <li> <p>Log in to the RegApp using your MGHPCC-SS account.</p> </li> <li> <p>Click on \"Manage Your MGHPCC-SS Account\" button, as shown below:</p> <p></p> </li> <li> <p>Review your currently saved account information, make any necessary corrections    or updates to fields, and then click on the \"Update MGHPCC-SS Account\" button.</p> </li> <li> <p>This will send an email to verify your updated account information, so please    check your email address.</p> </li> <li> <p>Confirm and validate the new account details by clicking the provided validation    web link and entering the unique Confirmation Code provided in the email    as shown below:</p> <p></p> </li> </ol>"},{"location":"get-started/create-a-user-portal-account/#how-to-request-a-principal-investigator-pi-account","title":"How to request a Principal Investigator (PI) Account?","text":"<p>The process for requesting and obtaining a PI Account is relatively simple. You can fill out this NERC Principal Investigator (PI) Account Request form to initiate the process.</p> <p>Alternatively, users can request a Principal Investigator (PI) user account by submitting a new ticket at the NERC's Support Ticketing System under the \"NERC PI Account Request\" option in the Help Topic dropdown menu, as shown in the image below:</p> <p></p> <p>Information</p> <p>Once your PI user request is reviewed and approved by the NERC's admin, you will receive an email confirmation from NERC's support system, i.e., help@nerc.mghpcc.org. Then, you can access NERC's ColdFront resource allocation management portal using the PI user role, as described here.</p>"},{"location":"get-started/user-onboarding-on-NERC/","title":"User Onboarding Process","text":""},{"location":"get-started/user-onboarding-on-NERC/#user-onboarding-process-overview","title":"User Onboarding Process Overview","text":"<p>NERC's Research allocations are available to faculty members and researchers, including postdoctoral researchers and students. In order to get access to resources provided by NERC's computational infrastructure, you must first register and obtain a user account.</p> <p>The overall user flow can be summarized using the following sequence diagram:</p> <p></p> <ol> <li> <p>All users including PI need to register to NERC via: https://regapp.mss.mghpcc.org/.</p> <p>For detailed instructions on creating a user account through RegApp, refer to this guide.</p> </li> <li> <p>PI will send a request for a Principal Investigator (PI) user account role     by submitting: NERC's PI Request Form.</p> <p>Alternatively, users can request a Principal Investigator (PI) user account by submitting a new ticket at the NERC's Support Ticketing System under the \"NERC PI Account Request\" option in the Help Topic dropdown menu, as shown in the image below:</p> <p></p> <p>Principal Investigator Eligibility Information</p> <ul> <li> <p>MGHPCC consortium members, whereby they enter into an service agreement with MGHPCC for the NERC services.</p> </li> <li> <p>Non-members of MGHPCC can also be PIs of NERC Services, but must also have an active non-member agreement with MGHPCC.</p> </li> <li> <p>External research focused institutions will be considered on a case-by-case basis and are subject to an external customer cost structure.</p> </li> </ul> </li> <li> <p>Wait until the PI request gets approved by the NERC's admin.</p> </li> <li> <p>Once a PI request is approved, PI can add a new project and also     search and add user(s) to the project - Other general user(s) can also     see the project(s) once they are added to a project via: https://coldfront.mss.mghpcc.org.</p> <p>For detailed instructions about NERC's ColdFront and how to use it, refer to this guide.</p> <p>Very Important: Project Title Length Limitation</p> <p>Please ensure that the project title is both concise and does not exceed a length of 45 characters.</p> </li> <li> <p>PI or project Manager can request resource allocation either NERC (OpenStack),     NERC-OCP (OpenShift) or NERC-OCP-EDU (OpenShift) for the newly added     project and select which user(s) can use the requested allocation.</p> <p>As a new NERC PI for the first time, am I entitled to any credits?</p> <p>As a new PI using NERC for the first time, you might wonder if you get any credits. Yes, you'll receive up to $1000 for the first month only. But remember, this credit can not be used in the following months. Also, it does not apply to GPU resource usage.</p> </li> <li> <p>Wait until the requested resource allocation gets approved by the NERC's admin.</p> </li> <li> <p>Once approved, PI and the corresponding project users can go to either     NERC Openstack horizon web interface: https://stack.nerc.mghpcc.org,     NERC OpenShift web console: https://console.apps.shift.nerc.mghpcc.org     or NERC Academic (EDU) OpenShift web console: https://console.apps.edu.nerc.mghpcc.org     based on your requested/approved Resource Type and they can start using     the NERC's resources based on the approved project quotas.</p> <p>For detailed guidance on using NERC (OpenStack), NERC-OCP (OpenShift), and NERC-OCP-EDU (OpenShift) please refer to the respective user guides below:</p> <ul> <li> <p>NERC (OpenStack)</p> </li> <li> <p>NERC OCP (OpenShift)</p> </li> <li> <p>NERC Academic (EDU) OpenShift</p> </li> </ul> </li> </ol>"},{"location":"get-started/allocation/adding-a-new-allocation/","title":"Adding a new Resource Allocation to the project","text":""},{"location":"get-started/allocation/adding-a-new-allocation/#adding-a-new-resource-allocation-to-the-project","title":"Adding a new Resource Allocation to the project","text":"<p>If one resource allocation is not sufficient for a project, PI or project managers may request additional allocations by clicking on the \"Request Resource Allocation\" button on the Allocations section of the project details. This will show the page where all existing users for the project will be listed on the bottom of the request form. PIs can select desired user(s) to make the requested resource allocations available on their NERC's OpenStack or OpenShift projects.</p> <p>Here, you can view the Resource Type, information about your Allocated Project, status, End Date of the allocation, and actions button or any pending actions as shown below:</p> <p></p>"},{"location":"get-started/allocation/adding-a-new-allocation/#adding-a-new-resource-allocation-to-your-openstack-project","title":"Adding a new Resource Allocation to your OpenStack project","text":"<p>Important: Requested/Approved Allocated OpenStack Storage Quota &amp; Cost</p> <p>Ensure you choose NERC (OpenStack) in the Resource option and specify your anticipated computing units. Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC (OpenStack) Resource Allocations, the Storage quotas are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers. Keep in mind that you can easily scale and expand your current resource allocations within your project by following this documentation later on.</p>"},{"location":"get-started/allocation/adding-a-new-allocation/#adding-a-new-resource-allocation-to-your-openshift-project","title":"Adding a new Resource Allocation to your OpenShift project","text":"<p>Important: Requested/Approved Allocated OpenShift Storage Quota &amp; Cost</p> <p>Ensure you choose NERC-OCP (OpenShift) in the Resource option (Always Remember: the first option, i.e. NERC (OpenStack) is selected by default!) and specify your anticipated computing units. Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC-OCP (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers. Keep in mind that you can easily scale and expand your current resource allocations within your project by following this documentation later on.</p>"},{"location":"get-started/allocation/adding-a-new-allocation/#adding-a-new-resource-allocation-to-your-academic-edu-openshift-project","title":"Adding a new Resource Allocation to your Academic (EDU) OpenShift project","text":"<p>Note: Requested/Approved Academic (EDU) OpenShift Storage Quota &amp; Cost</p> <p>Ensure you choose NERC-OCP-EDU (OpenShift) in the Resource option (Always Remember: the first option, i.e. NERC (OpenStack) is selected by default!). Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC-OCP-EDU (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers. Keep in mind that you can easily scale and expand your current resource allocations within your project by following this documentation later on.</p>"},{"location":"get-started/allocation/adding-a-project/","title":"A New Project Creation Process","text":""},{"location":"get-started/allocation/adding-a-project/#a-new-project-creation-process","title":"A New Project Creation Process","text":""},{"location":"get-started/allocation/adding-a-project/#what-pis-need-to-fill-in-order-to-request-a-project","title":"What PIs need to fill in order to request a Project?","text":"<p>Once logged in to NERC's ColdFront, PIs can choose Projects sub-menu located under the Project menu.</p> <p></p> <p></p> <p>Clicking on the \"Add a project\" button will show the interface below:</p> <p></p> <p>Very Important: Project Title Length Limitation</p> <p>Please ensure that the project title is both concise and does not exceed a length of 45 characters.</p> <p>PIs need to specify an appropriate title (less than 45 characters), description of their research work that will be performed on the NERC (in one or two paragraphs), the field(s) of science or research domain(s), and then click the \"Save\" button. Once saved successfully, PIs effectively become the \"manager\" of the project, and are free to add or remove users and also request resource allocation(s) to any Projects for which they are the PI. PIs are permitted to add users to their group, request new allocations, renew expiring allocations, and provide information such as publications and grant data. PIs can maintain all their research information under one project or, if they require, they can separate the work into multiple projects.</p>"},{"location":"get-started/allocation/allocation-change-request/","title":"Request change to Resource Allocation to an existing project","text":""},{"location":"get-started/allocation/allocation-change-request/#request-change-to-resource-allocation-to-an-existing-project","title":"Request change to Resource Allocation to an existing project","text":"<p>If past resource allocation is not sufficient for an existing project, PIs or project managers can request a change by clicking \"Request Change\" button on project resource allocation detail page as show below:</p> <p></p>"},{"location":"get-started/allocation/allocation-change-request/#request-change-resource-allocation-attributes-for-openstack-project","title":"Request Change Resource Allocation Attributes for OpenStack Project","text":"<p>This will bring up the detailed Quota attributes for that project as shown below:</p> <p></p> <p>Important: Requested/Approved Allocated OpenStack Storage Quota &amp; Cost</p> <p>For NERC (OpenStack) resource types, the Storage quotas are controlled by the values of the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" quota attributes. The Storage cost is determined by your requested and approved allocation values for these quota attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers.</p> <p>PI or project managers can provide a new value for the individual quota attributes, and give justification for the requested changes so that the NERC admin can review the change request and approve or deny based on justification and quota change request. Then submitting the change request, this will notify the NERC admin about it. Please wait untill the NERC admin approves/ deny the change request to see the change on your resource allocation for the selected project.</p> <p>Important Information</p> <p>PI or project managers can put the new values on the textboxes for ONLY quota attributes they want to change others they can be left blank so those quotas will not get changed!</p> <p>To use GPU resources on your VM, you need to specify the number of GPUs in the \"OpenStack GPU Quota\" attribute. Additionally, ensure that your other quota attributes, namely \"OpenStack Compute vCPU Quota\" and \"OpenStack Compute RAM Quota (MiB)\" have sufficient resources to meet the vCPU and RAM requirements for one of the GPU tier-based flavors. Refer to the GPU Tier documentation for specific requirements and further details on the flavors available for GPU usage.</p>"},{"location":"get-started/allocation/allocation-change-request/#allocation-change-requests-for-openstack-project","title":"Allocation Change Requests for OpenStack Project","text":"<p>Once the request is processed by the NERC admin, any user can view that request change trails for the project by looking at the \"Allocation Change Requests\" section that looks like below:</p> <p></p> <p>Any user can click on Action button to view the details about the change request. This will show more details about the change request as shown below:</p> <p></p>"},{"location":"get-started/allocation/allocation-change-request/#how-to-use-gpu-resources-in-your-openstack-project","title":"How to Use GPU Resources in your OpenStack Project","text":"<p>Comparison Between CPU and GPU</p> <p>To learn more about the key differences between CPUs and GPUs, please read this.</p> <p>A GPU instance is launched in the same way as any other compute instance, with a few considerations to keep in mind:</p> <ol> <li> <p>When launching a GPU based instance, be sure to select one of the     GPU Tier     based flavor.</p> </li> <li> <p>You need to have sufficient resource quota to launch the desired flavor. Always     ensure you know which GPU-based flavor you want to use, then submit an     allocation change request     to adjust your current allocation to fit the flavor's resource requirements.</p> <p>Resource Required for Launching a VM with \"NVIDIA A100 SXM4 40GB\" Flavor.</p> <p>Based on the GPU Tier documentation, NERC provides two variations of NVIDIA A100 SXM4 40GB flavors:</p> <ol> <li><code>gpu-su-a100sxm4.1</code>: Includes 1 NVIDIA A100 GPU</li> <li><code>gpu-su-a100sxm4.2</code>: Includes 2 NVIDIA A100 GPUs</li> </ol> <p>You should select the flavor that best fits your resource needs and ensure your OpenStack quotas are appropriately configured for the chosen flavor. To use a GPU-based VM flavor, choose the one that best fits your resource needs and make sure your OpenStack quotas meet the required specifications:</p> <ul> <li> <p>For the <code>gpu-su-a100sxm4.1</code> flavor:</p> <ul> <li>vCPU: 32</li> <li>RAM (GiB): 240</li> </ul> </li> <li> <p>For the <code>gpu-su-a100sxm4.2</code> flavor:</p> <ul> <li>vCPU: 64</li> <li>RAM (GiB): 480</li> </ul> </li> </ul> <p>Ensure that your OpenStack resource quotas are configured as follows:</p> <ul> <li>OpenStack GPU Quota: Meets or exceeds the number of GPUs required by the   chosen flavor.</li> <li>OpenStack Compute vCPU Quota: Meets or exceeds the vCPU requirement.</li> <li>OpenStack Compute RAM Quota (MiB): Meets or exceeds the RAM requirement.</li> </ul> <p>Properly configure these quotas to successfully launch a VM with the selected \"gpu-su-a100sxm4\" flavor.</p> </li> <li> <p>We recommend using ubuntu-22.04-x86_64     as the image for your GPU-based instance because we have tested the NVIDIA driver     with this image and obtained good results. That said, it is possible to run a     variety of other images as well.</p> </li> </ol>"},{"location":"get-started/allocation/allocation-change-request/#request-change-resource-allocation-attributes-for-openshift-project","title":"Request Change Resource Allocation Attributes for OpenShift Project","text":"<p>Important: Requested/Approved Allocated OpenShift Storage Quota &amp; Cost</p> <p>For NERC-OCP (OpenShift) and NERC-OCP-EDU (OpenShift) resource types, the Storage quotas are controlled by the values of the \"OpenShift Request Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" quota attributes. The Storage cost is determined by your requested and approved allocation values for these quota attributes.</p> <p>PI or project managers can provide a new value for the individual quota attributes, and give justification for the requested changes so that the NERC admin can review the change request and approve or deny based on justification and quota change request. Then submitting the change request, this will notify the NERC admin about it. Please wait untill the NERC admin approves/ deny the change request to see the change on your resource allocation for the selected project.</p> <p>Important Information</p> <p>PI or project managers can put the new values on the textboxes for ONLY quota attributes they want to change others they can be left blank so those quotas will not get changed!</p> <p>In order to use GPU resources on your pod, you must specify the number of GPUs you want to use in the \"OpenShift Request on GPU Quota\" attribute.</p>"},{"location":"get-started/allocation/allocation-change-request/#allocation-change-requests-for-openshift-project","title":"Allocation Change Requests for OpenShift Project","text":"<p>Once the request is processed by the NERC admin, any user can view that request change trails for the project by looking at the \"Allocation Change Requests\" section that looks like below:</p> <p></p> <p>Any user can click on Action button to view the details about the change request. This will show more details about the change request as shown below:</p> <p></p>"},{"location":"get-started/allocation/allocation-change-request/#how-to-use-gpu-resources-in-your-openshift-project","title":"How to Use GPU Resources in your OpenShift Project","text":"<p>Comparison Between CPU and GPU</p> <p>To learn more about the key differences between CPUs and GPUs, please read this.</p> <p>For OpenShift pods, we can specify different types of GPUs. Since OpenShift is not based on flavors, we can customize the resources as needed at the pod level while still utilizing GPU resources.</p> <p>You can read about how to specify a pod to use a GPU here.</p> <p>Also, you will be able to select a different GPU device for your workload, as explained here.</p>"},{"location":"get-started/allocation/allocation-details/","title":"Allocation details","text":""},{"location":"get-started/allocation/allocation-details/#allocation-details","title":"Allocation details","text":"<p>Access to ColdFront's allocations details is based on user roles. PIs and managers see the same allocation details as users, and can also add project users to the allocation, if they're not already on it, and remove users from an allocation.</p>"},{"location":"get-started/allocation/allocation-details/#how-to-view-resource-allocation-details-in-the-project","title":"How to View Resource Allocation Details in the Project","text":"<p>A single project can have multiple allocations. To view details about a specific resource allocation, click on any of the available allocations in the Allocations section of the project details. Here, you can view the Resource Type, information about your Allocated Project, status, End Date of the allocation, and Actions button or any pending actions as shown below:</p> <p></p> <p>Clicking the Action icon (shown as a folder icon on the right side of each allocation, as seen in the image above) for the corresponding allocation will open a page displaying detailed information about that allocation. You can access either the PI and Manager View or General User View of the allocation detail page for OpenStack or OpenShift Resource Allocation, depending on your role in the project.</p>"},{"location":"get-started/allocation/allocation-details/#how-to-find-id-of-the-resource-allocation","title":"How to find ID of the Resource Allocation","text":"<p>After clicking the Action button for the corresponding allocation, you will be redirected to a new allocation detail page. The web browser will display the URL in the following format:</p> <pre><code>https://coldfront.mss.mghpcc.org/allocation/&lt;Allocation_ID&gt;/\n</code></pre> <p>To find the ID of the resource allocation, observe the URL and note the <code>&lt;Allocation_ID&gt;</code> part. For example, in the URL <code>https://coldfront.mss.mghpcc.org/allocation/1/</code>, the resource Allocation ID is 1.</p>"},{"location":"get-started/allocation/allocation-details/#pi-and-manager-view","title":"PI and Manager View","text":"<p>PIs and managers can view important details of the project and underlying allocations. It shows all allocations including start and end dates, creation and last modified dates, users on the allocation and public allocation attributes. PIs and managers can add or remove users from allocations.</p>"},{"location":"get-started/allocation/allocation-details/#pi-and-manager-allocation-view-of-openstack-resource-allocation","title":"PI and Manager Allocation View of OpenStack Resource Allocation","text":""},{"location":"get-started/allocation/allocation-details/#pi-and-manager-allocation-view-of-openshift-resource-allocation","title":"PI and Manager Allocation View of OpenShift Resource Allocation","text":""},{"location":"get-started/allocation/allocation-details/#pi-and-manager-allocation-view-of-academic-edu-openshift-resource-allocation","title":"PI and Manager Allocation View of Academic (EDU) OpenShift Resource Allocation","text":""},{"location":"get-started/allocation/allocation-details/#general-user-view","title":"General User View","text":"<p>General Users who are not PIs or Managers on a project see a read-only view of the allocation details. If a user is on a project but not a particular allocation, they will not be able to see the allocation in the Project view nor will they be able to access the Allocation detail page.</p>"},{"location":"get-started/allocation/allocation-details/#general-user-view-of-openstack-resource-allocation","title":"General User View of OpenStack Resource Allocation","text":""},{"location":"get-started/allocation/allocation-details/#general-user-view-of-openshift-resource-allocation","title":"General User View of OpenShift Resource Allocation","text":""},{"location":"get-started/allocation/allocation-details/#general-user-view-of-academic-edu-openshift-resource-allocation","title":"General User View of Academic (EDU) OpenShift Resource Allocation","text":""},{"location":"get-started/allocation/archiving-a-project/","title":"Archiving an Existing Project","text":""},{"location":"get-started/allocation/archiving-a-project/#archiving-an-existing-project","title":"Archiving an Existing Project","text":"<p>Only a PI can archive their ColdFront project(s) by accessing NERC's ColdFront interface.</p> <p>Important Note</p> <p>If you archive a project then this will expire all your allocations on that project, which will clean up and also disable your group's access to the resources in those allocations. Also, you cannot make any changes to archived projects.</p> <p></p> <p>Once archived it is no longer visible on your projects list. All archived projects will be listed under your archived projects, which can be viewed by clicking the \"View archived projects\" button, as shown below:</p> <p></p> <p>All your archived projects are displayed here:</p> <p></p>"},{"location":"get-started/allocation/coldfront/","title":"What is NERC's ColdFront?","text":""},{"location":"get-started/allocation/coldfront/#what-is-nercs-coldfront","title":"What is NERC's ColdFront?","text":"<p>NERC uses NERC's ColdFront interface, an open source resource allocation management system called ColdFront to provide a single point-of-entry for administration, reporting, and measuring scientific impact of NERC resources for PI.</p> <p>Learning ColdFront</p> <p>A collection of animated gifs showcasing common functions in ColdFront is available, providing helpful insights into how these features can be utilized.</p>"},{"location":"get-started/allocation/coldfront/#how-to-get-access-to-nercs-coldfront","title":"How to get access to NERC's ColdFront","text":"<p>Any users who had registerd their user accounts through the MGHPCC Shared Services (MGHPCC-SS) Account Portal also known as \"RegApp\" can get access to NERC's ColdFront interface.</p> <p>General Users who are not PIs or Managers on a project see a read-only view of the NERC's ColdFront as described here.</p> <p>Whereas, once a PI Account request is granted, the PI will receive an email confirming the request approval and how to connect NERC's ColdFront.</p> <p>PI or project managers can use NERC's ColdFront as a self-service web-portal that can see an administrative view of it as described here and can do the following tasks:</p> <ul> <li> <p>Only PI can add a new project and archive any existing project(s)</p> </li> <li> <p>Manage existing projects</p> </li> <li> <p>Request allocations that fall under projects on NERC's resources such as clusters,     cloud resources, servers, storage, and software licenses</p> </li> <li> <p>Add/remove user access to/from allocated resources who is a member of the project     without requiring system administrator interaction</p> </li> <li> <p>Elevate selected users to 'manager' status, allowing them to handle some of the     PI asks such as request new resource allocations, add/remove users to/from resource     allocations, add project data such as grants and publications</p> </li> <li> <p>Monitor resource utilization such as storage and cloud usage</p> </li> <li> <p>Receive email notifications for expiring/renewing access to resources as well     as notifications when allocations change status - i.e. Active, Active (Needs     Renewal), Denied, Expired</p> </li> <li> <p>Provide information such as grants, publications, and other reportable data for     periodic review by center director to demonstrate need for the resources</p> </li> </ul>"},{"location":"get-started/allocation/coldfront/#how-to-login-to-nercs-coldfront","title":"How to login to NERC's ColdFront?","text":"<p>NERC's ColdFront interface provides users with login page as shown here:</p> <p></p> <p>Please click on \"Log In\" button. Then, it will show the login interface as shown below:</p> <p></p> <p>You need to click on \"Log in via OpenID Connect\" button. This will redirect you to CILogon welcome page where you can select your appropriate Identity Provider as shown below:</p> <p></p> <p>Once successful, you will be redirected to the ColdFront's main dashboard as shown below:</p> <p></p>"},{"location":"get-started/allocation/manage-users-to-a-project/","title":"Managing Users in the Project","text":""},{"location":"get-started/allocation/manage-users-to-a-project/#managing-users-in-the-project","title":"Managing Users in the Project","text":""},{"location":"get-started/allocation/manage-users-to-a-project/#addremove-users-tofrom-a-project","title":"Add/Remove User(s) to/from a Project","text":"<p>A user can only view the projects they are a member of. PIs or Project Manager(s) can add or remove users from their respective projects by navigating to the Users section of the project, as shown below:</p> <p></p>"},{"location":"get-started/allocation/manage-users-to-a-project/#add-users-to-a-project","title":"Add User(s) to a Project","text":"<p>Once we click on the \"Add Users\" button, it will show us the following search interface:</p> <p></p> <p>Searching multiple users at once!</p> <p>If you want to simultaneously search for multiple users in the system, you can input multiple usernames separated by space or newline, as shown below:</p> <p></p> <p>NOTE: This will return a list of all users matching those provided usernames only if they exist.</p> <p>They can search for any users in the system that are not already part of the project by providing exact matched username or partial text of other multiple fields. The search results show details about the user account such as email address, username, first name, last name etc. as shown below:</p> <p></p> <p>Delegating user as 'Manager'</p> <p>When adding a user to your project you can optionally designate them as a \"Manager\" by selecting their role using the drop down next to their email. Read more about user roles here.</p> <p>Thus, found user(s) can be selected and assigned directly to the available resource allocation(s) on the given project using this interface. While adding the users, their Role also can be selected from the dropdown options as either User or Manager. Once confirmed with selection of user(s) their roles and allocations, click on the \"Add Selected Users to Project\" button.</p>"},{"location":"get-started/allocation/manage-users-to-a-project/#remove-users-from-a-project","title":"Remove User(s) from a Project","text":"<p>Removing Users from the Project is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface:</p> <p></p> <p>PI or project managers can select the user(s) and then click on the \"Remove Selected Users From Project\" button.</p> <p>Very Important</p> <p>If you remove a user (or users) from a project, they will automatically be removed from all allocations they were previously assigned to within that project.</p>"},{"location":"get-started/allocation/manage-users-to-a-project/#user-roles","title":"User Roles","text":"<p>Access to ColdFront is role based so users see a read-only view of the allocation details for any allocations they are on. PIs see the same allocation details as general users and can also add project users to the allocation if they're not already on it. Even on the first time, PIs add any user to the project as the User role. Later PI or project managers can delegate users on their project to the 'manager' role. This allows multiple managers on the same project. This provides the user with the same access and abilities as the PI. A \"Manager\" is a user who has the same permissions as the PI to add/remove users, request/renew allocations, add/remove project info such as grants, publications, and research output. Managers may also complete the annual project review.</p> <p>What can a PI do that a manager can't?</p> <p>The only tasks a PI can do that a manager can't is create a new project or archive any existing project(s). All other project-related actions that a PI can perform can also be accomplished by any one of the managers assigned to that project.</p> <p>General User Accounts are not able to create/update projects and request Resource Allocations. Instead, these accounts must be associated with a Project that has Resources. General User accounts that are associated with a Project have access to view their project details and use all the resources associated with the Project on NERC.</p> <p>General Users (not PIs or Managers) can turn off email notifications at the project level. PIs also have the 'manager' status on a project. Managers can't turn off their notifications. This ensures they continue to get allocation expiration notification emails.</p>"},{"location":"get-started/allocation/manage-users-to-a-project/#delegating-user-to-manager-role","title":"Delegating User to Manager Role","text":"<p>You can also modify a users role of existing project users at any time by clicking on the Edit button next to the user's name.</p> <p>To change a user's role to 'manager' click on the edit icon next to the user's name on the Project Detail page:</p> <p></p> <p>Then toggle the \"Role\" from User to Manager:</p> <p></p> <p>Very Important</p> <p>Make sure to click the \"Update\" button to save the change.</p> <p>This delegation of \"Manager\" role can also be done when adding a user to your project. You can optionally designate them as a \"Manager\" by selecting their role using the drop down next to their email as described here.</p>"},{"location":"get-started/allocation/manage-users-to-a-project/#notifications","title":"Notifications","text":"<p>All users on a project will receive notifications about allocations including reminders of upcoming expiration dates and status changes. Users may uncheck the box next to their username to turn off notifications. Managers and PIs on the project are not able to turn off notifications.</p> <p></p>"},{"location":"get-started/allocation/managing-users-to-an-allocation/","title":"Managing Users in the Project Resource Allocation","text":""},{"location":"get-started/allocation/managing-users-to-an-allocation/#managing-users-in-the-project-resource-allocation","title":"Managing Users in the Project Resource Allocation","text":"<p>When the PI or Project Manager(s) view the Allocation Detail page of a project by clicking the corresponding allocation under the Allocations section on the Project Detail page, they will see options to add or remove users under the Users in Allocation section, as shown below:</p> <p></p>"},{"location":"get-started/allocation/managing-users-to-an-allocation/#adding-users-to-a-project-resource-allocation","title":"Adding User(s) to a Project Resource Allocation","text":"<p>Any available users who were previously added to a project but not yet included in the selected allocation can be easily added by clicking the Add Users button.</p> <p>After clicking, the following interface will appear, allowing PIs or Project Manager(s) to select the desired users using the checkboxes and then click the Add Selected Users to Allocation button:</p> <p></p> <p>Very Important</p> <p>The desired user must already be on the project to be added to the allocation.</p>"},{"location":"get-started/allocation/managing-users-to-an-allocation/#removing-users-from-a-project-resource-allocation","title":"Removing User(s) from a Project Resource Allocation","text":"<p>Removing User(s) from the Resource Allocation is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface:</p> <p></p> <p>PI or project managers can select the user(s) on the checkboxes and then click on the \"Remove Selected Users From Project\" button.</p> <p>Very Important</p> <p>If you remove a user (or users) from an allocation, they will be removed only from that specific allocation. They will still remain part of any other allocation(s) they are assigned to and will continue to be available as users within the project. You can remove the user(s) from all allocations of a project by removing them from the project as explained here.</p>"},{"location":"get-started/allocation/project-and-allocation-review/","title":"Project and Individual Allocation Annual Review Process","text":""},{"location":"get-started/allocation/project-and-allocation-review/#project-and-individual-allocation-annual-review-process","title":"Project and Individual Allocation Annual Review Process","text":""},{"location":"get-started/allocation/project-and-allocation-review/#project-annual-review-process","title":"Project Annual Review Process","text":"<p>NERC's ColdFront allows annual project reviews for NERC admins by mandating PIs to assess and update their projects. With the Project Review feature activated, each project undergoes a mandatory review every 365 days. During this process, PIs update project details, confirm project members, and input publications, grants, and research outcomes from the preceding year.</p> <p>Required Project Review</p> <p>The PI or any manager(s) of a project must complete the project review once every 365 days. ColdFront does not send notifications to PIs when project reviews are due. Instead, when the PI or Manager(s) of a project views their project they will find the notification that the project review is due. Additionally, when the project review is pending, PIs or Project Manager(s) cannot request new allocations or renew expiring allocations or change request to update the allocated allocation attributes' values. This is to enforce PIs need to review their projects annually. The PI or any managers on the project are able to complete the project review process.</p>"},{"location":"get-started/allocation/project-and-allocation-review/#project-reviews-by-pis-or-project-managers","title":"Project Reviews by PIs or Project Manager(s)","text":"<p>When a PI or any Project Manager(s) of a project logs into NERC's ColdFront web console and their project review is due, they will see a banner next to the project name on the home page:</p> <p></p> <p>If they try to request a new allocation or renew an expiring allocation or change request to update the allocated allocation attributes' values, they will get an error message:</p> <p></p>"},{"location":"get-started/allocation/project-and-allocation-review/#project-review-steps","title":"Project Review Steps","text":"<p>When they click on the \"Review Project\" link they're presented with the requirements and a description of why we're asking for this update:</p> <p></p> <p>The links in each step direct them to different parts of their Project Detail page. This review page lists the dates when grants and publications were last updated. If there are no grant or publications or at least one of them hasn't been udpated in the last year, we ask for a reason they're not updating the project information. This helps encourage PIs to provide updates if they have them. If not, they provide a reason and this is displayed for the NERC admins as part of the review process.</p> <p>Once the project review page is completed, the PI is redirected to the project detail page and they see the status change to \"project review pending\".</p> <p></p>"},{"location":"get-started/allocation/project-and-allocation-review/#allocation-renewals","title":"Allocation Renewals","text":"<p>When the requested allocation is approved, it must have an \"End Date\" - which is normally 365 days or 1 year from the date it is approved i.e. \"Start Date\". Automated emails are triggered to all users on an allocation when the end date is 60 days away, 30 days, 7 days, and then set the allocation status to \"Active (Needs Renewal)\", unless the user turns off notifications on the project.</p> <p>Very Important: Urgent Allocation Renewal is Required Before End Date</p> <p>If the allocation renewal isn't processed prior to the original allocation end date by the PI or Manager, the allocation will set the allocation status to \"Active (Needs Renewal)\" and the allocation users will get a notification email letting them know the allocation needs renewal!</p> <p></p> <p>Currently, a project will continue to be able to utilize allocations even after the allocation end date, which will result in ongoing costs for you. Such allocation will be marked as \"Active (Needs Renewal)\" as shown below:</p> <p></p> <p>Allocation renewals may not require any additions or changes to the allocation attributes from the PI or Manager. By default, if the PI or Manager clicks on the 'Activate' button, as shown below:</p> <p></p> <p>Then it will prompt for confirmation and allow the admin to review and submit the activation request by clicking on 'Submit' button, as shown below:</p> <p></p> <p>Emails are sent to all allocation users letting them know the renewal request has been submitted.</p> <p>Then the allocation status will change to \"Renewal Requested\" as shown below:</p> <p></p> <p>Once the renewal request is reviewed and approved by NERC admins, it will change into \"Active\" status and the expiration date is set to another 365 days as shown below:</p> <p></p> <p>Then an automated email notification will be sent to the PI and all users on the allocation that have enabled email notifications.</p>"},{"location":"get-started/allocation/project-and-allocation-review/#cost-associated-with-allocations-that-need-renewal-after-end-date","title":"Cost Associated with Allocations that Need Renewal after \"End Date\"","text":"<p>Currently, a project will continue be able to utilize allocations even after their \"End Date\", resulting in ongoing costs for you. Such allocations will be marked as \"Active (Needs Renewal)\". In the future, we plan to change this behavior so that allocations after end date will prevent associated VMs/pods from starting and may cause active VMs/pods to cease running.</p>"},{"location":"get-started/allocation/requesting-an-allocation/","title":"How to request a new Resource Allocation","text":""},{"location":"get-started/allocation/requesting-an-allocation/#how-to-request-a-new-resource-allocation","title":"How to request a new Resource Allocation","text":"<p>On the Project Detail page the project PI/manager(s) can request an allocation by clicking the \"Request Resource Allocation\" button, as shown below:</p> <p></p> <p>On the shown page, you will be able to choose either OpenStack Resource Allocation or OpenShift Resource Allocation by specifying either NERC (OpenStack), NERC-OCP-EDU (OpenShift) or NERC-OCP (OpenShift) in the Resource dropdown option.</p> <p>Very Important Note: The first option i.e. NERC (OpenStack), is selected by default.</p> <p>Default GPU Resource Quota for Initial Allocation Requests</p> <p>By default, the GPU resource quota is set to 0 for the initial resource allocation request for both OpenStack and OpenShift Resource Types. However, you will be able to change request and adjust the corresponding GPU quotas for both after they are approved for the first time. For NERC's OpenStack, please follow this guide on how to utilize GPU resources in your OpenStack project. For NERC's OpenShift or Academic (EDU) OpenShift resource type please refer to this reference to learn about how to use GPU resources in pod level.</p>"},{"location":"get-started/allocation/requesting-an-allocation/#request-a-new-openstack-resource-allocation-for-an-openstack-project","title":"Request A New OpenStack Resource Allocation for an OpenStack Project","text":"<p>If users have already been added to the project as described here, the Users selection section will be displayed as shown below:</p> <p></p> <p>In this section, the project PI/manager(s) can choose user(s) from the project to be included in this allocation before clicking the \"Submit\" button.</p> <p>Read the End User License Agreement Before Submission</p> <p>You should read the shown End User License Agreement (the \"Agreement\"). By clicking the \"Submit\" button, you agree to the Terms and Conditions.</p> <p>Important: Requested/Approved Allocated OpenStack Storage Quota &amp; Cost</p> <p>Ensure you choose NERC (OpenStack) in the Resource option and specify your anticipated computing units. Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC (OpenStack) Resource Allocations, the Storage quotas are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers. Keep in mind that you can easily scale and expand your current resource allocations within your project by following this documentation later on.</p>"},{"location":"get-started/allocation/requesting-an-allocation/#resource-allocation-quotas-for-openstack-project","title":"Resource Allocation Quotas for OpenStack Project","text":"<p>The amount of quota to start out a resource allocation after approval, can be specified using an integer field in the resource allocation request form as shown above. The provided unit value is computed as PI or project managers request resource quota. The basic unit of computational resources is defined in terms of integer value that corresponds to multiple OpenStack resource quotas. For example, 1 Unit corresponds to:</p> Resource Name Quota Amount x Unit <code>Instances</code> 1 <code>vCPUs</code> 1 <code>GPU</code> 0 <code>RAM(MiB)</code> 4096 <code>Volumes</code> 2 <code>Volume Storage(GiB)</code> 20 <code>Object Storage(GiB)</code> 1 <p>Information</p> <p>By default, 2 OpenStack Floating IPs, 10 Volume Snapshots and 10 Security Groups are provided to each approved project regardless of units of requested quota units.</p>"},{"location":"get-started/allocation/requesting-an-allocation/#request-a-new-openshift-resource-allocation-for-an-openshift-project","title":"Request A New OpenShift Resource Allocation for an OpenShift project","text":"<p>If users have already been added to the project as described here, the Users selection section will be displayed as shown below:</p> <p></p> <p>In this section, the project PI/manager(s) can choose user(s) from the project to be included in this allocation before clicking the \"Submit\" button.</p> <p>Read the End User License Agreement Before Submission</p> <p>You should read the shown End User License Agreement (the \"Agreement\"). By clicking the \"Submit\" button, you agree to the Terms and Conditions.</p>"},{"location":"get-started/allocation/requesting-an-allocation/#resource-allocation-quotas-for-openshift-project","title":"Resource Allocation Quotas for OpenShift Project","text":"<p>The amount of quota to start out a resource allocation after approval, can be specified using an integer field in the resource allocation request form as shown above. The provided unit value is computed as PI or project managers request resource quota. The basic unit of computational resources is defined in terms of integer value that corresponds to multiple OpenShift resource quotas. For example, 1 Unit corresponds to:</p> Resource Name Quota Amount x Unit <code>vCPUs</code> 1 <code>GPU</code> 0 <code>RAM(MiB)</code> 4096 <code>Persistent Volume Claims (PVC)</code> 2 <code>Storage(GiB)</code> 20 <code>Ephemeral Storage(GiB)</code> 5 <p>Important: Requested/Approved Allocated OpenShift Storage Quota &amp; Cost</p> <p>Ensure you choose NERC-OCP (OpenShift) in the Resource option (Always Remember: the first option, i.e. NERC (OpenStack) is selected by default!) and specify your anticipated computing units. Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC-OCP (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers. Keep in mind that you can easily scale and expand your current resource allocations within your project by following this documentation later on.</p>"},{"location":"get-started/allocation/requesting-an-allocation/#request-a-new-academic-edu-openshift-resource-allocation-for-an-academic-project","title":"Request A New Academic (EDU) OpenShift Resource Allocation for an Academic project","text":"<p>If users have already been added to the project as described here, the Users selection section will be displayed as shown below:</p> <p></p> <p>In this section, the project PI/manager(s) can choose user(s) from the project to be included in this allocation before clicking the \"Submit\" button.</p> <p>Read the End User License Agreement Before Submission</p> <p>You should read the shown End User License Agreement (the \"Agreement\"). By clicking the \"Submit\" button, you agree to the Terms and Conditions.</p>"},{"location":"get-started/allocation/requesting-an-allocation/#resource-allocation-quotas-for-academic-edu-openshift-project","title":"Resource Allocation Quotas for Academic (EDU) OpenShift Project","text":"<p>When requesting a new resource allocation, the unit quota field is not displayed by default, unlike for other resource types. Instead, an initial default allocation of 1 Unit is granted upon approval. After this initial approval, project PIs or managers can easily scale and expand the current approved resource allocations within the project by following this documentation later on.</p> <p>Each Unit represents a predefined set of computational resources, corresponding to multiple OpenShift resource quotas. For example, 1 Unit corresponds to:</p> Resource Name Quota Amount x Unit <code>vCPUs</code> 1 <code>GPU</code> 0 <code>RAM(MiB)</code> 4096 <code>Persistent Volume Claims (PVC)</code> 2 <code>Storage(GiB)</code> 20 <code>Ephemeral Storage(GiB)</code> 5 <p>Note: Requested/Approved Academic (EDU) OpenShift Storage Quota &amp; Cost</p> <p>Ensure you choose NERC-OCP-EDU (OpenShift) in the Resource option (Always Remember: the first option, i.e. NERC (OpenStack) is selected by default!). Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC-OCP-EDU (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers.</p>"},{"location":"get-started/best-practices/best-practices-for-bu/","title":"Best Practices for Boston University","text":""},{"location":"get-started/best-practices/best-practices-for-bu/#best-practices-for-boston-university","title":"Best Practices for Boston University","text":""},{"location":"get-started/best-practices/best-practices-for-bu/#further-references","title":"Further References","text":"<p>https://www.bu.edu/tech/services/security/cyber-security/sensitive-data/</p> <p>https://www.bu.edu/tech/support/information-security/</p> <p>https://www.bu.edu/tech/about/security-resources/bestpractice/</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/","title":"Best Practices for Harvard University","text":""},{"location":"get-started/best-practices/best-practices-for-harvard/#securing-your-public-facing-server","title":"Securing Your Public Facing Server","text":""},{"location":"get-started/best-practices/best-practices-for-harvard/#overview","title":"Overview","text":"<p>This document is aimed to provide you with a few concrete actions you can take to significantly enhance the security of your devices. This advice can be enabled even if your servers are not public facing. However, we strongly recommend implementing these steps if your servers are intended to be accessible to the internet at large.</p> <p>All recommendations and guidance are guided by our policy that has specific requirements, the current policy/requirements for servers at NERC can be found here.</p> <p>Harvard University Security Policy Information</p> <p>Please note that all assets deployed to your NERC project must be compliant with University Security policies. Please familiarize yourself with the Harvard University Information Security Policy and your role in securing data. If you have any questions about how Security should be implemented in the Cloud, please contact your school security officer: \"Havard Security Officer\".</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#know-your-data","title":"Know Your Data","text":"<p>Depending on the data that exists on your servers, you may have to take added or specific steps to safeguard that data. At Harvard, we developed a scale of data classification ranging from 1 to 5 in order of increasing data sensitivity.</p> <p>We have prepared added guidance with examples for both Administrative Data and Research Data.</p> <p>Additionally, if your work involved individuals situated in a European Economic Area, you may be subject to the requirements of the General Data Protection Regulations and more information about your responsibilities can be found here.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#host-protection","title":"Host Protection","text":"<p>The primary focus of this guide is to provide you with security essentials that we support and that you can implement with little effort.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#endpoint-protection","title":"Endpoint Protection","text":"<p>Harvard University uses the endpoint protection service: Crowdstrike, which actively checks a machine for indication of malicious activity and will act to both block the activity and remediate the issue. This service is offered free to our community members and requires the installation of an agent on the server that runs transparently. This software enables the Harvard security team to review security events and act as needed.</p> <p>Crowdstrike can be downloaded from our repository at: agents.itsec.harvard.edu this software is required for all devices owned by Harvard staff/faculty and available for all operating systems.</p> <p>Please note</p> <p>To acess this repository you need to be in Harvard Campus Network.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#patchupdate-regularly","title":"Patch/Update Regularly","text":"<p>It is common that vendors/developers will announce that they have discovered a new vulnerability in the software you may be using. A lot of these vulnerabilities are addressed by new releases that the developer issues. Keeping your software and server operating system up to date with current versions ensures that you are using a version of the software that does not have any known/published vulnerabilities.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#vulnerability-management","title":"Vulnerability Management","text":"<p>Various software versions have historically been found to be vulnerable to specific attacks and exploits. The risk of running older versions of software is that you may be exposing your machine to a possible known method of attack.</p> <p>To assess which attacks you might be vulnerable to and be provided with specific remediation guidance, we recommend enrolling your servers with our Tenable service which periodically scans the software on your server and correlates the software information with a database of published vulnerabilities. This service will enable you to prioritize which component you need to upgrade or otherwise define which vulnerabilities you may be exposed to.</p> <p>The Tenable agent run transparently and can be enabled to work according to the parameters set for your school; the agent can be downloaded here and configuration support can be found by filing a support request via HUIT support ticketing system: ServiceNow.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#safer-applications-development","title":"Safer Applications/ Development","text":"<p>Every application has its own unique operational constraints/requirements, and the advice below cannot be comprehensive however we can offer a few general recommendations</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#secure-credential-management","title":"Secure Credential Management","text":"<p>Credentials should not be kept on the server, nor should they be included directly in your programming logic.</p> <p>Attackers often review running code on the server to see if they can obtain any sensitive credentials that may have been included in each script. To better manage your credentials, we recommend either using:</p> <ul> <li> <p>1password Credential Manager</p> </li> <li> <p>AWS Secrets</p> </li> </ul>"},{"location":"get-started/best-practices/best-practices-for-harvard/#not-running-the-application-as-the-rootsuperuser","title":"Not Running the Application as the Root/Superuser","text":"<p>Frequently an application needs special permissions and access and often it is easiest to run an application in the root/superuser account. This is a dangerous practice since the application, when compromised, gives attackers an account with full administrative privileges. Instead, configuring the application to run with an account with only the permissions it needs to run is a way to minimize the impact of a given compromise.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#safer-networking","title":"Safer Networking","text":"<p>The goal in safer networking is to minimize the areas that an attacker can target.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#minimize-publicly-exposed-services","title":"Minimize Publicly Exposed Services","text":"<p>Every port/service open to the internet will be scanned to access your servers. We recommend that any service/port that is not needed to be accessed by the public be placed behind the campus firewall. This will significantly reduce the number of attempts by attackers to compromise your servers.</p> <p>In practice this usually means that you only expose posts 80/443 which enables you to serve websites, while you keep all other services such as SSH, WordPress-logins, etc behind the campus firewall.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#strengthen-ssh-logins","title":"Strengthen SSH Logins","text":"<p>Where possible, and if needed, logins to a Harvard service should be placed behind Harvardkey. For researchers however, the preferred login method is usually SSH and we recommend the following ways to strengthen your SSH accounts</p> <ol> <li> <p>Disable password only logins</p> <ul> <li> <p>In file <code>/etc/ssh/sshd_config</code> change <code>PasswordAuthentication</code> to <code>no</code> to     disable tunneled clear text passwords i.e. <code>PasswordAuthentication no</code>.</p> </li> <li> <p>Uncomment the permit empty passwords option in the second line, and, if     needed, change <code>yes</code> to <code>no</code> i.e. <code>PermitEmptyPasswords no</code>.</p> </li> <li> <p>Then run <code>service ssh restart</code>.</p> </li> </ul> </li> <li> <p>Use SSH keys with passwords enabled on them</p> </li> <li> <p>If possible, enroll the SSH service with a Two-factor authentication provider     such as DUO or YubiKey.</p> </li> </ol>"},{"location":"get-started/best-practices/best-practices-for-harvard/#attack-detection","title":"Attack Detection","text":"<p>Despite the best protection, a sophisticated attacker may still find a way to compromise your servers and in those scenarios, we want to enhance your ability to detect activity that may be suspicious.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#install-crowdstrike","title":"Install Crowdstrike","text":"<p>As stated above, Crowdstrike is both an endpoint protection service and also an endpoint detection service. This software understands activities that might be benign in isolation but coupled with other actions on the device may be indicative of a compromise. It also enables the quickest security response.</p> <p>Crowdstrike can be downloaded from our repository at: agents.itsec.harvard.edu this software is needed for all devices owned by Harvard staff/faculty and available for all operating systems.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#safeguard-your-system-logs","title":"Safeguard your System Logs","text":"<p>System logs are logs that check and track activity on your servers, including logins, installed applications, errors and more.</p> <p>Sophisticated attackers will try to delete these logs to frustrate investigations and prevent discovery of their attacks. To ensure that your logs are still accessible and available for review, we recommend that you configure your logs to be sent to a system separate from your servers. This can be either sending logs to an external file storage repository. Or configuring a separate logging system using Splunk.</p> <p>For help setting up logging please file a support request via our support ticketing system: ServiceNow.</p>"},{"location":"get-started/best-practices/best-practices-for-harvard/#escalating-an-issue","title":"Escalating an Issue","text":"<p>There are several ways you can report a security issue and they are all documented on HUIT Internet Security and Data Privacy group site.</p> <p>In the event you suspect a security issue has occurred or wanted someone to supply a security assessment, please feel free to reach out to the HUIT Internet Security and Data Privacy group, specifically the Operations &amp; Engineering team.</p> <ul> <li> <p>Email Harvard ITSEC-OPS</p> </li> <li> <p>Service Queue</p> </li> <li> <p>Harvard HUIT Slack Channel: #isdp-public</p> </li> </ul>"},{"location":"get-started/best-practices/best-practices-for-harvard/#further-references","title":"Further References","text":"<p>https://policy.security.harvard.edu/all-servers</p> <p>https://enterprisearchitecture.harvard.edu/security-minimal-viable-product-requirements-huit-hostedmanaged-server-instances</p> <p>https://policy.security.harvard.edu/security-requirements</p>"},{"location":"get-started/best-practices/best-practices-for-my-institution/","title":"Best Practices for My Institution","text":""},{"location":"get-started/best-practices/best-practices-for-my-institution/#best-practices-for-my-institution","title":"Best Practices for My Institution","text":""},{"location":"get-started/best-practices/best-practices-for-my-institution/#institutions-with-the-best-practices-outlines","title":"Institutions with the Best Practices outlines","text":"<p>The following institutions using our services have already provided guidelines for best practices:</p> <ol> <li> <p>Harvard University</p> </li> <li> <p>Boston University</p> </li> </ol> <p>Upcoming Best Practices for other institutions</p> <p>We are in the process of obtaining Best Practices for institutions not listed above.</p> <p>If your institution already have outlined Best Practices guidelines with your internal IT department, please contact us to list it here soon by emailing us at help@nerc.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System.</p>"},{"location":"get-started/best-practices/best-practices/","title":"Quick Guide and Best Practices","text":""},{"location":"get-started/best-practices/best-practices/#best-practices-for-the-nerc-users","title":"Best Practices for the NERC Users","text":"<p>By 2025, according to Gartner's forecast, the responsibility for approximately 99% of cloud security failures will likely lie with customers. These failures can be attributed to the difficulties in gauging and overseeing risks associated with on-prem cloud security. The MGHPCC will enter into a lightweight Memorandum of Understanding (MOU) with each institutional customer that consumes NERC services and that will also clearly explain about the security risks and some of the shared responsibilities for the customers while using the NERC. This ensures roles and responsibilities are distinctly understood by each party.</p> <p>NERC Principal Investigators (PIs): PIs are ultimately responsible for their end-users and the security of the systems and applications that are deployed as part of their project(s) on NERC. This includes being responsible for the security of their data hosted on the NERC as well as users, accounts and access management.</p> <p>Every individual user needs to comply with your Institution's Security and Privacy policies to protect their Data, Endpoints, Accounts and Access management. They must ensure any data created on or uploaded to the NERC is adequately secured. Each customer has complete control over their systems, networks and assets. It is essential to restrict access to the NERC provided user environment only to authorized users by using secure identity and access management. Furthermore, users have authority over various credential-related aspects, including secure login mechanisms, single sign-on (SSO), and multifactor authentication.</p> <p>Under this model, we are responsible for operation of the physical infrastructure that includes responsibility for protecting, patching and maintaining underlying virtualization layer, servers, disks, storage, network gears, other hardwares, and softwares. Whereas NERC users are responsible for the security of the guest operating system (OS) and software stack i.e. databases used to run their applications and data. They are also entrusted with safeguarding middleware, containers, workloads, and any code or data generated by the platform.</p> <p>All NERC users are responsible for their use of NERC services, which include:</p> <ul> <li> <p>Following the best practices for security on NERC services. Please review your     institutional guidelines next.</p> </li> <li> <p>Complying with security policies regarding VMs and containers. NERC admins are     not responsible for maintaining or deploying VMs or containers created by PIs     for their projects. See Harvard University and Boston University policies     here. We will be adding more     institutions under this page soon. Without prior notice, NERC reserves the right     to shut down any VM or container that is causing internal or external problems     or violating these policies.</p> </li> <li> <p>Adhering to institutional restrictions and compliance policies around the data     they upload and provide access to/from NERC. At NERC, we only offer users to     store internal data in which information is chosen to keep confidential but the     disclosure of which would not cause material harm to you, your users and your     institution. Your institution may have already classified and categorized data     and implemented security policies and guidance for each category. If your project     includes sensitive data and information then you might need to contact NERC's     admin as soon as possible to discuss other potential options.</p> </li> <li> <p>Backups and/or snapshots     are the user's responsibility for volumes/data, configurations, objects, and     their state, which are useful in the case when users accidentally delete/lose     their data. NERC admins cannot recover lost data. In addition, while NERC stores     data with high redundancy to deal with computer or disk failures, PIs should     ensure they have off-site backups for disaster recovery, e.g., to deal with     occasional disruptions and outages due to the natural disasters that impact the     MGHPCC data center.</p> </li> </ul>"},{"location":"get-started/cost-billing/billing-faqs/","title":"Billing FAQs","text":""},{"location":"get-started/cost-billing/billing-faqs/#billing-frequently-asked-questions-faqs","title":"Billing Frequently Asked Questions (FAQs)","text":"<p>Our primary focus is to deliver outstanding on-prem cloud services, prioritizing reliability, security, and cutting-edge solutions to meet your research and teaching requirements. To achieve this, we have implemented a cost-effective pricing model that enables us to maintain, enhance, and sustain the quality of our services. By adopting consistent cost structures across all institutions, we can make strategic investments in infrastructure, expand our service portfolio, and enhance our support capabilities for a seamless user experience.</p> <p>Most of the institutions using our services have an MOU (Memorandum Of Understanding) with us to be better aligned to a number of research regulations, policies and requirements but if your institution does not have an MOU with us, please have someone from your faculty or administration contact us to discuss it soon by emailing us at help@nerc.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System.</p>"},{"location":"get-started/cost-billing/billing-faqs/#questions-answers","title":"Questions &amp; Answers","text":"1. As a new NERC PI for the first time, am I entitled to any credits? <ul> <li> <p>Yes, you will receive up to $1000 of credit for the first month only.</p> </li> <li> <p>This credit is not transferable to subsequent months.</p> </li> <li> <p>This does not apply to the usage of GPU resources.</p> </li> </ul> 2. How often will I be billed? <p>You or your institution will be billed monthly within the first week of each month.</p> 3. If I have an issue with my bill, who do I contact? <p>Please send your requests by emailing us at help@nerc.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System.</p> 4. How do I control costs? <p>Upon creating a project, you will set these resource limits (quotas) for OpenStack (VMs), OpenShift (containers), and storage through ColdFront. This is the maximum amount of resources you can consume at one time.</p> 5. Are we invoicing for CPUs/GPUs only when the VM or Pod is active? <p>Yes. You will only be billed based on your utilization (cores, memory, GPU) when VMs exist (even if they are Stopped!) or when pods are running. Utilization will be translated into billable Service Units (SUs).</p> <p>Persistent storage related to an OpenStack VM or OpenShift Pod will continue to be billed even when the VM is stopped or the Pod is not running.</p> 6. Am I going to incur costs for allocations after end date? <p>Currently, a project will continue be able to utilize allocations even after their \"End Date\", resulting in ongoing costs for you. Such allocations will be marked as \"Active (Needs Renewal)\". In the future, we plan to change this behavior so that allocations after end date will prevent associated VMs/pods from starting and may cause active VMs/pods to cease running.</p> 7. Are VMs invoiced even when shut down? <p>Yes, VMs continue to incur charges as long as they are consuming resources. To avoid being billed, you can either shelve or delete the instance/VM.</p> <p>If you choose to delete the VM, it's a good idea to create a snapshot beforehand to preserve your data and configuration.</p> 8. What actions can I take to reduce usage and therefore cost? <p>For step-by-step instructions, please refer to this detailed guide and keep in mind that storage is billed by requested amount not used amount.</p> 9. Is storage charged separately? <p>Yes, but on the same invoice. To learn more, see our page on Storage.</p> <p>Very Important: Be sure to adjust your approved storage quotas accordingly to avoid unnecessary charges for unused or unneeded storage.</p> 10. Will OpenStack &amp; OpenShift show on a single invoice? <p>Yes. In the near future customers of NERC will be able to view per project service utilization via the XDMoD tool.</p> 11. What happens when a Flavor is expanded during the month? <p>a. Flavors cannot be expanded.</p> <p>b. You can create a snapshot of an existing VM/Instance and, with that snapshot, deploy a new flavor of VM/Instance.</p> 12. Will I be charged for storage attached to shut-off instances? <p>Yes.</p> 13. Are we Invoicing Storage using ColdFront Requests or resource usage? <p>a. Storage is invoiced based on Coldfront Requests.</p> <p>b. When you request additional storage through Coldfront, invoicing on that additional storage will occur when your request is fulfilled. When you request a decrease in storage through Request change using ColdFront, your invoicing will adjust accordingly when your request is made. In both cases 'invoicing' means 'accumulate hours for whatever storage quantity was added or removed'.</p> <p>For example:</p> <ol> <li> <p>I request an increase in storage, the request is approved and processed.</p> <ul> <li>At this point we start Invoicing.</li> </ul> </li> <li> <p>I request a decrease in storage.</p> <ul> <li>The invoicing for that storage stops immediately.</li> </ul> </li> </ol> 14. For OpenShift, what values are we using to track CPU &amp; Memory? <p>a. For invoicing we utilize <code>requests.cpu</code> for tracking CPU utilization &amp; <code>requests.memory</code> for tracking memory utilization.</p> <p>b. Utilization will be capped based on the limits you set in ColdFront for your resource allocations.</p> 15. If a single Pod exceeds the resources for a GPU SU, how is it invoiced? <p>It will be invoiced as 2 or more GPU SU's depending on how many multiples of the resources it exceeds.</p> 16. How often will we change the pricing? <p>a. Our current plan is no more than once a year for existing offerings.</p> <p>b. Additional offerings may be added throughout the year (i.e. new types of hardware or storage).</p> 17. Is there any NERC Pricing Calculator? <p>Yes. Start your estimate with no commitment based on your resource needs by using this online tool. For more information about how to use this tool, see How to use the NERC Pricing Calculator.</p>"},{"location":"get-started/cost-billing/billing-process-for-bu/","title":"Billing Process for Boston University","text":""},{"location":"get-started/cost-billing/billing-process-for-bu/#billing-process-for-boston-university","title":"Billing Process for Boston University","text":"<p>Boston University has elected to receive a centralized invoice for its university investigators and their designated user's use of NERC services. IS&amp;T will then internally recover the cost from investigators. The process for cost recovery is currently being implemented, and we will reach out to investigators once the process is complete to obtain internal funding information to process your monthly bill.</p>"},{"location":"get-started/cost-billing/billing-process-for-bu/#subsidization-of-boston-universitys-use-of-nerc","title":"Subsidization of Boston University's Use of NERC","text":"<p>Boston University will subsidize a portion of NERC usage by its investigators. The University will subsidize $100 per month of an investigator's total usage on NERC, regardless of the number of NERC projects an investigator has established. Monthly subsidies cannot be carried over to subsequent months. The subsidized amount and method are subject to change, and any adjustments will be conveyed directly to investigators and updated on this page.</p> <p>Please direct any questions about BU's billing process by emailing us at help@nerc.mghpcc.org or submitting a new ticket to the the NERC's Support Ticketing System. Questions about a specific invoice that you have received can be sent to IST-ISR-NERC@bu.edu.</p>"},{"location":"get-started/cost-billing/billing-process-for-harvard/","title":"Billing Process for Harvard University","text":""},{"location":"get-started/cost-billing/billing-process-for-harvard/#billing-process-for-harvard-university","title":"Billing Process for Harvard University","text":"<p>Direct Billing for NERC is a convenience service for Harvard Faculty and Departments. HUIT will pay the monthly invoices and then allocate the monthly usage costs on the Harvard University General Ledger. This follows a similar pattern with how other Public Cloud Providers (AWS, Azure, GCP) accounts are billed and leverage the HUIT Central Billing Portal. Your HUIT Customer Code will be matched to your NERC Project Allocation Name as a Billing Asset. In this process you will be asked for your GL billing code, which you can change as needed per project. Please be cognizant that only a single billing code is allowed per billing asset. Therefore, if you have multiple projects with different funds, if you are able, please create a separate project for each fund. Otherwise, you will need to take care of this with internal journals inside of your department or lab. During each monthly billing cycle, the NERC team will upload the billing Comma-separated values (CSV) files to the HUIT Central Billing system accessible AWS Object Storage (S3) bucket. The HUIT Central Billing system ingests billing data files provided by NERC, maps the usage costs to HUIT Billing customers (and GL Codes) and then includes those amounts in HUIT Monthly Billing of all customers. This is an automated process.</p> <p>Please follow these two steps to ensure proper billing setup:</p> <ol> <li> <p>Each Harvard PI must have a HUIT billing account linked to their NetID (abc123),     and NERC requires a HUIT \"Customer Code\" for billing purposes. To create     a HUIT billing account, sign up here     with your HarvardKey. The PI's submission of the corresponding HUIT     \"Customer Code\" is now seamlessly integrated into the PI user account role     submission process. This means that PIs can provide the corresponding HUIT     \"Customer Code\" either while submitting NERC's PI Request Form     or by submitting a new ticket at NERC's Support Ticketing System     under the \"NERC PI Account Request\" option in the Help Topic dropdown menu.</p> <p>What if you already have an existing Customer Code?</p> <p>Please note that if you already have an existing active NERC account, you need to provide your HUIT Customer Code to NERC. If you think your department may already have a HUIT account but you don't know the corresponding Customer Code then you can contact HUIT Billing to get the required Customer Code.</p> </li> <li> <p>During the Resource Allocation review and approval process, we will utilize the     HUIT \"Customer Code\" provided by the PI in step #1 to align it with the approved     allocation. Before confirming the mapping of the Customer Code to the Resource     Allocation, we will send an email to the PI to confirm its accuracy and then     approve the requested allocation. Subsequently, after the allocation is approved,     we will request the PI to initiate a change request     to input the correct \"Customer Code\" into the allocation's \"Institution-Specific     Code\" attribute's value.</p> <p>Very Important Note</p> <p>We recommend keeping your \"Institution-Specific Code\" updated at all times, ensuring it accurately reflects your current and valid Customer Code. The PI or project manager(s) have the authority to request changes for updating the \"Institution-Specific Code\" attribute for each resource allocation. They can do so by submitting a Change Request as outlined here.</p> <p>How to view Project Name, Project ID &amp; Institution-Specific Code?</p> <p>By clicking on the Allocation detail page through ColdFront, you can access information about the allocation of each resource, including OpenStack and OpenShift as described here. You can review and verify Allocated Project Name, Allocated Project ID and Institution-Specific Code attributes, which are located under the \"Allocation Attributes\" section on the detail page as described here.</p> <p>Once we confirm the six-digit HUIT Customer Code for the PI and the correct resource allocation, the NERC admin team will initiate the creation of a new ServiceNow ticket. This will be done by reaching out to HUIT Billing or directly emailing HUIT Billing at huit-billing@harvard.edu for the approved and active allocation request.</p> <p>In this email, the NERC admin needs to specify the Allocated Project ID, Allocated Project Name, Customer Code, and PI's Email address. Then, the HUIT billing team will generate a unique Asset ID to be utilized by the Customer's HUIT billing portal.</p> <p>Important Information regarding HUIT Billing SLA</p> <p>Please note that we will require the PI or Manager(s) to repeat step #2 for any new resource allocation(s) as well as renewed allocation(s). Additionally, the HUIT Billing SLA for new Cloud Billing assets is 2 business days, although most requests are typically completed within 8 hours.</p> <p>Harvard University Security Policy Information</p> <p>Please note that all assets deployed to your NERC project must be compliant with University Security policies as described here. Please familiarize yourself with the Harvard University Information Security Policy and your role in securing data. If you have any questions about how Security should be implemented in the Cloud, please contact your school security officer: \"Havard Security Officer\".</p> </li> </ol>"},{"location":"get-started/cost-billing/billing-process-for-my-institution/","title":"Billing Process for My Institution","text":""},{"location":"get-started/cost-billing/billing-process-for-my-institution/#billing-process-for-my-institution","title":"Billing Process for My Institution","text":""},{"location":"get-started/cost-billing/billing-process-for-my-institution/#memorandum-of-understanding-mou","title":"Memorandum of Understanding (MOU)","text":"<p>The New England Research Cloud (NERC) is a shared service offered through the Massachusetts Green High Performance Computing Center (MGHPCC). The MGHPCC will enter into a lightweight Memorandum of Understanding (MOU) with each institutional customer that consumes NERC services. The MOU is intended to ensure the institution maintains access to valuable and relevant cloud services provided by the MGHPCC via the NERC to be better aligned to a number of research regulations, policies, and requirements and also ensure NERC remains sustainable over time.</p>"},{"location":"get-started/cost-billing/billing-process-for-my-institution/#institutions-with-established-mous-and-billing-processes","title":"Institutions with established MOUs and Billing Processes","text":"<p>For cost recovery purposes, institutional customers may elect to receive one invoice for the usage of NERC services by its PIs and cost recovery internally. Every month, the NERC team will export, back up, and securely store the billing data for all PIs in the form of comma-separated values (CSV) files and provide it to the MGHPCC for billing purposes.</p> <p>The following institutions using our services have established MOU as well as billing processes with us:</p> <ol> <li> <p>Harvard University</p> </li> <li> <p>Boston University</p> </li> </ol> <p>Upcoming MOU with other institutions</p> <p>We are in the process of establishing MOUs for institutions not listed above.</p> <p>PIs from other institutions not listed above can still utilize NERC services with the understanding that they are directly accountable for managing their usage and ensuring all service charges are paid promptly. If you have any some common questions or need further information, see our Billing FAQs for comprehensive answers.</p> <p>If your institution does not have an MOU with us, please have someone from your faculty or administration contact us to discuss it soon by emailing us at help@nerc.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System.</p>"},{"location":"get-started/cost-billing/how-pricing-works/","title":"How does NERC pricing work?","text":""},{"location":"get-started/cost-billing/how-pricing-works/#how-does-nerc-pricing-work","title":"How does NERC pricing work?","text":"<p>As a new PI using NERC for the first time, am I entitled to any credits?</p> <p>As a new PI using NERC for the first time, you might wonder if you get any credits. Yes, you'll receive up to $1000 for the first month only. But remember, this credit can not be used in the following months. Also, it does not apply to GPU resource usage.</p> <p>NERC offers you a pay-as-you-go approach for pricing for our cloud infrastructure offerings (Tiers of Service), including Infrastructure-as-a-Service (IaaS) \u2013 Red Hat OpenStack and Platform-as-a-Service (PaaS) \u2013 Red Hat OpenShift. The exception is the Storage quotas on NERC Storage Tiers, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC (OpenStack) Resource Allocations, storage quotas are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes. Whereas for NERC-OCP (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes. If you have common questions or need more information, refer to our Billing FAQs for comprehensive answers. NERC offers a flexible cost model where an institution (with a per-project breakdown) is billed solely for the duration of the specific services required. Access is based on project-approved resource quotas, eliminating runaway usage and charges. There are no obligations of long-term contracts or complicated licensing agreements. Each institution will enter a lightweight MOU with MGHPCC that defines the services and billing model.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#calculations","title":"Calculations","text":""},{"location":"get-started/cost-billing/how-pricing-works/#service-units-sus","title":"Service Units (SUs)","text":"Name GPU vCPU RAM (GiB) Current Price H100 GPU 1 124 360 $4 A100sxm4 GPU 1 31 240 $2.078 A100 GPU 1 24 74 $1.803 V100 GPU 1 48 192 $1.214 K80 GPU 1 6 28.5 $0.463 CPU 0 1 4 $0.013 <p>Now Available on NERC: NVIDIA H100 GPUs</p> <p>The cutting-edge NVIDIA H100 80GB GPUs are now available for use with:</p> <p>\ud83d\udd39 NERC Red Hat OpenShift AI (RHOAI) via JupyterLab workbenches</p> <p>\ud83d\udd39 NERC OpenShift - based Containers</p> <p>To get started, read our latest announcement.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#breakdown","title":"Breakdown","text":""},{"location":"get-started/cost-billing/how-pricing-works/#cpugpu-sus","title":"CPU/GPU SUs","text":"<p>Service Units (SUs) can only be purchased as a whole unit. We will charge for Pods (summed up by Project) and VMs on a per-hour basis for any portion of an hour they are used, and any VM \"flavor\"/Pod reservation is charged as a multiplier of the base SU for the maximum resource they reserve.</p> <p>GPU SU Example:</p> <ul> <li> <p>A Project or VM with:</p> <p><code>1 A100 GPU, 24 vCPUs, 95MiB RAM, 199.2hrs</code></p> </li> <li> <p>Will be charged:</p> <p><code>1 A100 GPU SUs x 200hrs (199.2 rounded up) x $1.803</code></p> <p><code>$360.60</code></p> </li> </ul> <p>OpenStack CPU SU Example:</p> <ul> <li> <p>A Project or VM with:</p> <p><code>3 vCPU, 20 GiB RAM, 720hrs (24hr x 30days)</code></p> </li> <li> <p>Will be charged:</p> <p><code>5 CPU SUs due to the extra RAM (20GiB vs. 12GiB(3 x 4GiB)) x 720hrs x $0.013</code></p> <p><code>$46.80</code></p> </li> </ul> <p>Are VMs invoiced even when shut down?</p> <p>Yes, VMs incur charges as long as they are utilizing resources. Proactively managing your VMs helps optimize usage and reduce unnecessary costs. To avoid being billed for unused resources (i.e., GPU, vCPU, RAM), you can release the underlying compute resources in one of the following two ways:</p> <ol> <li>By Shelving the VM:</li> </ol> <p>Shelving temporarily shuts down the VM and releases all its compute resources    (i.e., GPU, vCPU, RAM), while preserving the disk and metadata. This allows    you to resume the VM later without needing to reconfigure it. It's a cost-effective    (Recommended) option if you plan to use the VM again in the future.</p> <ol> <li>By Deleting the VM:</li> </ol> <p>Deleting the VM    permanently removes it along with all associated resources, including compute,    storage, and network allocations. Choose this option if the VM is no longer    needed, as it fully eliminates any future charges.</p> <p>It is advisable to create a snapshot    of your VM prior to deletion to ensure you have a backup of your data and    configurations. We strongly recommend detaching any additional volumes    from your instance before creating any snapshots.</p> <p>Please note: The storage cost is determined by your requested and approved allocation values for the storage quotas defined under \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" in your NERC (OpenStack) Resource Allocations.</p> <p>Even if you have deleted all volumes, snapshots, and object storage buckets and objects in your OpenStack project, it's essential to adjust the approved storage values for your NERC (OpenStack) allocation to zero (0). Otherwise, charges will continue to apply based on the approved storage quota.</p> <p>You can easily scale or reduce your current resource allocations within your project. Follow this guide to request changes using NERC's ColdFront interface.</p> <p>For common questions or additional information, please refer to our Billing FAQs.</p> <p>OpenShift CPU SU Example:</p> <ul> <li> <p>Project with 3 Pods with:</p> <p>i. <code>1 vCPU, 3 GiB RAM, 720hrs (24hr*30days)</code></p> <p>ii. <code>0.1 vCPU, 8 GiB RAM, 720hrs (24hr*30days)</code></p> <p>iii. <code>2 vCPU, 4 GiB RAM, 720hrs (24hr*30days)</code></p> </li> </ul> <ul> <li> <p>Project Will be charged:</p> <p><code>RoundUP(Sum(</code></p> <p><code>1 CPU SUs due to first pod * 720hrs * $0.013</code></p> <p><code>2 CPU SUs due to extra RAM (8GiB vs 0.4GiB(0.1*4GiB)) * 720hrs * $0.013</code></p> <p><code>2 CPU SUs due to more CPU (2vCPU vs 1vCPU(4GiB/4)) * 720hrs * $0.013</code></p> <p><code>))</code></p> <p><code>=RoundUP(Sum(720(1+2+2)))*0.013</code></p> <p><code>$46.80</code></p> </li> </ul> <p>How to calculate cost for all running OpenShift pods?</p> <p>If you prefer a function for the OpenShift pods here it is:</p> <p><code>Project SU HR count = RoundUP(SUM(Pod1 SU hour count + Pod2 SU hr count + ...))</code></p> <p>OpenShift Pods are summed up to the project level so that fractions of CPU/RAM that some pods use will not get overcharged. There will be a split between CPU and GPU pods, as GPU pods cannot currently share resources with CPU pods.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#storage","title":"Storage","text":"<p>Storage is charged separately at a rate of $0.009 TiB/hr</p> <p>OpenStack volumes remain provisioned until they are deleted. VM's reserve volumes, and you can also create extra volumes yourself. In OpenShift pods, storage is only provisioned while it is active, and in persistent volumes, storage remains provisioned until it is deleted.</p> <p>Very Important: Requested/Approved Allocated Storage Quota and Cost</p> <p>The Storage cost is determined by your requested and approved allocation values. Once approved, these Storage quotas will need to be reserved from the total NESE storage pool for both NERC (OpenStack) and NERC-OCP (OpenShift) resources. For NERC (OpenStack) Resource Allocations, storage quotas are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes. Whereas for NERC-OCP (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes.</p> <p>Even if you have deleted all volumes, snapshots, and object storage buckets and objects in your OpenStack and OpenShift projects. It is very essential to adjust the approved values for your NERC (OpenStack) and NERC-OCP (OpenShift) resource allocations to zero (0) otherwise you will still be incurring a charge for the approved storage as explained in Billing FAQs.</p> <p>Keep in mind that you can easily scale and expand your current resource allocations within your project. Follow this guide on how to use NERC's ColdFront to reduce your Storage quotas for NERC (OpenStack) allocations and this guide for NERC-OCP (OpenShift) allocations.</p> <p>Storage Example 1:</p> <ul> <li> <p>Volume or VM with:</p> <p><code>500GiB for 699.2hrs</code></p> </li> <li> <p>Will be charged:</p> <p><code>.5 Storage TiB SU (.5 TiB x 700hrs) x $0.009 TiB/hr</code></p> <p><code>$3.15</code></p> </li> </ul> <p>Storage Example 2:</p> <ul> <li> <p>Volume or VM with:</p> <p><code>10TiB for 720hrs (24hr x 30days)</code></p> </li> <li> <p>Will be charged:</p> <p><code>10 Storage TiB SU (10TiB x 720 hrs) x $0.009 TiB/hr</code></p> <p><code>$64.80</code></p> </li> </ul> <p>Storage includes all types of storage Object, Block, Ephemeral &amp; Image.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#high-level-function","title":"High-Level Function","text":"<p>To provide a more practical way to calculate your usage, here is a function of how the calculation works for OpenShift and OpenStack.</p> <ol> <li> <p>OpenStack = (Resource (vCPU/RAM/GPU) assigned to VM flavor converted to     number of equivalent SUs) * (time VM has been running), rounded up to a whole     hour + Extra storage.</p> <p>NERC's OpenStack Flavor List</p> <p>You can find the most up-to-date information on the current NERC's OpenStack flavors with corresponding SUs by referring to this page.</p> </li> <li> <p>OpenShift = (Resource (vCPU/RAM) requested by Pod converted to the number     of SU) * (time Pod was running), summed up to project level rounded up to the     whole hour.</p> </li> </ol>"},{"location":"get-started/cost-billing/how-pricing-works/#where-can-i-view-my-current-usage-invoice","title":"Where Can I View My Current Usage Invoice?","text":"<p>Using NERC's ColdFront interface, now makes it easier to keep track of your daily usage charges for each allocation. A new allocation attribute <code>Cumulative Daily Charges for Month</code> is available directly within the Allocation Detail page, giving you a transparent, day-by-day view of your monthly costs.</p> <p>Steps:</p> <ol> <li> <p>Log in to the NERC ColdFront interface using the same institutional authentication method you used when registering with NERC via RegApp.</p> </li> <li> <p>Navigate to Projects and open the project associated with your allocation.</p> </li> <li> <p>Select the specific Allocation you want to review.</p> </li> <li> <p>In the Allocation Detail page, look for the Allocation Attribute labeled: <code>Cumulative Daily Charges for Month</code> as shown below:</p> </li> </ol> <p></p>"},{"location":"get-started/cost-billing/how-pricing-works/#understanding-the-cumulative-daily-charges-for-month-attribute-format","title":"Understanding the \"Cumulative Daily Charges for Month\" Attribute Format","text":"<p>The <code>Cumulative Daily Charges for Month</code> allocation attribute stores daily running totals in a simple, date-indexed (UTC) format, along with the total cost in USD.</p> <pre><code>YYYY-MM-DD: TOTAL USD\n</code></pre> <p>Each entry reflects the total accumulated billable charges for that month as of the end of that specific day. These values are refreshed nightly by a scheduled CronJob, ensuring that the cumulative totals remain up to date.</p> <p>For example:</p> <pre><code>2025-12-07: 123.00 USD\n</code></pre> <p>This indicates that as of December 7, 2025, the total billable usage for that allocation for the month has reached 123.00 USD.</p> <p>Cumulative Daily Charges for Month</p> <p>The value of this quota attribute is updated nightly to incorporate the latest available invoice data, ensuring daily cumulative totals remain current.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#how-to-use-this-information","title":"How to Use this Information","text":"<p>You can use the daily cumulative totals to:</p> <ul> <li> <p>Monitor your month-to-date spending with near real-time visibility.</p> </li> <li> <p>Identify any unusual spikes or sudden changes in usage.</p> </li> <li> <p>Plan workloads and manage your budget more effectively.</p> </li> <li> <p>Communicate usage patterns and trends to your team or project members.</p> </li> </ul>"},{"location":"get-started/cost-billing/how-pricing-works/#monthly-allocation-alert-emails","title":"Monthly Allocation Alert Emails","text":"<p>In addition, a new Allocation Attribute labeled Monthly Allocation Cost Alert is now available to provide automated cost alert notifications for Project PIs and Manager(s), as shown below:</p> <p></p> <p>This attribute helps users monitor potential budget overruns by allowing them to set a threshold amount in USD. When the cumulative charges for the month exceed this value, email alerts are automatically sent to the Project PIs and Manager(s).</p> <p>Important Note</p> <p>By default, the <code>Monthly Allocation Cost Alert</code> allocation attribute is set to <code>0</code> for all existing allocations. In this state, no email notifications will be sent. To enable alerts, Project PIs or Manager(s) must set a preferred threshold value (in USD) for this attribute by submitting a Change Request.</p> <p>If an allocation has the <code>Monthly Allocation Cost Alert</code> attribute configured, ColdFront will:</p> <ul> <li> <p>Monitor the total billable cost for the month to maintain better control over     spending and ensure proper oversight.</p> </li> <li> <p>Send alert email notifications to the Project PI and Manager(s) when the     allocation's monthly charges exceed the configured threshold.</p> </li> <li> <p>Ensure Project PIs and Manager(s) are promptly informed of over-usage through     automated email alerts, eliminating the need to wait until the end of the     monthly billing cycle to review charges.</p> </li> </ul>"},{"location":"get-started/cost-billing/how-pricing-works/#how-to-pay","title":"How to Pay?","text":"<p>To ensure a comprehensive understanding of the billing process and payment options for NERC offerings, we advise PIs/Managers to visit individual pages designated for each institution. These pages provide detailed information specific to each organization's policies and procedures regarding their billing. By exploring these dedicated pages, you can gain insights into the preferred payment methods, invoicing cycles, breakdowns of cost components, and any available discounts or offers. Understanding the institution's unique approach to billing ensures accurate planning, effective financial management, and a transparent relationship with us.</p> <p>If you have any some common questions or need further information, see our Billing FAQs for comprehensive answers.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#su-conservation-how-to-save-cost","title":"SU Conservation - How to Save Cost?","text":"<p>With SUs being the primary metric for resource consumption, it's crucial to actively manage your workloads when they're not in use.</p> <p>Below are practical ways to conserve SUs across different NERC services:</p>"},{"location":"get-started/cost-billing/how-pricing-works/#nerc-openstack","title":"NERC OpenStack","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate: Project -&gt; Compute -&gt; Instances.</p> <p>After launching an instance (On the left side bar, click on Project -&gt; Compute -&gt; Instances), several options are available under the Actions menu located on the right hand side of your screen as shown here:</p> <p></p> <p>Shelve your VM when not in use:</p> <p>Only Available From Next Billing Cycle</p> <p>We will implement the invoicing piece of this feature as of the June 2025 Invoicing cycle.</p> <p>In NERC OpenStack, if your VM does not need to run continuously, you can shelve it to free up consumed resources such as vCPUs, RAM, and disk. This action releases all allocated resources while preserving the VM's state and metadata.</p> <ul> <li> <p>Click Action -&gt; Shelve Instance.</p> </li> <li> <p>Releases all computing resources (i.e., GPU, vCPU, RAM), while preserving   the disk and metadata.</p> </li> <li> <p>We strongly recommend detaching volumes before shelving.</p> </li> <li> <p>Status will change to <code>Shelved Offloaded</code>.</p> </li> </ul> <p>You can later unshelve the VM without needing to recreate it - allowing you to reduce costs without losing any progress.</p> <ul> <li>To unshelve the instance, click Action -&gt; Unshelve Instance.</li> </ul> <p>For more details on shelving a VM, see the explanation here.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#nerc-openshift","title":"NERC OpenShift","text":"<p>Scale your pods to 0 replicas:</p> <p>In NERC OpenShift, if your application or job is idle, you can scale its pod replica count to 0. This effectively frees up compute resources (i.e., GPU, vCPU, and RAM) while retaining the configuration, metadata, environment settings, and persistent volume claims (PVCs) for future use.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#using-web-console","title":"Using Web Console","text":"<ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>In the Navigation Menu, navigate to the Workloads -&gt; Topology menu.</p> </li> <li> <p>Click the pod or application you want to scale to see the Overview panel to     the right.</p> </li> <li> <p>In the Details tab (usually the default tab when you open the deployment):</p> </li> <li> <p>Look for the Pod count or Replicas section.</p> </li> <li> <p>Use the up/down arrows next to the number to adjust the replica count.</p> </li> <li> <p>Set it to 0 by clicking down arrow as shown below:</p> <p></p> </li> <li> <p>OpenShift will automatically scale down the pods to 0.</p> </li> </ol> <p>When you need to run your application again, you can scale up the pod count or replicas to reclaim the necessary resources.</p>"},{"location":"get-started/cost-billing/how-pricing-works/#using-the-openshift-oc-cli","title":"Using the OpenShift <code>oc</code> CLI","text":""},{"location":"get-started/cost-billing/how-pricing-works/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Install and configure the OpenShift CLI (oc), see How to Setup the     OpenShift CLI Tools     for more information.</p> <p>Information</p> <p>Some users may have access to multiple projects. Run the following command to switch to a specific project space: <code>oc project &lt;your-project-namespace&gt;</code>.</p> <p>Please confirm the correct project is being selected by running <code>oc project</code>, as shown below:</p> <pre><code>oc project\nUsing project \"&lt;your_openshift_project_where_pod_deployed&gt;\" on server \"https://api.shift.nerc.mghpcc.org:6443\".\n</code></pre> </li> </ul> <p>If your application or job is idle, you can scale your pod's replica count to 0 by running the following <code>oc</code> command:</p> <pre><code>oc scale deployment &lt;your-deployment&gt; --replicas=0\n</code></pre> <p>When you need to run your application again, you can scale up the pod count or replicas to reclaim the necessary resources by running:</p> <pre><code>oc scale deployment &lt;your-deployment&gt; --replicas=1\n</code></pre>"},{"location":"get-started/cost-billing/how-pricing-works/#nerc-rhoai","title":"NERC RHOAI","text":"<p>Toggle the Workbench to \"Stopped\":</p> <p>In NERC Red Hat OpenShift AI (RHOAI), workbench environments can be toggled between <code>Running</code> and <code>Stopped</code> states.</p> <ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>After logging in to the NERC OpenShift console, access the NERC's Red Hat OpenShift AI dashboard by clicking the application launcher icon (the black-and-white icon that looks like a grid), located on the header.</p> </li> <li> <p>When you've completed a workload such as model development or experimentation using the Data Science Project (DSP) Workbench, you can stop the compute resources used by the workbench by clicking <code>Stop</code>, next to the Status column for the workbench. Then the status of the workbench change from <code>Running</code> to <code>Stopped</code>, as shown below:</p> <p></p> <p>This action immediately releases the compute resources allocated to the notebook environment within the Workbench setup.</p> <p>To restart your workbench, click <code>Start</code> next to the Status column for the workbench. The status will change from Stopped to <code>Starting</code> while the server initializes, and then to <code>Running</code> once the workbench has successfully started.</p> </li> </ol>"},{"location":"get-started/cost-billing/nerc-pricing-calculator/","title":"NERC Pricing Calculator","text":""},{"location":"get-started/cost-billing/nerc-pricing-calculator/#nerc-pricing-calculator","title":"NERC Pricing Calculator","text":"<p>The NERC Pricing Calculator is a google excel based tool for estimating the cost of utilizing various NERC resources in different NERC service offerings. It offers a user-friendly interface, allowing users to input their requirements and customize configurations to generate accurate and tailored cost estimates for optimal budgeting and resource allocation.</p> <p>Start your estimate with no commitment, and explore NERC services and pricing for your research needs by using this online tool.</p> <p>How to use the NERC Pricing Calculator?</p> <p>Please note that you must make a copy of this tool before estimating any costs. Once copied, you can update the resource-type columns in your own working sheet, and the tool will automatically calculate your potential Service Units (SU), rates, and costs per hour, month, and year.</p> <p>The tool contains five sheets at the bottom, as shown below:</p> <p></p> <p>If you want to estimate costs based on the available NERC OpenStack flavors (which define the compute, memory, and storage capacity for your dedicated instances), you can select and use the second sheet titled \"OpenStack Flavor\".</p> <p>For estimating the NERC OpenShift resource usage, use the first sheet titled \"OpenShift SU\" and enter the pod-specific resource requests in each row. If your application scales to multiple pods, add a separate row for each replica.</p> <p>For the Storage cost calculation, use the third sheet titled \"Calculate Storage\".</p> <p>For the BareMetal cost estimation, use the fourth sheet titled \"BM SU\".</p> <p>Finally, the combined overall cost will be displayed on the last sheet titled \"Total Cost\".</p> <p>For more information about how NERC pricing works, see How does NERC pricing work and to know more about billing process for your own institution, see Billing Process for My Institution.</p>"},{"location":"get-started/cost-billing/pricing-for-bare-metal-machines/","title":"Pricing for Bare Metal Machines on NERC","text":""},{"location":"get-started/cost-billing/pricing-for-bare-metal-machines/#pricing-for-bare-metal-bm-machines-on-nerc","title":"Pricing for Bare Metal (BM) Machines on NERC","text":""},{"location":"get-started/cost-billing/pricing-for-bare-metal-machines/#bare-metal-machines","title":"Bare Metal Machines","text":"<p>Very Important Disclosures Regarding Pricing</p> <p>All prices listed on this page are introductory and subject to change without notice. Please stay informed by reviewing this page regularly or contacting us by emailing us at help@esi.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System by selecting \"ESI Inquiry\" as the Help Topic option as shown below:</p> <p></p> <p>Bare metal instances pricing is currently available as a limited offering.</p>"},{"location":"get-started/cost-billing/pricing-for-bare-metal-machines/#bare-metal-pricing-calculations","title":"Bare Metal Pricing Calculations","text":"Name GPU vCPU RAM (GiB) Current Price H100 GPU 4 512 1536 $16 A100sxm4 GPU 4 128 1024 $8.312 FC830 0 176 2048 $3.97 FC430 0 40 128 $0.75 <p>How to request Bare Metal Machines?</p> <p>Interested parties who wish to explore this option are encouraged to send detailed requests, including:</p> <ul> <li> <p>Intended Use Case</p> </li> <li> <p>Expected Workload</p> </li> <li> <p>Funding Resource (Optional)</p> </li> </ul> <p>This information will help us evaluate the demand and feasibility of expanding bare metal offerings in the future.</p> <p>For inquiries or to submit your request, please contact us to discuss it by emailing us at help@esi.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System by selecting \"ESI Inquiry\" as the Help Topic option.</p>"},{"location":"get-started/cost-billing/pricing-for-bare-metal-machines/#bm-su-example","title":"BM SU Example","text":"<ul> <li> <p>A Project with: 1 H100, 1 A100SXM4, 2 FC830s, and 3 FC430s</p> <p><code>1 H100 Server, 720hrs (24hr*30days)</code></p> <p><code>1 A100SXM4 Server, 720hrs (24hr*30days)</code></p> <p><code>2 FC830 Servers, 720hrs (24hr*30days)</code></p> <p><code>3 FC430 Servers, 720hrs (24hr*30days)</code></p> </li> <li> <p>Will be charged:</p> </li> </ul> <pre><code>1 H100 SU * 720hrs * $24.16 = $11,520.00\n\n1 A100SXM4 * 720hrs * $8.312 = $5,984.64\n\n2 FC830 SU * 720hrs * $3.97 = $5,716.80\n\n3 FC430 SU * 720hrs * $0.75 = $1,620.00\n</code></pre> <p>Total charge = $24,841.44</p>"},{"location":"migration-moc-to-nerc/Step1/","title":"Creating NERC Project and Networks","text":""},{"location":"migration-moc-to-nerc/Step1/#creating-nerc-project-and-networks","title":"Creating NERC Project and Networks","text":"<p>This process includes some waiting for emails and approvals. It is advised to start this process and then move to Step 2 and continue with these steps once you recieve approval.</p>"},{"location":"migration-moc-to-nerc/Step1/#account-creation-quota-request","title":"Account Creation &amp; Quota Request","text":"<ol> <li> <p>Register for your new NERC account    here.</p> <ol> <li>Wait for an approval email.</li> </ol> </li> <li> <p>Register to be a PI for a NERC account    here.</p> <ol> <li>Wait for an approval email.</li> </ol> </li> <li> <p>Request the quota necessary for all of your MOC Projects to be added    to NERC here    (link also in PI approval email).</p> <p></p> <ol> <li> <p>Log in with your institution login by clicking on    Log in via OpenID Connect (highlighted in yellow above).</p> <p></p> </li> <li> <p>Under Projects &gt;&gt; Click on the name of your project    (highlighted in yellow above).</p> <p></p> </li> <li> <p>Scroll down until you see Request Resource Allocation    (highlighted in yellow above) and click on it.</p> <p></p> </li> <li> <p>Fill out the Justification (highlighted in purple above) for    the quota allocation.</p> </li> <li> <p>Using your \"MOC Instance information\" table you gathered from your MOC    project calculate the total number of Instances, VCPUs, RAM and use your    \"MOC Volume Information\" table to calculate Disk space you will need.</p> </li> <li> <p>Using the up and down arrows (highlighted in yellow above) or by    entering the number manually select the multiple of 1 Instance, 2 vCPUs,    0 GPUs, 4GB RAM, 2 Volumes and 100GB Disk and 1GB Object Storage that you    will need.</p> <ol> <li>For example if I need 2 instances 2 vCPUs, 3GB RAM, 3 Volumes and    30GB of storage I would type in 2 or click the up arrow once to select    2 units.</li> </ol> </li> <li> <p>Click Submit (highlighted in green above).</p> </li> </ol> </li> <li> <p>Wait for your allocation approval email.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step1/#setup","title":"Setup","text":""},{"location":"migration-moc-to-nerc/Step1/#login-to-the-dashboard","title":"Login to the Dashboard","text":"<ol> <li> <p>Log into the    NERC OpenStack Dashboard    using your OpenID Connect password.</p> <p></p> <ol> <li> <p>Click Connect.</p> <p></p> </li> <li> <p>Select your institution from the drop down (highlighted in yellow    above).</p> </li> <li> <p>Click Log On (highlighted in purple).</p> </li> <li> <p>Follow your institution's log on instructions.</p> </li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step1/#setup-nerc-network","title":"Setup NERC Network","text":"<ol> <li> <p>You are then brought to the Project&gt;Compute&gt;Overview location of    the Dashboard.</p> <p></p> <ol> <li> <p>This will look very familiar as the MOC and NERC Dashboard are quite    similar.</p> </li> <li> <p>Follow the instructions    here    to set up your network/s (you may also use the default_network    if you wish).</p> <ol> <li>The networks don't have to exactly match the MOC. You only need the    networks for creating your new instances (and accessing them once we    complete the migration).</li> </ol> </li> <li> <p>Follow the instructions    here    to set up your router/s (you may also use the default_router if you wish).</p> </li> <li> <p>Follow the instructions    here    to set up your Security Group/s.</p> <ol> <li>This is where you can use your \"MOC Security Group Information\"    table to create similar Security Groups to the ones you had in the MOC.</li> </ol> </li> <li> <p>Follow the instructions    here    to set up your SSH Key-pair/s.</p> </li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/","title":"Identify Volumes, Instances &amp; Security Groups on the MOC that need to be Migrated to the NERC","text":""},{"location":"migration-moc-to-nerc/Step2/#identify-volumes-instances-security-groups-on-the-moc-that-need-to-be-migrated-to-the-nerc","title":"Identify Volumes, Instances &amp; Security Groups on the MOC that need to be Migrated to the NERC","text":"<p>Please read the instructions in their entirety before proceeding. Allow yourself enough time to complete them.</p> <p>Volume Snapshots will not be migrated. If you have a Snapshot you wish to backup please \"Create Volume\" from it first.</p>"},{"location":"migration-moc-to-nerc/Step2/#confirm-access-and-login-to-moc-dashboard","title":"Confirm Access and Login to MOC Dashboard","text":"<ol> <li>Go to the MOC Dashboard.</li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#sso-google-login","title":"SSO / Google Login","text":"<ol> <li> <p>If you have SSO through your Institution or google select    Institution Account from the dropdown.</p> <p></p> </li> <li> <p>Click Connect.</p> </li> <li> <p>Click on University Logins (highlighted in yellow below)    if you are using SSO with your Institution.</p> <p></p> <ol> <li>Follow your Institution's login steps after that, and skip to    Gathering MOC information for the    Migration.</li> </ol> </li> <li> <p>Click Google (highlighted in purple above) if your SSO    is through Google.</p> <ol> <li>Follow standard Google login steps to get in this    way, and skip to Gathering MOC information for the    Migration.</li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#keystone-credentials","title":"Keystone Credentials","text":"<ol> <li> <p>If you have a standard login and password leave the dropdown    as Keystone Credentials.</p> <p></p> </li> <li> <p>Enter your User Name.</p> </li> <li> <p>Enter your Password.</p> </li> <li> <p>Click Connect.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#dont-know-your-login","title":"Don't know your login?","text":"<ol> <li> <p>If you do not know your login information please create a    Password Reset ticket.</p> <p></p> </li> <li> <p>Click Open a New Ticket (highlighted in yellow above).</p> <p></p> </li> <li> <p>Click the dropdown and select Forgot Pass &amp; SSO Account    Link (highlighted in blue above).</p> </li> <li> <p>In the text field (highlighted in purple above) provide    the Institution email, project you are working on and the email    address you used to create the account.</p> </li> <li> <p>Click Create Ticket (highlighted in yellow above) and    wait for the pinwheel.</p> </li> <li> <p>You will receive an email to let you know that the MOC support    staff will get back to you.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#gathering-moc-information-for-the-migration","title":"Gathering MOC information for the Migration","text":"<ol> <li> <p>You are then brought to the Project&gt;Compute&gt;Overview location of the    Dashboard.</p> <p></p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#create-tables-to-hold-your-information","title":"Create Tables to hold your information","text":"<p>Create 3 tables of all of your Instances, your Volumes and Security Groups, for example, if you have 2 instances, 3 volumes and 2 Security Groups like the samples below your lists might look like this:</p>"},{"location":"migration-moc-to-nerc/Step2/#moc-instance-information-table","title":"MOC Instance Information Table","text":"Instance Name MOC VCPUs MOC Disk MOC RAM MOC UUID Fedora_test 1 10GB 1GB 16a1bfc2-8c90-4361-8c13-64ab40bb6207 Ubuntu_Test 1 10GB 2GB 6a40079a-59f7-407c-9e66-23bc5b749a95 total 2 20GB 3GB"},{"location":"migration-moc-to-nerc/Step2/#moc-volume-information-table","title":"MOC Volume Information Table","text":"MOC Volume Name MOC Disk MOC Attached To Bootable MOC UUID NERC Volume Name Fedora 10GiB Fedora_test Yes ea45c20b-434a-4c41-8bc6-f48256fc76a8 9c73295d-fdfa-4544-b8b8-a876cc0a1e86 10GiB Ubuntu_Test Yes 9c73295d-fdfa-4544-b8b8-a876cc0a1e86 Snapshot of Fed_Test 10GiB Fedora_test No ea45c20b-434a-4c41-8bc6-f48256fc76a8 total 30GiB"},{"location":"migration-moc-to-nerc/Step2/#moc-security-group-information-table","title":"MOC Security Group Information Table","text":"Security Group Name Direction Ether Type IP Protocol Port Range Remote IP Prefix ssh_only_test Ingress IPv4 TCP 22 0.0.0.0/0 ping_only_test Ingress IPv4 ICMP Any 0.0.0.0/0"},{"location":"migration-moc-to-nerc/Step2/#gather-the-instance-information","title":"Gather the Instance Information","text":"<p>Gather the Instance UUIDs (of only the instances that you need to migrate to the NERC).</p> <ol> <li> <p>Click    Instances    (highlighted in pink in image above)</p> <p></p> </li> <li> <p>Click the Instance Name (highlighted in Yellow above) of the first    instance you would like to gather data on.</p> <p></p> </li> <li> <p>Locate the ID row (highlighted in green above) and copy and save the ID    (highlighted in purple above).</p> <ol> <li>This is the UUID of your first Instance.</li> </ol> </li> <li> <p>Locate the RAM, VCPUs &amp; Disk rows (highlighted in yellow) and copy and    save the associated values (highlighted in pink).</p> </li> <li> <p>Repeat this section for each    Instance you have.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#gather-the-volume-information","title":"Gather the Volume Information","text":"<p>Gather the Volume UUIDs (of only the volumes that you need to migrate to the NERC).</p> <p></p> <ol> <li> <p>Click Volumes dropdown.</p> </li> <li> <p>Select Volumes    (highlighted in purple above).</p> <p></p> </li> <li> <p>Click the Volume Name (highlighted in yellow above) of the first    volume you would like to gather data on.</p> <ol> <li> <p>The name might be the same as the ID (highlighted in blue above).</p> <p></p> </li> </ol> </li> <li> <p>Locate the ID row (highlighted in green above) and copy and save the ID    (highlighted in purple above).</p> <ol> <li>This is the UUID of your first Volume.</li> </ol> </li> <li> <p>Locate the Size row (highlighted in yellow above) and copy and save    the Volume size (highlighted in pink above).</p> </li> <li> <p>Locate the Bootable row (highlighted in gray above) and copy and save    the Volume size (highlighted in red above).</p> </li> <li> <p>Locate the Attached To row (highlighted in blue above) and copy and save    the Instance this Volume is attached to (highlighted in orange above).</p> <ol> <li>If the volume is not attached to an image it will state    \"Not attached\".</li> </ol> </li> <li> <p>Repeat this section for each Volume    you have.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step2/#gather-your-security-group-information","title":"Gather your Security Group Information","text":"<p>If you already have all of your Security Group information outside of the OpenStack Dashboard skip to the section.</p> <p>Gather the Security Group information (of only the security groups that you need to migrate to the NERC).</p> <p></p> <ol> <li> <p>Click Network dropdown</p> </li> <li> <p>Click    Security    Groups (highlighted in yellow above).</p> <p></p> </li> <li> <p>Click Manage Rules (highlighted in yellow above) of the first    Security Group you would like to gather data on.</p> <p></p> </li> <li> <p>Ignore the first 2 lines (highlighted in yellow above).</p> </li> <li> <p>Write down the important information for all lines after (highlighted in    blue above).</p> <ol> <li>Direction, Ether Type, IP Protocol, Port Range, Remote IP Prefix,    Remote Security Group.</li> </ol> </li> <li> <p>Repeat this section    for each security group you have.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/","title":"Steps to Migrate Volumes from MOC to NERC","text":""},{"location":"migration-moc-to-nerc/Step3/#steps-to-migrate-volumes-from-moc-to-nerc","title":"Steps to Migrate Volumes from MOC to NERC","text":""},{"location":"migration-moc-to-nerc/Step3/#create-a-spreadsheet-to-track-the-values-you-will-need","title":"Create a spreadsheet to track the values you will need","text":"<ol> <li> <p>The values you will want to keep track of are.</p> Label Value MOCAccess MOCSecret NERCAccess NERCSecret MOCEndPoint <code>https://kzn-swift.massopen.cloud</code> NERCEndPoint <code>https://stack.nerc.mghpcc.org:13808</code> MinIOVolume MOCVolumeBackupID ContainerName NERCVolumeBackupID NERCVolumeName </li> <li> <p>It is also helpful to have a text editor open so that you can insert    the values from the spreadsheet into the commands that need to be run.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#create-a-new-moc-mirror-to-nerc-instance","title":"Create a New MOC Mirror to NERC Instance","text":"<ol> <li> <p>Follow the instructions    here    to set up your instance.</p> <p></p> <ol> <li> <p>When selecting the Image please select moc-nerc-migration    (highlighted in yellow above).</p> </li> <li> <p>Once the Instance is Running move onto the next step</p> </li> </ol> </li> <li> <p>Name your new instance something you will remember, <code>MirrorMOC2NERC</code>    for example.</p> </li> <li> <p>Assign a Floating IP to your new instance. If you need assistance please    review the Floating IP steps here.</p> <ol> <li>Your floating IPs will not be the same as the ones you had in the    MOC. Please claim new floating IPs to use.</li> </ol> </li> <li> <p>SSH into the MirrorMOC2NERC Instance. The user to use for login is <code>centos</code>.    If you have any trouble please review the SSH steps here.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#setup-application-credentials","title":"Setup Application Credentials","text":""},{"location":"migration-moc-to-nerc/Step3/#gather-moc-application-credentials","title":"Gather MOC Application Credentials","text":"<ol> <li> <p>Follow the instructions here to create your Application    Credentials.</p> <ol> <li>Make sure to save the <code>clouds.yaml</code> as <code>clouds_MOC.yaml</code>.</li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#gathering-nerc-application-credentials","title":"Gathering NERC Application Credentials","text":"<ol> <li> <p>Follow the instructions under the header Command Line setup here to create your Application Credentials.</p> <ol> <li>Make sure to save the <code>clouds.yaml</code> as <code>clouds_NERC.yaml</code>.</li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#combine-the-two-cloudsyaml-files","title":"Combine the two clouds.yaml files","text":"<ol> <li> <p>Make a copy of <code>clouds_MOC.yaml</code> and save as <code>clouds.yaml</code></p> </li> <li> <p>Open <code>clouds.yaml</code> in a text editor of your choice.</p> <p></p> <ol> <li>Change the <code>openstack</code> (highlighted in yellow above) value to <code>moc</code>    (highlighted in yellow two images below).</li> </ol> </li> <li> <p>Open <code>clouds_NERC.yaml</code> in a text editor of your choice.</p> <p></p> <ol> <li> <p>Change the <code>openstack</code> (highlighted in yellow above) value to <code>nerc</code>    (highlighted in green below).</p> </li> <li> <p>Highlight and copy everything from nerc to the end of the line that    starts with auth_type</p> <p></p> </li> <li> <p>Paste the copied text into <code>clouds.yaml</code> below the line that starts    with auth_type. Your new <code>clouds.yaml</code> will look similar to the image    above.</p> </li> </ol> </li> <li> <p>For further instructions on <code>clouds.yaml</code> files go    Here.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#moving-application-credentials-to-vm","title":"Moving Application Credentials to VM","text":"<ol> <li> <p>SSH into the VM created at the top of this page for example <code>MirrorMOC2NERC</code>.</p> </li> <li> <p>Create the openstack config folder and empty <code>clouds.yaml</code> file.</p> <pre><code>mkdir -p ~/.config/openstack\ncd ~/.config/openstack\ntouch clouds.yaml\n</code></pre> </li> <li> <p>Open the <code>clouds.yaml</code> file in your favorite text editor.     (vi is preinstalled).</p> </li> <li> <p>Copy the entire text inside the <code>clouds.yaml</code> file on your local computer.</p> </li> <li> <p>Paste the contents of the local <code>clouds.yaml</code> file into the <code>clouds.yaml</code>     on the VM.</p> </li> <li> <p>Save and exit your VM text editor.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#confirm-the-instances-are-shut-down","title":"Confirm the Instances are Shut Down","text":"<ol> <li> <p>Confirm the instances are Shut Down. This is a very important step    because we will be using the force modifier when we make our backup. The    volume can become corrupted if the Instance is not in a Shut Down state.</p> </li> <li> <p>Log into the Instance page of the    MOC Dashboard</p> <p></p> </li> <li> <p>Check the Power State of all of the instances you plan to migrate volumes    from are set to Shut Down (highlighted in yellow in image above).</p> <ol> <li> <p>If they are not please do so from the Actions Column.</p> <p></p> <ol> <li> <p>Click the drop down arrow under actions.</p> </li> <li> <p>Select Shut Off Instance (blue arrow pointing to it in image    above).</p> </li> </ol> </li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#backup-and-move-volume-data-from-moc-to-nerc","title":"Backup and Move Volume Data from MOC to NERC","text":"<ol> <li>SSH into the VM created at the top of this page. For steps on how to do    this please see instructions here.</li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#create-ec2-credentials-in-moc-nerc","title":"Create EC2 credentials in MOC &amp; NERC","text":"<ol> <li> <p>Generate credentials for Kaizen with the command below.</p> <pre><code>openstack --os-cloud moc ec2 credentials create\n</code></pre> <p></p> <ol> <li>Copy the <code>access</code> (circled in red above) and <code>secret</code> (circled in blue    above) values into your table as <code>&lt;MOCAccess&gt;</code> and <code>&lt;MOCSecret&gt;</code>.</li> </ol> </li> <li> <p>Generate credentials for the NERC with the command below.</p> <pre><code>openstack --os-cloud nerc ec2 credentials create\n</code></pre> <p></p> <ol> <li>Copy the <code>access</code> (circled in red above) and <code>secret</code> (circled in blue    above) values into your table as as <code>&lt;NERCAccess&gt;</code>    and <code>&lt;NERCSecret&gt;</code>.</li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#find-object-store-endpoints","title":"Find Object Store Endpoints","text":"<ol> <li> <p>Look up information on the <code>object-store</code> service in MOC with the command     below.</p> <pre><code>openstack --os-cloud moc catalog show object-store -c endpoints\n</code></pre> <p></p> <ol> <li>If the value is different than <code>https://kzn-swift.massopen.cloud</code> copy    the base URL for this service (circled in red above).</li> </ol> </li> <li> <p>Look up information on the <code>object-store</code> service on NERC with the command     below.</p> <pre><code>openstack --os-cloud nerc catalog show object-store -c endpoints\n</code></pre> <p></p> <ol> <li>If the value is different than <code>https://stack.nerc.mghpcc.org:13808</code>    copy the base URL for this service (circled in red above).</li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#configure-minio-client-aliases","title":"Configure minio client aliases","text":"<ol> <li> <p>Create a MinIO alias for MOC using the base URL of the \"public\"     interface of the object-store service <code>&lt;MOCEndPoint&gt;</code> and the EC2 access key     (ex. <code>&lt;MOCAccess&gt;</code>) &amp; secret key (ex. <code>&lt;MOCSecret&gt;</code>) from your table.</p> <pre><code>$ mc alias set moc https://kzn-swift.massopen.cloud &lt;MOCAccess&gt; &lt;MOCSecret&gt;\nmc: Configuration written to `/home/centos/.mc/config.json`. Please update your access credentials.\n mc: Successfully created `/home/centos/.mc/share`.\nmc: Initialized share uploads `/home/centos/.mc/share/uploads.json` file.\nmc: Initialized share downloads `/home/centos/.mc/share/downloads.json` file.\nAdded `moc` successfully.\n</code></pre> </li> <li> <p>Create a MinIO alias for NERC using the base URL of the \"public\"     interface of the object-store service <code>&lt;NERCEndPoint&gt;</code> and the EC2 access key     (ex. <code>&lt;NERCAccess&gt;</code>) &amp; secret key (ex. <code>&lt;NERCSecret&gt;</code>) from your table.</p> <pre><code>$ mc alias set nerc https://stack.nerc.mghpcc.org:13808 &lt;NERCAccess&gt; &lt;NERCSecret&gt;\nAdded `nerc` successfully.\n</code></pre> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#backup-moc-volumes","title":"Backup MOC Volumes","text":"<ol> <li> <p>Locate the desired Volume UUID from the table you created in     Step 2 Gathering MOC Information.</p> </li> <li> <p>Add the first Volume ID from your table to the code below in the     <code>&lt;MOCVolumeID&gt;</code> field and create a Container Name to replace the     <code>&lt;ContainerName&gt;</code> field. Container Name should be easy to remember as well     as unique so include your name. Maybe something like <code>thomasa-backups</code>.</p> <pre><code>openstack --os-cloud moc volume backup create --force --container &lt;ContainerName&gt; &lt;MOCVolumeID&gt;\n+-------+---------------------+\n| Field | Value               |\n+-------+---------------------+\n| id    | &lt;MOCVolumeBackupID&gt; |\n| name  | None                |\n</code></pre> <ol> <li>Copy down your <code>&lt;MOCVolumeBackupID&gt;</code> to your table.</li> </ol> </li> <li> <p>Wait for the backup to become available. You can run the command below to     check on the status. If your volume is 25 or larger this might be a good time     to go get a warm beverage or lunch.</p> <pre><code>openstack --os-cloud moc volume backup list\n+---------------------+------+-------------+-----------+------+\n| ID                  | Name | Description | Status    | Size |\n+---------------------+------+-------------+-----------+------+\n| &lt;MOCVolumeBackupID&gt; | None | None        | creating  |   10 |\n...\nopenstack --os-cloud moc volume backup list\n+---------------------+------+-------------+-----------+------+\n| ID                  | Name | Description | Status    | Size |\n+---------------------+------+-------------+-----------+------+\n| &lt;MOCVolumeBackupID&gt; | None | None        | available |   10 |\n</code></pre> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#gather-minio-volume-data","title":"Gather MinIO Volume data","text":"<ol> <li>Get the volume information for future commands. Use the same     <code>&lt;ContainerName&gt;</code> from when you created the volume backup. It is worth     noting that this value shares the ID number with the VolumeID.<pre><code>$ mc ls moc/&lt;ContainerName&gt;\n[2022-04-29 09:35:16 EDT]     0B &lt;MinIOVolume&gt;/\n</code></pre> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#create-a-container-on-nerc","title":"Create a Container on NERC","text":"<ol> <li>Create the NERC container that we will send the volume to. Use     the same <code>&lt;ContainerName&gt;</code> from when you created the volume backup.<pre><code>$ mc mb nerc/&lt;ContainerName&gt;\nBucket created successfully `nerc/&lt;ContainerName&gt;`.\n</code></pre> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#mirror-the-volume-from-moc-to-nerc","title":"Mirror the Volume from MOC to NERC","text":"<ol> <li>Using the volume label from MinIO <code>&lt;MinIOVolume&gt;</code> and the <code>&lt;ContainerName&gt;</code>     for the command below you will kick off the move of your volume. This takes     around 30 sec per GB of data in your volume.<pre><code>$ mc mirror moc/&lt;ContainerName&gt;/&lt;MinIOVolume&gt; nerc/&lt;ContainerName&gt;/&lt;MinIOVolume&gt;\n...123a30e_sha256file:  2.61GB / 2.61GB [=========...=========] 42.15Mib/s 1m3s\n</code></pre> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#copy-the-backup-record-from-moc-to-nerc","title":"Copy the Backup Record from MOC to NERC","text":"<ol> <li> <p>Now that we've copied the backup data into the NERC environment, we need     to register the backup with the NERC backup service. We do this by copying     metadata from MOC. You will need the original <code>&lt;MOCVolumeBackupID&gt;</code> you used     to create the original Backup.</p> <pre><code>openstack --os-cloud moc volume backup record export -f value &lt;MOCVolumeBackupID&gt; &gt; record.txt\n</code></pre> </li> <li> <p>Next we will import the record into NERC.</p> <pre><code>openstack --os-cloud nerc volume backup record import -f value $(cat record.txt)\n&lt;NERCVolumeBackupID&gt;\nNone\n</code></pre> <ol> <li>Copy <code>&lt;NERCVolumeBackupID&gt;</code> value into your table.</li> </ol> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#create-an-empty-volume-on-nerc-to-receive-the-backup","title":"Create an Empty Volume on NERC to Receive the Backup","text":"<ol> <li>Create a volume in the NERC environment to receive the backup. This must be     the same size or larger than the original volume which can be changed by     modifying the <code>&lt;size&gt;</code> field. Remove the \"--bootable\" flag if you are not     creating a bootable volume. The <code>&lt;NERCVolumeName&gt;</code> field can be any name you     want, I would suggest something that will help you keep track of what instance     you want to attach it to. Make sure to fill in the table you created in     Step 2with the <code>&lt;NERCVolumeName&gt;</code>     value in the <code>NERC Volume Name</code> column.<pre><code>openstack --os-cloud nerc volume create --bootable --size &lt;size&gt; &lt;NERCVolumeName&gt;\n+---------------------+----------------+\n| Field               | Value          |\n+---------------------+----------------+\n| attachments         | []             |\n| availability_zone   | nova           |\n...\n| id                  | &lt;NERCVolumeID&gt; |\n...\n| size                | &lt;size&gt;         |\n+---------------------+----------------+\n</code></pre> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#restore-the-backup","title":"Restore the Backup","text":"<ol> <li> <p>Restore the Backup to the Volume you just created.</p> <pre><code>openstack --os-cloud nerc volume backup restore &lt;NERCVolumeBackupID&gt; &lt;NERCVolumeName&gt;\n</code></pre> </li> <li> <p>Wait for the volume to shift from <code>restoring-backup</code> to <code>available</code>.</p> <pre><code>openstack --os-cloud nerc volume list\n+----------------+------------+------------------+------+-------------+\n| ID             | Name       | Status           | Size | Attached to |\n+----------------+------------+------------------+------+-------------+\n| &lt;NERCVolumeID&gt; | MOC Volume | restoring-backup |    3 | Migration   |\nopenstack --os-cloud nerc volume list\n+----------------+------------+-----------+------+-------------+\n| ID             | Name       | Status    | Size | Attached to |\n+----------------+------------+-----------+------+-------------+\n| &lt;NERCVolumeID&gt; | MOC Volume | available |    3 | Migration   |\n</code></pre> </li> <li> <p>Repeat these Backup and Move Volume     Data     steps for each volume you need to migrate.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step3/#create-nerc-instances-using-moc-volumes","title":"Create NERC Instances Using MOC Volumes","text":"<ol> <li> <p>If you have volumes that need to be attached to an instance please follow    the next steps.</p> </li> <li> <p>Follow the instructions here to set up your instance/s.</p> <ol> <li> <p>Instead of using an Image for your Boot Source you will use a Volume    (orange arrow in image below).</p> <p></p> </li> <li> <p>Select the <code>&lt;NERCVolumeName&gt;</code> you created in step Create an Empty    Volume on NERC to Recieve the    Backup</p> </li> <li> <p>The Flavor will be important as this decides how much vCPUs, RAM,    and Disk this instance will consume of your total.</p> <ol> <li>If for some reason the earlier approved resource quota is not    sufficient you can request further quota by following    these steps.</li> </ol> </li> </ol> </li> <li> <p>Repeat this section    for each instance you need to create.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step4/","title":"Remove Volume Backups to Conserve Storage","text":""},{"location":"migration-moc-to-nerc/Step4/#remove-volume-backups-to-conserve-storage","title":"Remove Volume Backups to Conserve Storage","text":"<p>If you find yourself low on Volume Storage please follow the steps below to remove your old Volume Backups. If you are very low on space you can do this every time you finish copying a new volume to the NERC. If on the other hand you have plety of remaining space feel free to leave all of your Volume Backups as they are.</p> <ol> <li>SSH into the MirrorMOC2NERC Instance. The user to use for    login is <code>centos</code>. If you have any trouble please review the SSH steps    here.</li> </ol>"},{"location":"migration-moc-to-nerc/Step4/#check-remaining-moc-volume-storage","title":"Check Remaining MOC Volume Storage","text":"<ol> <li> <p>Log into the MOC Dashboard and go to Project &gt; Compute &gt;    Overview.</p> <p></p> </li> <li> <p>Look at the Volume Storage meter (highlighted in yellow in image above).</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step4/#delete-moc-volume-backups","title":"Delete MOC Volume Backups","text":"<ol> <li> <p>Gather a list of current MOC Volume Backups with the command below.</p> <pre><code>openstack --os-cloud moc volume backup list\n+---------------------+------+-------------+-----------+------+\n| ID                  | Name | Description | Status    | Size |\n+---------------------+------+-------------+-----------+------+\n| &lt;MOCVolumeBackupID&gt; | None | None        | available |   10 |\n</code></pre> </li> <li> <p>Only remove Volume Backups you are sure have been moved to the NERC.     with the command below you can delete Volume Backups.</p> <pre><code>openstack --os-cloud moc volume backup delete &lt;MOCVolumeBackupID&gt;\n</code></pre> </li> <li> <p>Repeat the MOC Volume Backup section for     all MOC Volume Backups you wish to remove.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step4/#delete-moc-container-containername","title":"Delete MOC Container <code>&lt;ContainerName&gt;</code>","text":"<p>Remove the Container created i.e. <code>&lt;ContainerName&gt;</code> on MOC side with a unique name during migration. Replace the <code>&lt;ContainerName&gt;</code> field with your own container name created during migration process:</p> <pre><code>openstack --os-cloud moc container delete --recursive &lt;ContainerName&gt;\n</code></pre> <p>Verify the <code>&lt;ContainerName&gt;</code> is removed from MOC:</p> <pre><code>openstack --os-cloud moc container list\n</code></pre>"},{"location":"migration-moc-to-nerc/Step4/#check-remaining-nerc-volume-storage","title":"Check Remaining NERC Volume Storage","text":"<ol> <li> <p>Log into the NERC Dashboard and go to Project &gt; Compute &gt;    Overview.</p> <p></p> </li> <li> <p>Look at the Volume Storage meter (highlighted in yellow in image above).</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step4/#delete-nerc-volume-backups","title":"Delete NERC Volume Backups","text":"<ol> <li> <p>Gather a list of current NERC Volume Backups with the command below.</p> <pre><code>openstack --os-cloud nerc volume backup list\n+---------------------+------+-------------+-----------+------+\n| ID                  | Name | Description | Status    | Size |\n+---------------------+------+-------------+-----------+------+\n| &lt;MOCVolumeBackupID&gt; | None | None        | available |   3  |\n</code></pre> </li> <li> <p>Only remove Volume Backups you are sure have been migrated to NERC Volumes.     Keep in mind that you might not have named the volume the same as on the MOC     so check your table from Step 2 to     confirm. You can confirm what Volumes you have on NERC with the following command.</p> <pre><code>openstack --os-cloud nerc volume list\n+----------------+------------------+--------+------+----------------------------------+\n| ID             | Name             | Status | Size | Attached to                      |\n+----------------+------------------+--------+------+----------------------------------+\n| &lt;NERCVolumeID&gt; | &lt;NERCVolumeName&gt; | in-use |    3 | Attached to MOC2NERC on /dev/vda |\n</code></pre> </li> <li> <p>To remove volume backups please use the command below.</p> <pre><code>openstack --os-cloud nerc volume backup delete &lt;MOCVolumeBackupID&gt;\n</code></pre> </li> <li> <p>Repeat the NERC Volume Backup section for     all NERC Volume Backups you wish to remove.</p> </li> </ol>"},{"location":"migration-moc-to-nerc/Step4/#delete-nerc-container-containername","title":"Delete NERC Container <code>&lt;ContainerName&gt;</code>","text":"<p>Remove the Container created i.e. <code>&lt;ContainerName&gt;</code> on NERC side with a unique name during migration to mirror the Volume from MOC to NERC. Replace the <code>&lt;ContainerName&gt;</code> field with your own container name created during migration process:</p> <pre><code>openstack --os-cloud nerc container delete --recursive &lt;ContainerName&gt;\n</code></pre> <p>Verify the <code>&lt;ContainerName&gt;</code> is removed from NERC:</p> <pre><code>openstack --os-cloud nerc container list\n</code></pre>"},{"location":"openshift/","title":"OpenShift","text":""},{"location":"openshift/#openshift-tutorial-index","title":"OpenShift Tutorial Index","text":"<p>If you're just starting out, we recommend starting from OpenShift Overview and going through the tutorial in order.</p> <p>If you just need to review a specific step, you can find the page you need in the list below.</p>"},{"location":"openshift/#openshift-getting-started","title":"OpenShift Getting Started","text":"<ul> <li>OpenShift Overview &lt;&lt;-- Start Here</li> </ul>"},{"location":"openshift/#openshift-web-console","title":"OpenShift Web Console","text":"<ul> <li> <p>Access the NERC's OpenShift Web Console</p> </li> <li> <p>Web Console Overview</p> </li> </ul>"},{"location":"openshift/#openshift-command-line-interface-cli-tools","title":"OpenShift command-line interface (CLI) Tools","text":"<ul> <li> <p>OpenShift CLI Tools Overview</p> </li> <li> <p>How to Setup the OpenShift CLI Tools</p> </li> </ul>"},{"location":"openshift/#creating-your-first-application-on-openshift","title":"Creating Your First Application on OpenShift","text":"<ul> <li> <p>Creating A Sample Application</p> </li> <li> <p>Creating Your Own Developer Catalog Service</p> </li> </ul>"},{"location":"openshift/#editing-applications","title":"Editing Applications","text":"<ul> <li> <p>Editing your applications</p> </li> <li> <p>Scaling and Performance Guide</p> </li> </ul>"},{"location":"openshift/#serverless-computing","title":"Serverless Computing","text":"<ul> <li> <p>Understanding OpenShift Serverless</p> </li> <li> <p>Knative Serving User Guide: Deploying and Managing Serverless Workloads</p> </li> </ul>"},{"location":"openshift/#set-up-domain-name-for-your-application","title":"Set up Domain Name for Your Application","text":"<ul> <li>Set up Domain Name for Your Application</li> </ul>"},{"location":"openshift/#gpus","title":"GPUs","text":"<ul> <li>Introduction to GPUs on NERC OpenShift</li> </ul>"},{"location":"openshift/#storage","title":"Storage","text":"<ul> <li> <p>Storage Overview</p> </li> <li> <p>Minio</p> </li> <li> <p>Rclone</p> </li> </ul>"},{"location":"openshift/#deleting-applications","title":"Deleting Applications","text":"<ul> <li>Deleting your applications</li> </ul>"},{"location":"openshift/#decommission-openshift-resources","title":"Decommission OpenShift Resources","text":"<ul> <li>Decommission OpenShift Resources</li> </ul>"},{"location":"openshift/applications/creating-a-sample-application/","title":"Creating A Sample Application","text":""},{"location":"openshift/applications/creating-a-sample-application/#creating-a-sample-application","title":"Creating A Sample Application","text":"<p>NERC's OpenShift service is a platform that provides a cloud-native environment for developing and deploying applications.</p> <p>Here, we walk through the process of creating a simple web application, deploying it. This example uses the Node.js programming language, but the process with other programming languages will be similar. Instructions provided show the tasks using both the web console and the command-line tool.</p> <p>Red Hat Tutorials</p> <p>You can follow this tutorial from Red Hat, which provides an end-to-end example of deploying an application on the OpenShift Container Platform using either the OpenShift CLI (<code>oc</code>) or the web console.</p>"},{"location":"openshift/applications/creating-a-sample-application/#using-nercs-openshift-web-console","title":"Using NERC's OpenShift Web Console","text":"<ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>In the Navigation Menu, navigate to the Workloads -&gt; Topology menu.</p> </li> <li> <p>Creating applications using samples: Use existing code samples to get started     with creating applications on the OpenShift Container Platform. Here, you can     select different type of application you want to create (e.g. Node.js, Python,     Ruby, etc.).</p> <p>For that you can right-click on the page and select the \"From Catalog\" option from the Add to Project in-context menu as shown below:</p> <p></p> <p>Alternatively, You can go to the Home -&gt; Software Catalog menu as shown below:</p> <p></p> <p>Or, if you want to create an application from your own source code located in a git repository then you can select Import from Git from the in-context menu in the Topology view as shown below:</p> <p></p> <p>Alternatively, select the \"Import from Git\" button, represented by the \"+\" icon in the top navigation bar as shown below:</p> <p></p> <p>You will be prompted to enter a URL that points to a Git repo. Red Hat OpenShift will copy the repo to an internal storage location, inspect it, and attempt to discern the import strategy to build it. OpenShift will choose one of the following methods to build the application:</p> <ul> <li> <p>Option 1: Using the Builder image for the programming language used     (i.e., the s2i option).</p> </li> <li> <p>Option 2: Using the Dockerfile that is found in the Git repo.</p> </li> <li> <p>Option 3: Using the DevFile that is found in the Git Repo.</p> </li> </ul> <p>In the Git Repo URL text box, enter your git repo url. For example: <code>https://github.com/myuser/mypublicrepo.git</code>.</p> <p>Best Practices for Container Permissions in OpenShift</p> <ul> <li> <p>On OpenShift, every container in a standard namespace (unless security settings are modified) runs as a user with a random user ID (UID) and group ID (GID) <code>0</code>. Therefore, any folders that need to be written to, and any files that need to be modified (even temporarily), must be accessible to this user.  </p> <p>The best practice is to set ownership to <code>1001:0</code> (user <code>default</code>, group <code>0</code>).</p> </li> <li> <p>If this is not possible, another solution is to set the appropriate permissions for all users, such as <code>775</code>.</p> </li> </ul> </li> <li> <p>Click \"Create\" to create your application.</p> </li> <li> <p>Once your application has been created, you can view the details by clicking    on the application name in the Project Overview page.</p> </li> <li> <p>On the Topology view page, click on your application, or the application    circle if you are in graphical topology view. In the details panel    that displays, scroll to the Routes section on the \"Resources\" tab and click    on the link to go to the sample application. This will open your application    in a new browser window. The link will look similar to <code>https://&lt;your-application-name&gt;-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.</p> </li> </ol> <p>Example: Developing Applications on OpenShift</p> <p>For a quick tutorial on how to use image-based and source-based deployment techniques with examples in JavaScript (Node.js), Spring Boot, or Python, please refer to this guide.</p>"},{"location":"openshift/applications/creating-a-sample-application/#additional-resources","title":"Additional resources","text":"<p>For more options and customization please read this.</p>"},{"location":"openshift/applications/creating-a-sample-application/#using-the-cli-oc-command-on-your-local-terminal","title":"Using the CLI (oc command) on your local terminal","text":"<p>Alternatively, you can create an application on the NERC's OpenShift cluster by using the oc new-app command from the command line terminal.</p> <p>i. Make sure you have the <code>oc</code> CLI tool installed and configured on your local machine following these steps.</p> <p>Information</p> <p>Some users may have access to multiple projects. Run the following command to switch to a specific project space: <code>oc project &lt;your-project-namespace&gt;</code>.</p> <p>ii. To create an application, you will need to specify the language and runtime for your application. You can do this by using the <code>oc new-app</code> command and specifying a language and runtime. For example, to create a Node.js application, you can run the following command: <code>oc new-app nodejs</code></p> <p>iii. If you want to create an application from an existing Git repository, you can use the <code>--code</code> flag to specify the URL of the repository. For example: <code>oc new-app --code https://github.com/myuser/mypublicrepo</code>. If you want to use a different name, you can add the <code>--name=&lt;newname&gt;</code> argument to the <code>oc new-app</code> command. For example: <code>oc new-app --name=mytestapp https://github.com/myuser/mypublicrepo</code>. The platform will try to automatically detect the programming language of the application code and select the latest version of the base language image available. If <code>oc new-app</code> can't find any suitable Source-To-Image (S2I) builder images based on your source code in your Git repository or unable to detect the programming language or detects the wrong one, you can always specify the image you want to use as part of the new-app argument, with <code>oc new-app &lt;image url&gt;~&lt;git url&gt;</code>. If it is using a test application based on Node.js, we could use the same command as before but add <code>nodejs~</code> before the URL of the Git repository. For example: <code>oc new-app nodejs~https://github.com/myuser/mypublicrepo</code>.</p> <p>Important Note</p> <p>If you are using a private remote Git repository, you can use the <code>--source-secret</code> flag to specify an existing source clone secret that will get injected into your BuildConfig to access the repository. For example: <code>oc new-app https://github.com/myuser/yourprivaterepo --source-secret=yoursecret</code>.</p> <p>iv. Once your application has been created, You can run <code>oc status</code> to see if your application was successfully built and deployed. Builds and deployments can sometimes take several minutes to complete, so you may run this several times. you can view the details by running the <code>oc get pods</code> command. This will show you a list of all the pods running in your project, including the pod for your new application. The <code>oc rsh pod/&lt;pod_name&gt;</code> command opens a remote shell session inside the specified OpenShift pod i.e. , allowing you to interact with its container.</p> <p>v. When using the <code>oc</code> command-line tool to create an application, a route is not automatically set up to make your application web accessible. Run the following to make the test application web accessible: <code>oc create route edge --service=mytestapp --insecure-policy=Redirect</code>. Once the application is deployed and the route is set up, it can be accessed at a web URL similar to <code>https://mytestapp-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.</p>"},{"location":"openshift/applications/creating-a-sample-application/#for-more-additional-resources","title":"For more additional resources","text":"<p>For more options and customization please read this.</p>"},{"location":"openshift/applications/creating-a-sample-application/#using-the-developer-catalog-on-nercs-openshift-web-console","title":"Using the Developer Catalog on NERC's OpenShift Web Console","text":"<p>The Developer Catalog offers a streamlined process for deploying applications and services supported by Operator-backed services like CI/CD, Databases, Builder Images, and Helm Charts. It comprises a diverse array of application components, services, event sources, and source-to-image builders ready for integration into your project.</p> <p>About Quick Start Templates</p> <p>By default, the templates build using a public source repository on GitHub that contains the necessary application code. For more options and customization please read this.</p>"},{"location":"openshift/applications/creating-a-sample-application/#steps","title":"Steps","text":"<ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>You need to go to the Home -&gt; Software Catalog menu as shown below:</p> <p></p> </li> <li> <p>Then, you can search for available services in the Software Catalog by     entering <code>mariadb</code> in the search bar and selecting the desired service or     component to include in your project. For this example, select Databases     from the category list on the left to filter all database services, and then     click MariaDB to view the service details.</p> <p></p> <p>To Create Your Own Developer Catalog Service</p> <p>You also have the option to create and integrate custom services into the Developer Catalog using a template, as described here.</p> </li> <li> <p>Once selected by clicking the template, you will see Instantiate Template web     interface as shown below:</p> <p></p> </li> <li> <p>Clicking \"Instantiate Template\" will display an automatically populated     template containing details for the MariaDB service. Click \"Create\" to begin     the creation process and enter any custom information required.</p> </li> <li> <p>View the MariaDB service in the Topology view as shown below:</p> <p></p> </li> </ol>"},{"location":"openshift/applications/creating-a-sample-application/#for-additional-resources","title":"For Additional resources","text":"<p>For more options and customization please read this.</p>"},{"location":"openshift/applications/creating-your-own-developer-catalog-service/","title":"Creating Your Own Developer Catalog Service","text":""},{"location":"openshift/applications/creating-your-own-developer-catalog-service/#creating-your-own-developer-catalog-service","title":"Creating Your Own Developer Catalog Service","text":"<p>Here, we walk through the process of creating a simple RStudio Web Server using an OpenShift Template, which bundles all the necessary resources required to run it, such as ConfigMap, Pod, Route, Service, etc., and then initiate and deploy the RStudio server from that template.</p> <p>This example template file is readily accessible from the Git Repository.</p> <p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/rstudio-testapp.git\n</code></pre> <p>More about Writing Templates</p> <p>For more options and customization please read this.</p> <ol> <li> <p>Click the \"Import YAML\" button, represented by the \"+\" icon in the top     navigation bar as shown below::</p> <p></p> <p>Next, the Import YAML editor box will open, as shown below:</p> <p></p> </li> <li> <p>Either drag and drop the locally downloaded rstudio-server-template.yaml    file or copy and paste its contents into the opened Import YAML editor box, as    shown below:</p> <p></p> </li> <li> <p>You need to go to the Home -&gt; Software Catalog menu as shown below:</p> <p></p> </li> <li> <p>Then, you will be able to use the created Developer Catalog template by searching    for \"rstudio\" on catalog as shown below:</p> <p></p> </li> <li> <p>Once selected by clicking the template, you will see Instantiate Template web    interface as shown below:</p> <p></p> </li> <li> <p>Based on our template definition, we request that users input a preferred PASSWORD     as a variable for the RStudio Server. The following interface will prompt     you to enter the password, which will be used for logging into the RStudio Server.</p> <p>Variables</p> <p>This variable is mandatory for the application to be created.</p> <ul> <li>PASSWORD - Password for logging into RStudio Server</li> </ul> <p></p> </li> <li> <p>Once successfully initiated, you can either open the application URL using the    Open URL icon as shown below or you can naviate to the route URL by    navigating to the \"Routes\" section under the Location path as shown below:</p> <p></p> </li> <li> <p>To get the Username to be used for login on RStudio Server, you need to click    on running pod i.e. rstudio-server as shown below:</p> <p></p> </li> <li> <p>Then select the YAML section to find out the attribute value for runAsUser    that is used as the Username while Sign in to RStudio Server as shown below:</p> <p></p> </li> <li> <p>Finally, you will be able to view the RStudio Web Server interface!</p> </li> </ol> <p>Modifying uploaded templates</p> <p>You can edit a template that has already been uploaded to your project: <code>oc edit template &lt;template&gt;</code>.</p> <p>Another Example of Using an OpenShift Template to Setup an R Shiny Server</p> <p>Here is another example of running an R Shiny Server using an OpenShift Template. You can find more details here.</p>"},{"location":"openshift/applications/deleting-applications/","title":"Deleting your applications","text":""},{"location":"openshift/applications/deleting-applications/#deleting-your-applications","title":"Deleting your applications","text":""},{"location":"openshift/applications/deleting-applications/#deleting-applications-on-nercs-openshift-web-console","title":"Deleting applications on NERC's OpenShift Web Console","text":"<p>To delete an application and all of its associated components using the Topology view:</p> <ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>In the Navigation Menu, navigate to the Workloads -&gt; Topology menu.</p> </li> <li> <p>Click the application you want to delete to see the side panel with    the resource details of the application.</p> </li> <li> <p>Click the Actions drop-down menu displayed on the upper right of the panel,    and select Delete Application to see a confirmation dialog box as shown below:</p> <p></p> </li> <li> <p>Enter the name of the application and click Delete to delete it.</p> </li> </ol> <p>Or, if you are using Graph view then you can also right-click the application you want to delete and click Delete Application to delete it as shown below:</p> <p></p>"},{"location":"openshift/applications/deleting-applications/#deleting-applications-using-the-oc-command-on-your-local-terminal","title":"Deleting applications using the oc command on your local terminal","text":"<p>Alternatively, you can delete the resource objects by using the oc delete command from the command line terminal. Make sure you have the <code>oc</code> CLI tool installed and configured on your local machine following these steps.</p> <p>How to select resource object?</p> <p>You can delete a single resource object by name, or delete a set of resource objects by specifying a label selector.</p> <p>When an application is deployed, resource objects for that application will typically have an app label applied to them with value corresponding to the name of the application. This can be used with the label selector to delete all resource objects for an application.</p> <p>To test what resource objects would be deleted when using a label selector, use the <code>oc get</code> command to query the set of objects which would be matched.</p> <p><code>oc get all --selector app=&lt;application-name&gt; -o name</code></p> <p>For example:</p> <pre><code>oc get all --selector app=rstudio-server -o name\npod/rstudio-server\nservice/rstudio-server\nroute.route.openshift.io/rstudio-server\n</code></pre> <p>If you are satisfied that what is shown are the resource objects for your application, then run <code>oc delete</code>.</p> <p><code>oc delete all --selector app=&lt;application-name&gt;</code></p> <p>Or,</p> <p><code>oc delete all -l app=&lt;application-name&gt;</code></p> <p>Important Note</p> <p>Selector all matches on a subset of all resource object types that exist. It targets the core resource objects that would be created for a build and deployment. It will not include resource objects such as persistent volume claims (pvc), config maps (configmap), secrets (secret), and others.</p> <p>You will either need to delete these resource objects separately, or if they also have been labelled with the app tag, list the resource object types along with all.</p> <p><code>oc delete all,configmap,pvc,serviceaccount,rolebinding --selector app=&lt;application-name&gt;</code></p> <p>Or,</p> <p><code>oc delete all,configmap,pvc,serviceaccount,rolebinding -l app=&lt;application-name&gt;</code></p> <p>If you are not sure what labels have been applied to resource objects for your application, you can run oc describe on the resource object to see the labels applied to it. For example:</p> <pre><code>oc describe pod/rstudio-server\nName:         rstudio-server\nNamespace:    64b664c37f2a47c39c3cf3942ff4d0be\nPriority:     0\nNode:         wrk-11/10.30.6.21\nStart Time:   Fri, 16 Dec 2022 10:59:23 -0500\nLabels:       app=rstudio-server\n            template.openshift.io/template-instance-owner=44a3fae8-4e8e-4058-a4a8-0af7bbb41f6\n...\n</code></pre> <p>Important Note</p> <p>It is important to check what labels have been used with your application if you have created it using a template, as templates may not follow the convention of using the <code>app</code> label.</p>"},{"location":"openshift/applications/editing-applications/","title":"Editing your applications","text":""},{"location":"openshift/applications/editing-applications/#editing-applications","title":"Editing applications","text":"<p>You can edit the configuration and the source code of the application you create using the Topology view.</p>"},{"location":"openshift/applications/editing-applications/#editing-the-source-code-of-an-application","title":"Editing the source code of an application","text":"<p>You can click the \"Edit Source Code\" icon, displayed at the bottom-right of the deployed application, to access your source code and modify it as shown below:</p> <p></p> <p>Information</p> <p>This feature is available only when you create applications using the From Git, Container Image, From Catalog, and From Dockerfile options.</p>"},{"location":"openshift/applications/editing-applications/#editing-the-application-configuration","title":"Editing the application configuration","text":"<ol> <li> <p>In the Topology view, right-click the application to see the edit options    available as shown below:</p> <p></p> <p>Or, In the Topology view, click the deployed application to reveal the right-side Overview panel. From the Actions drop-down list, we can see the similar edit options available as shown below:</p> <p></p> </li> <li> <p>Click on any of the options available to edit resource used by your application,    the pop-up form will be pre-populated with the values you had added while creating    the applicaiton.</p> </li> <li> <p>Click Save to restart the build and deploy a new image.</p> </li> </ol>"},{"location":"openshift/applications/editing-applications/#rate-limits-while-pulling-container-image","title":"Rate Limits While Pulling Container Image","text":"<p>By default, Container Images are pulled from Registry i.e. Docker Hub or other commercial and private registries, which enforce rate limits on anonymous users. If your setup involves frequent image pulls, you may face these restrictions.</p> <p>To prevent disruptions, you can authenticate with Registry by creating a Secret in your namespace and linking it to your service account.</p> <ul> <li> <p>Prerequisites:</p> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI to enable <code>oc</code> commands. Refer to this user guide.</p> </li> </ul>"},{"location":"openshift/applications/editing-applications/#using-command-line-to-create-docker-image-pull-secret","title":"Using Command Line to Create Docker Image Pull Secret","text":"<ul> <li> <p>First, generate a Kubernetes Secret using the following <code>oc</code> commands.</p> <p>You can have either a Secret storing Image registry credentials as shown below:</p> <pre><code>oc create secret docker-registry &lt;your-secret-name&gt; \\\n  --docker-server=&lt;registry i.e. docker.io or quay.io&gt; \\\n  --docker-username=&lt;your-username&gt; \\\n  --docker-password=&lt;your-secret-token&gt; \\\n  --docker-email=&lt;your-email&gt;\n</code></pre> <p>Use Access Token Instead of Password!</p> <p>It is recommended to use an access token instead of your actual password as <code>&lt;your-secret-token&gt;</code>. If you have two-factor authentication (2FA) enabled on your account i.e. <code>&lt;your-username&gt;</code>, using an access token is the only way to authenticate. Similarly, other registries also provide access tokens for authentication in user accounts.</p> <p>Or, based on the configuration file, create a generic Kubernetes Secret by following the steps below:</p> <p>i. Create a <code>.dockerconfigjson</code> file manually by running:</p> <pre><code>echo -n \"&lt;your-username&gt;:&lt;your-secret-token&gt;\" | base64\n</code></pre> <p>Replace <code>&lt;your-username&gt;</code> and <code>&lt;your-secret-token&gt;</code> with your own.</p> <p>ii. Then, create a <code>config.json</code> file with the following structure. Here is     an example for <code>docker.io</code>:</p> <pre><code>{\n\"auths\": {\n    \"https://index.docker.io/v1/\": {\n    \"auth\": \"&lt;base64-encoded-credentials&gt;\"\n    }\n  }\n}\n</code></pre> <p>iii. Now, use this JSON file to create a Kubernetes secret:</p> <pre><code>oc create secret generic &lt;your-secret-name&gt; \\\n  --from-file=.dockerconfigjson=config.json \\\n  --type=kubernetes.io/dockerconfigjson\n</code></pre> </li> <li> <p>Finally, patch the default service account in your namespace to use this     secret when pulling images:</p> <pre><code>oc patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"&lt;your-secret-name&gt;\"}]}'\n</code></pre> <p>Another Way</p> <p>Alternatively, you can use <code>oc secrets link</code> command like this:</p> <p><code>oc secrets link default &lt;your-secret-name&gt; --for=pull</code></p> <p>You can check if the secret is correctly applied by running:</p> <pre><code>oc get serviceaccount default -o json | jq '.imagePullSecrets'\n</code></pre> <p>This ensures OpenShift authenticates with the Registry to bypass anonymous rate limits when pulling images.</p> </li> </ul>"},{"location":"openshift/applications/editing-applications/#reference-image-pull-secret-in-yaml-deployment-files","title":"Reference Image Pull Secret in YAML Deployment Files","text":"<p>Alternatively, instead of directly applying the Image Pull Secret to your Service Account, once you have successfully created the Secret in your working namespace, you can reference it under <code>spec.imagePullSecrets</code> in your YAML file as shown below:</p> <pre><code>...\nspec:\n...\ncontainers:\n- name: ...\n    image: &lt;Image&gt;\n    imagePullPolicy: IfNotPresent\n    ...\nimagePullSecrets:\n    - name: &lt;your-secret-name&gt;\n...\n</code></pre>"},{"location":"openshift/applications/editing-applications/#configuring-image-pull-secret-via-openshift-web-console","title":"Configuring Image Pull Secret via OpenShift Web Console","text":"<p>Alternatively, if you are using the OpenShift Web Console, you can add or edit a Secret by creating a new one with Image registry credentials or by Uploading a configuration file via the web form itself. If a secret is already set up, simply select an existing Pull Secret from the dropdown menu:</p> <p></p>"},{"location":"openshift/applications/knative-serving/","title":"Knative Serving User Guide: Deploying and Managing Serverless Workloads","text":""},{"location":"openshift/applications/knative-serving/#knative-serving-user-guide-deploying-and-managing-serverless-workloads","title":"Knative Serving User Guide: Deploying and Managing Serverless Workloads","text":"<p>Here, we walk through the process of deploying a Knative Serving service, see its use of Configuration and Revision, and practice a blue-green deployment and canary release.</p> <p>In this tutorial, you will:</p> <ul> <li> <p>Deploy an OpenShift Serverless <code>service</code> utilizing the knative client (<code>kn</code>)     or the OpenShift client (<code>oc</code>) CLI.</p> </li> <li> <p>Deploy multiple <code>revisions</code> of a service.</p> </li> <li> <p>Understand the <code>underlying components</code> of a serverless service.</p> </li> <li> <p>Understand how Serverless is able to <code>scale-to-zero</code>.</p> </li> <li> <p>Run different revisions of a service via <code>blue-green</code> deployments and <code>canary</code>     release.</p> </li> </ul>"},{"location":"openshift/applications/knative-serving/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI     to enable <code>oc</code> commands. Refer to this user guide.</p> </li> <li> <p>The next step is to install the Knative CLI (<code>kn</code>) on the bootstrap machine     used to access the NERC's OpenShift Container Platform (OCP) cluster. The Knative     client <code>kn</code> allows you to create Knative resources from the command line or     from within Shell scripts. Follow the official documentation     to download and install the Knative CLI.</p> <p>You can verify the Knative CLI installation by running the following command:</p> <pre><code>kn version\n</code></pre> <p>For more information regarding the CLI commands available for Knative Functions, Serving, and Eventing.</p> </li> </ul>"},{"location":"openshift/applications/knative-serving/#using-knative-serving","title":"Using Knative Serving","text":"<p>Knative Serving is ideal for running application services inside Kubernetes, offering simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. The Knative platform manages your service's deployments, revisions, networking, and scaling. Since Knative Serving can automatically scale down to zero when not in use.</p> <p>Knative Serving is a platform for streamlined application deployment, traffic-based auto-scaling from zero to N, and traffic-split rollouts.</p> <p>Other key features include:</p> <ul> <li> <p>Simplified deployment of serverless containers</p> </li> <li> <p>Traffic-based auto-scaling, including scale-to-zero</p> </li> <li> <p>Routing and network programming</p> </li> <li> <p>Point-in-time application snapshots and their configuration</p> </li> </ul>"},{"location":"openshift/applications/knative-serving/#creating-serverless-applications","title":"Creating serverless applications","text":"<p>You can create a serverless application by using one of the following methods:</p> <ul> <li> <p>Create a Knative Service from the OCP Web Console by navigating to the   Serverless -&gt; Serving menu.</p> </li> <li> <p>Create a Knative Service by using the Knative (<code>kn</code>) CLI.</p> </li> <li> <p>Create and apply a Knative Service object as a YAML file, by using the <code>oc</code>   CLI.</p> </li> </ul>"},{"location":"openshift/applications/knative-serving/#deploying-a-test-inferenceservice-in-your-namespace-for-istio-sidecar-validation","title":"Deploying a Test InferenceService in Your Namespace for Istio Sidecar Validation","text":"<p>Create a \"dummy\" <code>InferenceService</code> for your namespace that needs just basic/standalone knative serving features:</p> <pre><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  annotations:\n    security.opendatahub.io/enable-auth: 'true' # (1)!\n    serving.knative.openshift.io/enablePassthrough: 'true' # (2)!\n    serving.kserve.io/deploymentMode: Serverless # (3)!\n    sidecar.istio.io/inject: 'true' # (4)!\n    sidecar.istio.io/rewriteAppHTTPProbers: 'true' # (5)!\n    serving.kserve.io/stop: 'true' # (6)!\n  labels:\n    opendatahub.io/dashboard: 'false'\n  name: dummy-inference-service-for-knative-serving\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: dummy # (7)!\n      name: \"\"\n</code></pre> <ol> <li> <p>Ensures security/auth is enabled, typically via OpenShift/ODH integration.</p> </li> <li> <p>Crucial for TLS/HTTPS passthrough, often required when Istio is involved.</p> </li> <li> <p>Enforces the use of Knative Serving for scale-to-zero and autoscaling.</p> </li> <li> <p>Injects the Istio proxy sidecar into the resulting pod.</p> </li> <li> <p>Ensures health probes work correctly with the sidecar.</p> </li> <li> <p>A KServe instruction to pause or scale the service down immediately.</p> </li> <li> <p>Placeholder format, indicating this is likely a test/infrastructure resource.</p> </li> </ol> <p>Why Set Up a 'Dummy' InferenceService?</p> <p>Deploy the provided YAML to create a dummy KServe InferenceService used to validate Knative Serving and Istio Service Mesh integration in an OpenShift Data Hub (ODH) environment. Without this, the Knative Service cannot create an external route.</p> <p>The issue you encountered (Waiting for load balancer to be ready) occurs when the namespace is not part of the Service Mesh. In this case, Istio cannot create the external routing rules required to expose the service.</p> <p>This dummy <code>InferenceService</code> uses a non-functional placeholder model but is configured to exercise Knative serverless behavior and Istio sidecar-based routing.</p> <p>The \"dummy\" <code>InferenceService</code> can be deployed using the following <code>oc</code> command:</p> <pre><code>oc apply -f &lt;filename&gt;\n</code></pre>"},{"location":"openshift/applications/knative-serving/#deploy-knative-service-in-your-namespace","title":"Deploy Knative Service in Your Namespace","text":"<p>Procedure:</p> <p>1. Deploy Knative Services:</p> <p>After successfully deploying the dummy <code>InferenceService</code> in your project namespace, create a Knative <code>Service</code> with Istio sidecar injection enabled and configure it to use an OpenShift pass-through route using the following example:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: &lt;service_name&gt; # (1)!\n  namespace: &lt;your-namespace&gt; # (2)!\n  annotations:\n    serving.knative.openshift.io/enablePassthrough: 'true' # (3)!\nspec:\n  template:\n    spec:\n      containers:\n      - image: &lt;image_url&gt;\n</code></pre> <ol> <li> <p>Choose an appropriate name for the Service.</p> </li> <li> <p>Your project's namespace where you want to run the serverless application.     If this line is omitted, the namespace currently selected in the OpenShift Web     Console will be used as the default.</p> </li> <li> <p>MANDATORY: Instructs Knative Serving to generate an OpenShift Pass-through     enabled route.</p> </li> </ol> <p>Very Important: Always Enable the OpenShift Pass-through Route</p> <p>Please note that you must add the following annotation to the Service. This annotation is required to ensure proper routing and to prevent external access issues when using Knative with OpenShift and Istio. Only then will your Knative Service work correctly with the Service Mesh.</p> <pre><code>...\nspec:\n  template:\n    metadata:\n      ...\n      annotations:\n        serving.knative.openshift.io/enablePassthrough: 'true'\n        ...  \n...\n</code></pre> <p>For example, we are going to deploy a Knative Serving service, see its use of Configuration and Revision.</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>```sh\nkn service create test-webapp \\\n  --annotation-service serving.knative.openshift.io/enablePassthrough=true \\\n  --env RESPONSE=\"Hello Serverless!\" \\\n  --image docker.io/openshift/hello-openshift\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>```yaml\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: test-webapp\n  annotations:\n    serving.knative.openshift.io/enablePassthrough: 'true'\nspec:\n  template:\n    spec:\n      containers:\n        - image: docker.io/openshift/hello-openshift\n          env:\n            - name: RESPONSE\n              value: \"Hello Serverless!\"\n```\n</code></pre> <p>The OpenShift Web Console will display the Knative Service (<code>KSVC</code>) as shown below:</p> <p></p> <p>2. Deploy a <code>curl</code> pod to test the connections:</p> <p>Run the following script on your local terminal:</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: curl\n  labels:\n    app: curl\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: curl\n  template:\n    metadata:\n      labels:\n        app: curl\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: curl\n        image: curlimages/curl\n        command:\n        - sleep\n        - \"3600\"\nEOF\n</code></pre> <p>3. Verification:</p> <p>Applying the application service configuration from the previous step creates a route, service, revision, and other resources managed by Knative Serving. You can verify these components using the following command:</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>**Service:**\n\n```sh\nkn service list\n```\n\n**Output:**\n\n```sh\nNAME          URL                                                               LATEST              AGE     CONDITIONS   READY   REASON\ntest-webapp   https://test-webapp-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org   test-webapp-00001   3m16s   3 OK / 3     True\n```\n\n**Revisions:**\n\n```sh\nkn revision list\n```\n\n**Output:**\n\n```sh\nNAME                SERVICE       TRAFFIC   TAGS   GENERATION   AGE     CONDITIONS   READY   REASON\ntest-webapp-00001   test-webapp   100%             1            3m17s   4 OK / 4     True\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>**Service:**\n\n```sh\noc get serving\n```\n\n**Output:**\n\n```sh\nNAME                                    URL                                                               READY   REASON\nroute.serving.knative.dev/test-webapp   https://test-webapp-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org   True\n\nNAME                                            LATESTCREATED       LATESTREADY         READY   REASON\nconfiguration.serving.knative.dev/test-webapp   test-webapp-00001   test-webapp-00001   True\n\nNAME                                             CONFIG NAME   GENERATION   READY   REASON   ACTUAL REPLICAS   DESIRED REPLICAS\nrevision.serving.knative.dev/test-webapp-00001   test-webapp   1            True             0                 0\n\nNAME                                      URL                                                               LATESTCREATED       LATESTREADY         READY   REASON\nservice.serving.knative.dev/test-webapp   https://test-webapp-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org   test-webapp-00001   test-webapp-00001   True\n```\n\n**Revisions:**\n\n```sh\noc get rev \\\n  --selector=serving.knative.dev/service=test-webapp \\\n  --sort-by=\"{.metadata.creationTimestamp}\"\n```\n</code></pre> <p>4. Invoke Service:</p> <pre><code>EXTERNAL_URL=$(oc get ksvc test-webapp -o custom-columns=:.status.url --no-headers)\noc exec deployment/curl -it -- curl -ik $EXTERNAL_URL\n</code></pre> <p>Output:</p> <pre><code>HTTP/2 200\ncontent-length: 17\ncontent-type: text/plain; charset=utf-8\ndate: Sat, 13 Dec 2025 21:31:58 GMT\nserver: istio-envoy\nx-envoy-upstream-service-time: 5627\n\nHello Serverless!\n</code></pre> <p>Controlling Knative Service Visibility (Internal vs External Access)</p> <p>Use the following commands to control service visibility:</p> <pre><code>kn service update test-webapp --cluster-local      # Make the service internal\nkn service update test-webapp --no-cluster-local   # Expose the service externally\n</code></pre> <p>The <code>--cluster-local</code> flag makes the Knative Service accessible only within the cluster, preventing external access. The <code>--no-cluster-local</code> flag exposes the service externally, allowing it to be accessed through an external route.</p> <p>5. Cleanup:</p> <p>Delete the resources that were created after testing successful:</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>```sh\nkn service delete test-webapp\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>```sh\noc delete deployment/curl\noc delete ksvc/test-webapp\n```\n</code></pre>"},{"location":"openshift/applications/knative-serving/#creating-a-service-using-offline-mode","title":"Creating a service using offline mode","text":"<p>In offline mode, create a local Knative service descriptor file:</p> <pre><code>kn service create test-webapp-offline \\\n  --annotation-service serving.knative.openshift.io/enablePassthrough=true \\\n  --env RESPONSE=\"Hello Serverless!\" \\\n  --image docker.io/openshift/hello-openshift \\\n  --target ./ \\\n  --namespace test\n</code></pre> <ul> <li> <p>The <code>--target ./</code> flag offline mode and specifies <code>./</code> as the directory for   storing the new directory tree.</p> </li> <li> <p>The <code>--namespace test</code> option places the new service in the 'test' namespace.   You can change it to the name of your desired namespace. If you do not use   <code>--namespace</code>, and you are logged in to the OCP cluster, the descriptor file   is created in the current namespace. Based on the namespace name a new directory   will be created.</p> </li> </ul> <p>Output:</p> <pre><code>Service 'test-webapp-offline' created in namespace 'test'.\n</code></pre> <p>Examine the created directory structure:</p> <pre><code>tree ./\n</code></pre> <p>Output:</p> <pre><code>./\n\u2514\u2500\u2500 test\n    \u2514\u2500\u2500 ksvc\n        \u2514\u2500\u2500 test-webapp-offline.yaml\n\n2 directories, 1 file\n</code></pre> <ul> <li> <p>The current <code>./</code> directory specified with <code>--target</code> contains the new <code>test/</code>   directory that is named after the specified namespace.</p> </li> <li> <p>The <code>test/</code> directory contains the <code>ksvc</code> directory, named after the resource type.</p> </li> <li> <p>The <code>ksvc</code> directory contains the descriptor file <code>test-webapp-offline.yaml</code>,   named according to the specified service name i.e. <code>test-webapp-offline</code>.</p> </li> </ul> <p>Use the service descriptor file to create the service on the cluster:</p> <pre><code>kn service create -f test/ksvc/test-webapp-offline.yaml\n</code></pre>"},{"location":"openshift/applications/knative-serving/#autoscaling","title":"AutoScaling","text":"<p>You can review the autoscaler configuration by running the following command:</p> <pre><code>oc get configmap config-autoscaler -n knative-serving -o yaml\n</code></pre>"},{"location":"openshift/applications/knative-serving/#scale-bounds","title":"Scale bounds","text":"<p>In the service definition above, note that no annotations are applied in the resource spec. By default, OpenShift Serverless will scale these deployments down to zero running pods and automatically scale them up as load increases. Resource limits can be set through the Kubernetes resource definition.</p> <p>In the Service resource definition, you do not need to define any compute infrastructure or configure scaling behavior. You only specify the optional minimum and maximum scale. The minimum scale can be set to zero, meaning no pods run until a request arrives, at which point a pod is automatically created with minimal startup delay. This enables compute resources to be used strictly on an as-needed basis.</p> <p>For example, to specify a minimum of 1 pod and a maximum of 3 pods, add the following syntax:</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>```sh\nkn service create &lt;service_name&gt; --image &lt;image_uri&gt; --scale-min 1 --scale-max 3\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>```yaml\n...\nspec:\n  template:\n    metadata:\n      ...\n      annotations:\n        autoscaling.Knative.dev/min-scale: \"1\"\n        autoscaling.knative.dev/max-scale: \"3\"\n        ...\n...\n```\n</code></pre> <p>When testing, add the annotations to set <code>min-scale</code> to <code>1</code>, so the pod never shuts down. This lets you more easily access logs or open a terminal to examine the state of your function container.</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>```sh\nkn service create test-webapp \\\n  --annotation-service serving.knative.openshift.io/enablePassthrough=true \\\n  --env RESPONSE=\"Hello Serverless!\" \\\n  --image docker.io/openshift/hello-openshift \\\n  --scale-min 1 \\\n  --scale-max 3\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>```yaml\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: test-webapp\n  annotations:\n    serving.knative.openshift.io/enablePassthrough: 'true'\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.Knative.dev/min-scale: \"1\"\n        autoscaling.knative.dev/max-scale: \"3\"\n    spec:\n      containers:\n        - image: docker.io/openshift/hello-openshift\n          env:\n            - name: RESPONSE\n              value: \"Hello Serverless!\"\n```\n</code></pre>"},{"location":"openshift/applications/knative-serving/#concurrency","title":"Concurrency","text":"<p>Concurrency defines how many simultaneous requests each application replica can handle at a given time. It can be configured as either a soft limit or a hard limit:</p> <ul> <li> <p>Soft limit: A target number of concurrent requests that is not strictly     enforced. During traffic spikes, this limit may be exceeded.</p> <p>You can set a soft concurrency target for your Knative Service by adding the <code>autoscaling.knative.dev/target</code> annotation to the spec, or by using the <code>kn service</code> command with the appropriate flags, as shown below where the concurrency target is set to 50 requests.</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>```sh\nkn service create &lt;service_name&gt; --image &lt;image_uri&gt; --concurrency-target 50\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>```yaml\n...\nspec:\n  template:\n    metadata:\n      ...\n      annotations:\n        autoscaling.knative.dev/target: \"50\"\n        ...\n...\n```\n</code></pre> </li> <li> <p>Hard limit: A strictly enforced maximum number of concurrent requests. Once     the limit is reached, additional requests are queued until capacity becomes     available.</p> <p>You can specify a hard concurrency limit for your Knative service by modifying the <code>containerConcurrency</code> spec, or by using the <code>kn service</code> command with the correct flags, as shown below where the concurrency limit is set to 50 requests.</p> <p>=== \"Using the Knative CLI\"</p> <pre><code>```sh\nkn service create &lt;service_name&gt; --image &lt;image_uri&gt; --concurrency-limit 50\n```\n</code></pre> <p>=== \"Using YAML\"</p> <pre><code>```yaml\n...\nspec:\n  template:\n    ...\n    spec:\n      containerConcurrency: 50\n      ...\n...\n```\n</code></pre> </li> </ul>"},{"location":"openshift/applications/knative-serving/#traffic-splitting","title":"Traffic splitting","text":"<p>In a Knative application, traffic can be managed using traffic splits. Traffic splits are configured as part of a route, which is managed by a Knative Service.</p> <p></p> <p>Configuring a route allows requests to be distributed across different revisions of a service, as defined by the traffic specification in the Knative Service object.</p> <p>A traffic specification includes one or more revisions, each receiving a defined percentage of traffic. The total traffic allocation must equal 100%, which is validated by Knative. Revisions can reference a specific, named revision or the latest revision, which automatically updates when new revisions are created. Each revision can also be assigned a tag to generate an additional access URL.</p> <p>The traffic specification can be modified by:</p> <ul> <li> <p>Editing the Service YAML directly</p> </li> <li> <p>Using the Knative (<code>kn</code>) CLI with the <code>--traffic</code> flag</p> </li> <li> <p>Using the OpenShift web console</p> </li> </ul>"},{"location":"openshift/applications/knative-serving/#traffic-splitting-editing-the-service-yaml-directly","title":"Traffic splitting editing the Service YAML directly","text":"<p>When you create a Knative Service, it does not have any default traffic specification settings.</p> <p>Traffic spec examples:</p> <p>The following example shows a traffic specification where 100% of traffic is routed to the latest revision of the service. Under the <code>status</code> section, you can see the name of the revision that <code>latestRevision</code> points to:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: &lt;your-namespace&gt;\nspec:\n...\n  traffic:\n  - latestRevision: true\n    percent: 100\nstatus:\n  ...\n  traffic:\n  - percent: 100\n    revisionName: example-service\n</code></pre> <p>The following example shows a traffic specification where 100% of traffic is routed to the revision tagged as <code>current</code>, with the revision name specified as <code>example-service</code>. The revision tagged as <code>latest</code> remains available, even though no traffic is directed to it:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: &lt;your-namespace&gt;\nspec:\n...\n  traffic:\n  - tag: current\n    revisionName: example-service\n    percent: 100\n  - tag: latest\n    latestRevision: true\n    percent: 0\n</code></pre> <p>The following example demonstrates how the traffic specification can be extended to split traffic between multiple revisions. In this case, 50% of traffic is routed to the revision tagged as <code>current</code> and 50% to the revision tagged as <code>candidate</code>. The revision tagged as <code>latest</code> remains available, even though it receives no traffic:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: &lt;your-namespace&gt;\nspec:\n...\n  traffic:\n  - tag: current\n    revisionName: example-service-1\n    percent: 50\n  - tag: candidate\n    revisionName: example-service-2\n    percent: 50\n  - tag: latest\n    latestRevision: true\n    percent: 0\n</code></pre>"},{"location":"openshift/applications/knative-serving/#traffic-splitting-using-the-knative-cli","title":"Traffic splitting using the Knative CLI","text":"<p>Procedure:</p> <p>Specify the revision of your service and what percentage of traffic you want to route to it by using the <code>--traffic</code> tag with a standard <code>kn service update</code> command:</p> <p>Example command:</p> <pre><code>kn service update &lt;service_name&gt; --traffic &lt;revision&gt;=&lt;percentage&gt;\n</code></pre> <p>Where:</p> <ul> <li> <p><code>&lt;service_name&gt;</code> is the name of the Knative Service for which you are configuring   traffic routing.</p> </li> <li> <p><code>&lt;revision&gt;</code> is the revision that will receive a portion of the traffic. This   can be the revision name or a tag assigned using the <code>--tag</code> flag.</p> </li> <li> <p><code>&lt;percentage&gt;</code> is the percentage of traffic routed to the specified revision.</p> </li> </ul> <p>Optional: The <code>--traffic</code> flag can be specified multiple times in a single command. For example, if you have a revision tagged as <code>@latest</code> and another revision named <code>stable</code>, you can define the traffic split between them as follows:</p> <pre><code>kn service update test-webapp --traffic @latest=20,stable=80\n</code></pre>"},{"location":"openshift/applications/knative-serving/#splitting-traffic-between-revisions","title":"Splitting Traffic between Revisions","text":"<p>Procedure:</p> <p>1. Create and deploy an app as a Knative service:</p> <pre><code>kn service create test-webapp \\\n  --annotation-service serving.knative.openshift.io/enablePassthrough=true \\\n  --env RESPONSE=\"Hello Serverless!\" \\\n  --image docker.io/openshift/hello-openshift\n</code></pre> <pre><code>kn revision list\n</code></pre> <p>Output:</p> <pre><code>NAME                SERVICE       TRAFFIC   TAGS   GENERATION   AGE   CONDITIONS   READY   REASON\ntest-webapp-00001   test-webapp   100%             1            8s    4 OK / 4     True\n</code></pre> <p>2. Create a new Revision:</p> <pre><code>kn service update test-webapp --scale-max 10 --annotation autoscaling.knative.dev/metric=rps --scale-target 1 --scale-window=10s\n</code></pre> <pre><code>kn revision list\n</code></pre> <p>Output:</p> <pre><code>NAME                SERVICE       TRAFFIC   TAGS   GENERATION   AGE    CONDITIONS   READY   REASON\ntest-webapp-00002   test-webapp   100%             2            27s    3 OK / 4     True\ntest-webapp-00001   test-webapp                    1            2m5s   3 OK / 4     True\n</code></pre> <p>3. Set 100% traffic distribution to the second revision:</p> <pre><code>kn service update test-webapp --traffic test-webapp-00002=100\n</code></pre> <p>4. Create a new Third Revision:</p> <pre><code>kn service update test-webapp --env RESPONSE=\"Hello NERC Serverless!\" --revision-name=nerc-revision\n</code></pre> <pre><code>kn revision list\n</code></pre> <p>Output:</p> <pre><code>NAME                        SERVICE       TRAFFIC   TAGS   GENERATION   AGE     CONDITIONS   READY   REASON\ntest-webapp-nerc-revision   test-webapp                    3            44s     3 OK / 4     True\ntest-webapp-00002           test-webapp   100%             2            2m21s   3 OK / 4     True\ntest-webapp-00001           test-webapp                    1            3m59s   3 OK / 4     True\n</code></pre> <p>5. Tag the Third Revision:</p> <pre><code>kn service update test-webapp --tag @latest=nerc\n</code></pre> <pre><code>kn revision list\n</code></pre> <p>Output:</p> <pre><code>NAME                        SERVICE       TRAFFIC   TAGS   GENERATION   AGE     CONDITIONS   READY   REASON\ntest-webapp-nerc-revision   test-webapp             nerc   3            108s    3 OK / 4     True\ntest-webapp-00002           test-webapp   100%             2            3m25s   3 OK / 4     True\ntest-webapp-00001           test-webapp                    1            5m3s    3 OK / 4     True\n</code></pre> <p>6. Split Traffic Between the Second (80%) and the Third Tagged Revision (20%):</p> <pre><code>kn service update test-webapp --traffic nerc=20,test-webapp-00002=80\n</code></pre> <pre><code>kn revision list\n</code></pre> <p>Output:</p> <pre><code>NAME                        SERVICE       TRAFFIC   TAGS   GENERATION   AGE     CONDITIONS   READY   REASON\ntest-webapp-nerc-revision   test-webapp   20%       nerc   3            3m25s   3 OK / 4     True\ntest-webapp-00002           test-webapp   80%              2            5m2s    3 OK / 4     True\ntest-webapp-00001           test-webapp                    1            6m40s   3 OK / 4     True\n</code></pre>"},{"location":"openshift/applications/knative-serving/#managing-traffic-between-revisions-by-using-the-openshift-web-console","title":"Managing Traffic between revisions by using the OpenShift web console","text":"<p>To split traffic between multiple revisions of an application in the Topology view:</p> <ol> <li> <p>Navigate to the Serverless -&gt; Serving menu. Click the Knative service     to see its overview in the side panel.</p> </li> <li> <p>Click the Resources tab, to see a list of Revisions and Routes for     the service.</p> <p></p> </li> <li> <p>Go to \"Actions\" -&gt; \"Edit Service\":     </p> <p>Then modify the service configuration in the YAML editor, and click Save. For example, change the <code>timeoutseconds</code> from <code>300</code> to <code>301</code>.  </p> <p></p> <p>This change in the configuration triggers a new revision.</p> <p>In the Topology view, the latest revision is displayed and the Resources tab for the service now displays the two revisions as shown below:</p> <p></p> </li> <li> <p>Either, go to \"Actions\" -&gt; \"Set traffic distribution\" Or, in the Resources     tab, click Set Traffic Distribution button to see the traffic distribution     dialog box:</p> <p></p> <p>Add the split traffic percentage portion for the two revisions in the Splits field.</p> <p>Add tags to create custom URLs for the two revisions (Optional)</p> <p>Click Save to see two nodes representing the two revisions in the Topology view.</p> <p></p> </li> </ol>"},{"location":"openshift/applications/knative-serving/#rerouting-traffic-using-bluegreen-and-canary-deployments","title":"Rerouting Traffic Using Blue/green and canary deployments","text":"<p>Blue\u2013green deployment is a release strategy that maintains two identical environments: one active (blue) and one idle (green) - allowing traffic to be switched for seamless updates with minimal downtime and easy rollback.</p> <p>Developers can then specify exactly what percentage of incoming network traffic should be routed to the new \"canary\" version versus the existing \"stable\" version. This granular control is essential for validating a new release with a small subset of real user traffic before a full rollout.</p> <p>Procedure:</p> <ol> <li> <p>Create and deploy an app as a Knative service:</p> <pre><code>kn service create test-webapp \\\n--annotation-service serving.knative.openshift.io/enablePassthrough=true \\\n--env RESPONSE=\"Hello Serverless!\" \\\n--image docker.io/openshift/hello-openshift\n</code></pre> </li> <li> <p>Find the name of the first revision created when you deployed the service by     running the following command:</p> <pre><code>oc get ksvc test-webapp -o=jsonpath='{.status.latestCreatedRevisionName}'\n</code></pre> <p>Output:</p> <pre><code>test-webapp-00001\n</code></pre> </li> <li> <p>Add the following YAML to the service spec to send inbound traffic to the     revision:</p> <pre><code>...\nspec:\n    traffic:\n    - revisionName: &lt;first_revision_name&gt;\n        percent: 100 # All traffic goes to this revision\n...\n</code></pre> </li> <li> <p>Verify that you can view your app at the URL output you get from running the     following command:</p> <pre><code>oc get ksvc test-webapp\n</code></pre> </li> <li> <p>Deploy a second revision of your application by changing at least one field in     the service's template spec and redeploying it. For example, you can update     the service image or an environment variable. Redeploy the service by applying     the updated YAML file or using the <code>kn service update</code> command:</p> <pre><code>kn service update test-webapp --env RESPONSE=\"Hello NERC Serverless!\" --revision-name=nerc-revision\n</code></pre> </li> <li> <p>Find the name of the second, latest revision that was created when you redeployed     the service, by running the command:</p> <pre><code>oc get ksvc test-webapp -o=jsonpath='{.status.latestCreatedRevisionName}'\n</code></pre> <p>Output:</p> <pre><code>test-webapp-nerc-revision\n</code></pre> <p>At this point, both the first and second revisions of the service are deployed and running.</p> </li> <li> <p>Update your existing service to create a new test endpoint for the second     revision, while directing all other traffic to the first revision.</p> <p>Example of updated service spec with test endpoint:</p> <pre><code>...\nspec:\n    traffic:\n    - revisionName: &lt;first_revision_name&gt;\n        percent: 100  # All traffic is still routed to the first revision\n    - revisionName: &lt;second_revision_name&gt;\n        percent: 0    # No traffic is routed to the second revision\n        tag: v2       # Named route for testing\n...\n</code></pre> <p>After redeploying the service by reapplying the YAML, the second revision is staged. No traffic is routed to this revision at the main URL, and Knative creates a new service named <code>v2</code> for testing the newly deployed revision.</p> </li> <li> <p>Get the URL of the new service for the second revision, by running the     following command:</p> <pre><code>oc get ksvc test-webapp --output jsonpath=\"{.status.traffic[*].url}\"\n</code></pre> <p>You can use this URL i.e. <code>https://v2-test-webapp-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code> to verify that the new version of the application is functioning correctly before routing any traffic to it.</p> </li> <li> <p>Update your existing service to split traffic evenly, sending 50% of traffic     to the first revision and 50% to the second revision.</p> <p>Example of updated service spec splitting traffic 50/50 between revisions:</p> <pre><code>...\nspec:\n    traffic:\n    - revisionName: &lt;first_revision_name&gt;\n        percent: 50\n    - revisionName: &lt;second_revision_name&gt;\n        percent: 50\n        tag: v2\n...\n</code></pre> </li> <li> <p>When ready to route all traffic to the new version, update the service to     send 100% of traffic to the second revision.</p> <p>Example of updated service spec sending all traffic to the second revision:</p> <pre><code>...\nspec:\n    traffic:\n    - revisionName: &lt;first_revision_name&gt;\n        percent: 0\n    - revisionName: &lt;second_revision_name&gt;\n        percent: 100\n        tag: v2\n</code></pre> <p>Important Note</p> <p>If you do not plan to roll back to the first revision, you can remove it instead of setting its traffic to 0%. Non-routable revisions are automatically garbage-collected.</p> </li> <li> <p>Visit the URL of the first revision to confirm that no traffic is being     routed to the old version of the application.</p> </li> </ol> <p>There are many other configurable options to manage the scaling of your workloads, as outlined in the documentation.</p> <p>For more information, see the documentation on Getting started with OpenShift Serverless using Knative Serving.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/","title":"Scaling and Performance Guide","text":""},{"location":"openshift/applications/scaling-and-performance-guide/#scaling-and-performance-guide","title":"Scaling and Performance Guide","text":""},{"location":"openshift/applications/scaling-and-performance-guide/#understanding-pod","title":"Understanding Pod","text":"<p>Pods serve as the smallest unit of compute that can be defined, deployed, and managed within the OpenShift Container Platform (OCP). The OCP utilizes the Kubernetes concept of a pod, which consists of one or more containers deployed together on a single host.</p> <p>Pods are essentially the building blocks of a Kubernetes cluster, analogous to a machine instance (either physical or virtual) for a container. Each pod is assigned its own internal IP address, granting it complete ownership over its port space. Additionally, containers within a pod can share local storage and network resources.</p> <p>The lifecycle of a pod typically involves several stages: first, the pod is defined; then, it is scheduled to run on a node within the cluster; finally, it runs until its container(s) exit or until it is removed due to some other circumstance. Depending on the cluster's policy and the exit code of its containers, pods may be removed after exiting, or they may be retained to allow access to their container logs.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#example-pod-configurations","title":"Example pod configurations","text":"<p>The following is an example definition of a pod from a Rails application. It demonstrates many features of pods, most of which are discussed in other topics and thus only briefly mentioned here:</p> <p></p> <ol> <li> <p>Pods can be \"tagged\" with one or more labels, which can then be used to select    and manage groups of pods in a single operation. The labels are stored in key/value    format in the <code>metadata</code> hash.</p> </li> <li> <p>The pod restart policy with possible values <code>Always</code>, <code>OnFailure</code>, and <code>Never</code>.    The default value is <code>Always</code>. Read this    to learn about \"Configuring how pods behave after restart\".</p> </li> <li> <p>OpenShift Container Platform defines a security context for containers which    specifies whether they are allowed to run as privileged containers, run as a user    of their choice, and more. The default context is very restrictive but administrators    can modify this as needed.</p> </li> <li> <p><code>containers</code> specifies an array of one or more container definitions.</p> </li> <li> <p>The container specifies where external storage volumes are mounted within the    container. In this case, there is a volume for storing access to credentials the    registry needs for making requests against the OpenShift Container Platform API.</p> </li> <li> <p>Specify the volumes to provide for the pod. Volumes mount at the specified path.    Do not mount to the container root, <code>/</code>, or any path that is the same in the host    and the container. This can corrupt your host system if the container is sufficiently    privileged, such as the host <code>/dev/pts</code> files. It is safe to mount the host by    using <code>/host</code>.</p> </li> <li> <p>Each container in the pod is instantiated from its own container image.</p> <p>Rate Limits While Pulling Container Image</p> <p>By default, Container Images are pulled from Registry i.e. Docker Hub or other commercial and private registries, which enforce rate limits on anonymous users. If your setup involves frequent image pulls, you may face these restrictions.</p> <p>You need to create a Secret and link it to your Service Account or include it within the <code>spec.imagePullSecrets</code>, as explained in this document, which provides instructions on how to resolve the rate limit issue while pulling container images.</p> </li> <li> <p>Pods making requests against the OpenShift Container Platform API is a common    enough pattern that there is a <code>serviceAccount</code> field for specifying which service    account user the pod should authenticate as when making the requests. This enables    fine-grained access control for custom infrastructure components.</p> </li> <li> <p>The pod defines storage volumes that are available to its container(s) to use.    In this case, it provides an ephemeral volume for a secret volume containing the    default service account tokens. If you attach persistent volumes that have high    file counts to pods, those pods can fail or can take a long time to start.</p> </li> </ol> <p>Viewing pods</p> <p>You can refer to this user guide on how to view all pods, their usage statics (i.e. CPU, memory, and storage consumption) and logs in your project using the OpenShift CLI (<code>oc</code>) commands.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#compute-resources","title":"Compute Resources","text":"<p>Containers run inside pods, and every container consumes compute resources. Each container running on a node consumes compute resources, which are measurable quantities that can be requested, allocated, and consumed.</p> <p>When authoring a pod configuration YAML file, you can optionally specify how much CPU, memory (RAM), and local ephemeral storage each container needs in order to better schedule pods in the cluster and ensure satisfactory performance as shown below:</p> <p></p>"},{"location":"openshift/applications/scaling-and-performance-guide/#how-to-define-cpu-and-memory","title":"How to Define CPU and Memory?","text":"<p>CPU, Memory, and Ephemeral Storage Units Explained</p> <p>CPU is measured in units called millicores, where 1000 millicores (\"m\") = 1 vCPU or 1 Core. Each node in a cluster inspects the operating system to determine the amount of CPU cores on the node, then multiplies that value by 1000 to express its total capacity. For example, if a node has 2 cores, the node's CPU capacity would be represented as 2000m. If you wanted to use 1/10 of a single core, it would be represented as 100m.</p> <p>Memory and ephemeral storage are measured in bytes. In addition, it may be used with SI suffixes (E, P, T, G, M, K) or their power-of-two-equivalents (Ei, Pi, Ti, Gi, Mi, Ki).</p> <p>CPU and memory can be specified in a couple of ways:</p> <p>Resource requests and limits are optional parameters specified at the container level. OpenShift computes a Pod's request and limit as the sum of requests and limits across all of its containers. OpenShift then uses these parameters for scheduling and resource allocation decisions.</p> <p></p> <ul> <li> <p>The request value specifies the minimum resource the container is guaranteed.     The request value is also used by the scheduler to assign pods to nodes during     scheduling.</p> <p>Pods will get the amount of memory they request. If they exceed their memory request, they could be killed if another pod happens to need this memory. Pods are only ever killed when using less memory than requested if critical system or high priority workloads need the memory utilization.</p> <p>Likewise, each container within a Pod is granted the CPU resources it requests, subject to availability. Additional CPU cycles may be allocated if resources are available and not required by other active Pods/Jobs.</p> <p>Important Information</p> <p>If a Pod's total requests are not available on a single node, then the Pod will remain in a Pending state (i.e. not running) until these resources become available.</p> </li> <li> <p>The limit value specifies the upper cap. A container cannot exceed this.     Limit is the value applications should be tuned to use. Pods will be memory,     CPU throttled when they exceed their available memory and CPU limit.</p> </li> </ul> <p>What happens if I did not specify the Compute Resources on Pod YAML?</p> <p>If you don't specify the compute resources for your objects i.e. containers, to restrict them from running with unbounded compute resources from our cluster the objects will use the limit ranges specified for your project namespace. With limit ranges, we restrict resource consumption for specific objects in a project. You can also be able to view the current limit range for your project by going into the Administration -&gt; LimitRange menu as shown below:</p> <p></p>"},{"location":"openshift/applications/scaling-and-performance-guide/#updating-compute-resources-of-a-running-pod-via-resource-limits","title":"Updating Compute Resources of a Running Pod via Resource Limits","text":"<p>Tips</p> <p>Changing CPU/Memory requests affects scheduling and also the HPA's CPU% target math.</p> <p>Keep <code>limits \u2265 requests</code>; memory usage above the limit can trigger OOM kills.</p> <p>Resource limits control how much CPU and memory a container will consume on a node. You can specify a limit on how much memory and CPU a container can consume in both request and limit values. You can also specify the min request and max limit of a given container as well as the max ratio between request and limit.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#using-nerc-ocp-web-console","title":"Using NERC OCP Web Console","text":"<p>We can easily configure and modify the Resource Limit by right-click the application to see the edit options available as shown below:</p> <p></p> <p>Then selecting the Edit resource limits link to set the amount of CPU and Memory resources a container is guaranteed or allowed to use when running. In the pod specifications, you must specify the resource requests, such as CPU and memory as described here.</p> <p>The HPA uses this specification to determine the resource utilization and then scales the target up or down. Utilization values are calculated as a percentage of the resource requests of each pod. Missing resource request values can affect the optimal performance of the HPA.</p> <p></p>"},{"location":"openshift/applications/scaling-and-performance-guide/#using-oc-cli-command","title":"Using <code>oc</code> CLI command","text":"<p>You can also update the Pod template in the controller (Deployment/StatefulSet/DaemonSet) using YAML and roll out new pods with the new limits/requests.</p> <p>Handy checks / rollback</p> <p>To See current resources for container \"app\" by running:</p> <pre><code>oc get deploy my-app -o jsonpath='{.spec.template.spec.containers[?(@.name==\"app\")].resources}'\n</code></pre> <p>To Roll back if needed:</p> <pre><code>oc rollout undo deploy/my-app\n</code></pre>"},{"location":"openshift/applications/scaling-and-performance-guide/#update-a-deployment-recommended","title":"Update a Deployment (recommended)","text":"<p>Edit your Deployment YAML to set <code>resources</code> on each container, then <code>oc apply -f &lt;file&gt;.yaml</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels: { app: my-app }\n  template:\n    metadata:\n      labels: { app: my-app }\n    spec:\n      containers:\n        - name: app\n          image: your-registry/your-image:tag\n          # \u2b07\ufe0f Update these\n          resources:\n            requests:\n              cpu: \"250m\"\n              memory: \"256Mi\"\n            limits:\n              cpu: \"1\"        # 1 vCPU\n              memory: \"512Mi\"\n</code></pre> <p>After you apply the changes, OpenShift performs a rolling update: old pods are terminated and new pods start with the updated resources. To verify the rollout, run <code>oc rollout status deploy/my-app</code> (replace <code>my-app</code> with the value of <code>metadata.name</code> in your Deployment).</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#patch-the-deployment-no-file-needed","title":"Patch the Deployment (no file needed)","text":"<ul> <li> <p>Using Strategic-merge patch (by container name):</p> <pre><code>oc patch deploy/my-app --type=strategic -p \\\n'{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"resources\":{\"requests\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"512Mi\"}}}]}}}}'\noc rollout status deploy/my-app\n</code></pre> </li> <li> <p>Using JSON Patch (replace <code>resources</code> on first container):</p> <pre><code>oc patch deploy/my-app --type=json -p='[\n{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/resources\",\n\"value\":{\"requests\":{\"cpu\":\"250m\",\"memory\":\"256Mi\"},\"limits\":{\"cpu\":\"1\",\"memory\":\"512Mi\"}}}\n]'\n</code></pre> </li> </ul>"},{"location":"openshift/applications/scaling-and-performance-guide/#one-liner-helper","title":"One-liner helper","text":"<pre><code>oc set resources deployment/my-app \\\n  --containers=app \\\n  --requests=cpu=250m,memory=256Mi \\\n  --limits=cpu=1,memory=512Mi\noc rollout status deploy/my-app\n</code></pre>"},{"location":"openshift/applications/scaling-and-performance-guide/#standalone-pod-not-controlled-by-a-deployment","title":"Standalone Pod (not controlled by a Deployment)","text":"<p>Pods are mostly immutable. To change resources, recreate the Pod with the new values:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: app\n      image: your-registry/your-image:tag\n      resources:\n        requests:\n          cpu: \"250m\"\n          memory: \"256Mi\"\n        limits:\n          cpu: \"1\"\n          memory: \"512Mi\"\n</code></pre> <p>Note: Delete the old pod and create the new one (same name if desired).</p> <p>Very Important Information on Using <code>oc patch</code></p> <p>We can use <code>oc patch</code> command on the controller's Pod template (Deployment/StatefulSet/DaemonSet). That triggers a rolling update with the new requests/limits. Policies like LimitRange/ResourceQuota can block higher values. If you use an HPA, remember its CPU target is based on requests, so keep those set appropriately. But we can't patch a standalone Pod's container resources; we can recreate it instead as explained here.</p> <p>We can also remove <code>limits</code> using JSON Patch by running:</p> <pre><code>oc patch deploy/my-app --type=json -p='[\n    {\"op\":\"remove\",\"path\":\"/spec/template/spec/containers/0/resources/limits\"}\n]'\n</code></pre>"},{"location":"openshift/applications/scaling-and-performance-guide/#how-to-specify-pod-to-use-gpu","title":"How to specify pod to use GPU?","text":"<p>So from a Topology view, the only thing you have to worry about is asking for GPU resources when defining your pods, with something like the following for requesting (NVIDIA A100 GPU):</p> <pre><code>spec:\n  containers:\n  - name: &lt;Your Pod Name&gt;\n    image: &lt;Your GPU Enabled Container Image&gt;\n    resources:\n      requests:\n        ...\n        nvidia.com/gpu: &lt;Number Of GPUs&gt;\n      limits:\n        ...\n        nvidia.com/gpu: &lt;Number Of GPUs&gt;\n  tolerations:\n    - key: nvidia.com/gpu.product\n      operator: Equal\n      value: &lt;GPU Type&gt;\n      effect: NoSchedule\n  nodeSelector:\n    nvidia.com/gpu.product: &lt;GPU Type&gt;\n</code></pre> <p>GPU Resource Spec: resources, tolerations &amp; nodeSelector</p> <p>When requesting GPU resources directly from pods and deployments, you must include the <code>spec.tolerations</code> and <code>spec.nodeSelector</code> shown above, for your desired GPU type. Also, the <code>spec.containers.resources.requests</code> and <code>spec.containers.resources.limits</code> needs to include the <code>nvidia.com/gpu</code> specification that indicates the number of GPUs you want in your container.</p> <p>In the sample Pod Spec above, you can allocate GPUs to containers by specifying the GPU resource <code>nvidia.com/gpu</code> and indicating the desired number of GPUs. This number should not exceed the GPU quota specified by the value of the \"OpenShift Request on GPU Quota\" attribute that has been approved for your \"NERC-OCP (OpenShift)\" resource allocation on NERC's ColdFront as described here.</p> <p>ColdFront OpenShift GPU Resource Quota</p> <p>There is no quota attribute to specify a desired GPU type. You can only specify the number of GPUs required per allocation (OpenShift Project) by setting the value in the \"OpenShift Request on GPU Quota\" attribute.</p> <p>If you need to increase the quota value for the total GPUs available on your OpenShift allocation, you can submit an allocation quota change request as explained here.</p> <p>Below is an example of a running pod YAML that requests the GPU device i.e. <code>NVIDIA-A100-SXM4-40GB</code> with a count of 2:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n      command: [\"sleep\"]\n      args: [\"infinity\"]\n      resources:\n        limits:\n          nvidia.com/gpu: 2\n  tolerations:\n    - key: nvidia.com/gpu.product\n      operator: Equal\n      value: NVIDIA-A100-SXM4-40GB\n      effect: NoSchedule\n  nodeSelector:\n    nvidia.com/gpu.product: NVIDIA-A100-SXM4-40GB\n</code></pre> <p>On opened YAML editor paste the contents of the above given pod YAML as shown below:</p> <p></p> <p>After the pod is running, navigate to the pod details and execute the following command in the Terminal to view the currently available NVIDIA GPU devices:</p> <p></p> <p>Additionally, you can execute the following command to narrow down and retrieve the name of the GPU device:</p> <pre><code>nvidia-smi --query-gpu=gpu_name --format=csv,noheader --id=0 | sed -e 's/ /-/g'\n\nNVIDIA-A100-SXM4-40GB\n</code></pre>"},{"location":"openshift/applications/scaling-and-performance-guide/#how-to-select-a-different-gpu-device","title":"How to select a different GPU device?","text":"<p>We can specify information about the GPU product type, family, count, and so on, as shown in the Pod Spec above. Also, these node labels can be used in the Pod Spec to schedule workloads based on criteria such as the GPU device name, as shown under nodeSelector to specify a different NVIDIA GPU.</p> <p>Below is an example of a running pod YAML that requests the GPU device i.e. <code>Tesla-V100-PCIE-32GB</code> with a count of 1:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod2\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n      command: [\"sleep\"]\n      args: [\"infinity\"]\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n          nvidia.com/gpu: 1\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n          nvidia.com/gpu: 1\n  tolerations:\n    - key: nvidia.com/gpu.product\n      operator: Equal\n      value: Tesla-V100-PCIE-32GB\n      effect: NoSchedule\n  nodeSelector:\n    nvidia.com/gpu.product: Tesla-V100-PCIE-32GB\n</code></pre> <p>When you run the <code>nvidia-smi</code> command in the terminal, you can observe the availability of the different V100 NVIDIA GPU device, as shown below:</p> <p></p>"},{"location":"openshift/applications/scaling-and-performance-guide/#requesting-an-h100-gpu-in-a-pod-specification","title":"Requesting an H100 GPU in a Pod Specification","text":"<p>Below is an example of a running pod YAML that requests the GPU device <code>NVIDIA-H100-80GB-HBM3</code> with a count of 1:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod3\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n      command: [\"sleep\"]\n      args: [\"infinity\"]\n      resources:\n        limits:\n          nvidia.com/gpu: 1\n  tolerations:\n    - key: nvidia.com/gpu.product\n      operator: Equal\n      value: NVIDIA-H100-80GB-HBM3\n      effect: NoSchedule\n  nodeSelector:\n    nvidia.com/gpu.product: NVIDIA-H100-80GB-HBM3\n</code></pre> <p>When you run the <code>nvidia-smi</code> command in the terminal, you can observe the availability of the different H100 NVIDIA GPU device, as shown below:</p> <p></p>"},{"location":"openshift/applications/scaling-and-performance-guide/#scaling","title":"Scaling","text":"<p>Scaling defines the number of pods or instances of the application you want to deploy. Bare pods not managed by a replication controller will not be rescheduled in the event of a node disruption. You can deploy your application using <code>Deployment</code> or <code>Deployment Config</code> objects to maintain the desired number of healthy pods and manage them from the web console. You can create deployment strategies that help reduce downtime during a change or an upgrade to the application. For more information about deployment, please read this.</p> <p>Benefits of Scaling</p> <p>This will allow for a quicker response to peaks in demand, and reduce costs by automatically scaling down when resources are no longer needed. This is especially useful for GPU-based resources, which are often limited and more costly.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#scaling-application-pods-resources-and-observability","title":"Scaling application pods, resources and observability","text":"<p>The Topology view provides the details of the deployed components in the Overview panel. You can use the Details, Resources and Observe tabs to scale the application pods, check build status, services, routes, metrics, and events as follows:</p> <p>Click on the component node to see the Overview panel to the right.</p> <p>Use the Details tab to:</p> <ul> <li> <p>Scale your pods using the up and down arrows to increase or decrease the number     of pods or instances of the application manually as shown below:</p> <p></p> <p>Alternatively, we can easily configure and modify the pod counts by right-click the application to see the edit options available and selecting the Edit Pod Count as shown below:</p> <p></p> </li> <li> <p>Check the Labels, Annotations, and Status of the application.</p> </li> </ul> <p>Click the Resources tab to:</p> <ul> <li> <p>See the list of all the pods, view their status, access logs, and click on the     pod to see the pod details.</p> </li> <li> <p>See the builds, their status, access logs, and start a new build if needed.</p> </li> <li> <p>See the services and routes used by the component.</p> </li> </ul> <p>Click the Observe tab to:</p> <ul> <li> <p>See the metrics to see CPU usage, Memory usage and Bandwidth consumption.</p> </li> <li> <p>See the Events.</p> </li> </ul>"},{"location":"openshift/applications/scaling-and-performance-guide/#scaling-manually","title":"Scaling manually","text":"<p>To manually scale a <code>DeploymentConfig</code> object, use the <code>oc scale</code> command.</p> <pre><code>oc scale dc &lt;dc_name&gt; --replicas=&lt;replica_count&gt;\n</code></pre> <p>For example, the following command sets the replicas in the frontend <code>DeploymentConfig</code> object to 3.</p> <pre><code>oc scale dc frontend --replicas=3\n</code></pre> <p>The number of replicas eventually propagates to the desired and current state of the deployment configured by the <code>DeploymentConfig</code> object <code>frontend</code>.</p> <p>Scaling applications based on a schedule (Cron)</p> <p>You can also integrate schedule based scaling uses OpenShift/Kubernetes native resources called CronJob that execute a task periodically (date + time) written in Cron format. For example, scaling an app to 5 replicas at 0900; and then scaling it down to 1 pod at 2359. To learn more about this, please refer to this blog post.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#autoscaling","title":"AutoScaling","text":"<p>We can configure automatic scaling, or autoscaling, for applications to match incoming demand. This feature automatically adjusts the scale of a replication controller or deployment configuration based on metrics collected from the pods belonging to that replication controller or deployment configuration. You can create a Horizontal Pod Autoscaler (HPA) for any deployment, deployment config, replica set, replication controller, or stateful set.</p> <p>For instance, if an application receives no traffic, it is scaled down to the minimum number of replicas configured for the application. Conversely, replicas can be scaled up to meet demand if traffic to the application increases.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#understanding-horizontal-pod-autoscalers-hpa","title":"Understanding Horizontal Pod Autoscalers (HPA)","text":"<p>If your application experiences fluctuating load, you can use a Horizontal Pod Autoscaler (HPA) to specify the minimum and maximum number of pods to run, as well as the target CPU utilization or memory utilization for your pods.</p> <p>Configure HPA to automatically scale your Pods based on CPU utilization and a custom metric.</p> Metric Description CPU Utilization Number of CPU cores used. Can be used to calculate a percentage of the pod's requested CPU. Memory Utilization Amount of memory used. Can be used to calculate a percentage of the pod's requested memory. <p>After you create a HPA, OCP begins to query the CPU and/or memory resource metrics on the pods. When these metrics are available, the HPA computes the ratio of the current metric utilization with the desired metric utilization, and scales up or down accordingly. The query and scaling occurs at a regular interval, but can take one to two minutes before metrics become available.</p> <p>For replication controllers, this scaling corresponds directly to the replicas of the replication controller. For deployment configurations, scaling corresponds directly to the replica count of the deployment configuration. Note that autoscaling applies only to the latest deployment in the <code>Complete</code> phase.</p> <p>For more information on how the HPA works, read this documentation.</p> <p>Very Important Note</p> <p>To implement the HPA, all targeted pods must have a Resource limits set on their containers. The HPA will not have CPU and Memory metrics until Resource limits are set. CPU request and limit must be set before CPU utilization can be set. Memory request and limit must be set before Memory utilization can be set.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#creating-a-hpa-by-using-the-web-console","title":"Creating a HPA by using the web console","text":"<p>From the web console, you can create a HPA that specifies the minimum and maximum number of pods you want to run on a <code>Deployment</code> or <code>DeploymentConfig</code> object. You can also define the amount of CPU or memory usage that your pods should target. The HPA increases and decreases the number of replicas between the minimum and maximum numbers to maintain the specified CPU utilization across all pods.</p>"},{"location":"openshift/applications/scaling-and-performance-guide/#to-create-an-hpa-in-the-web-console","title":"To create an HPA in the web console","text":"<ul> <li> <p>In the Topology view, click the node to reveal the side pane.</p> </li> <li> <p>From the Actions drop-down list, select Add HorizontalPodAutoscaler as     shown below:</p> <p></p> </li> <li> <p>This will open the Add HorizontalPodAutoscaler form as shown below:</p> <p></p> <p>Configure via: Form or YAML View</p> <p>While creating or editing the horizontal pod autoscaler in the web console, you can switch from Form view to YAML view.</p> </li> <li> <p>From the Add HorizontalPodAutoscaler form, define the name, minimum and maximum     pod limits, the CPU and memory usage, and click Save.</p> </li> </ul>"},{"location":"openshift/applications/scaling-and-performance-guide/#to-edit-an-hpa-in-the-web-console","title":"To edit an HPA in the web console","text":"<ul> <li> <p>In the Topology view, click the node to reveal the side pane.</p> </li> <li> <p>From the Actions drop-down list, select Edit HorizontalPodAutoscaler     to open the Edit Horizontal Pod Autoscaler form.</p> </li> <li> <p>From the Edit Horizontal Pod Autoscaler form, edit the minimum and maximum     pod limits and the CPU and memory usage, and click Save.</p> </li> </ul>"},{"location":"openshift/applications/scaling-and-performance-guide/#to-remove-an-hpa-in-the-web-console","title":"To remove an HPA in the web console","text":"<ul> <li> <p>In the Topology view, click the node to reveal the side panel.</p> </li> <li> <p>From the Actions drop-down list, select Remove HorizontalPodAutoscaler.</p> </li> <li> <p>In the confirmation pop-up window, click Remove to remove the HPA.</p> </li> </ul> <p>Best Practices</p> <p>Read this document to learn more about best practices regarding Horizontal Pod Autoscaler (HPA) autoscaling.</p>"},{"location":"openshift/applications/understanding-openshift-serverless/","title":"Understanding OpenShift Serverless","text":""},{"location":"openshift/applications/understanding-openshift-serverless/#serverless-computing","title":"Serverless Computing","text":"<p>Serverless is a cloud-native development model that enables developers to build and run applications without managing servers. The term \"serverless\" does not mean that servers do not exist; rather, it means that server management is abstracted away from application development. In simple terms, serverless handles scaling, resource allocation, and other operational tasks, allowing you to focus entirely on writing code.</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#what-is-serverless","title":"What is Serverless?","text":"<ul> <li> <p>A deployment model for containers and functions that allocates resources on demand.</p> </li> <li> <p>Enables developers to focus on delivering applications.</p> </li> <li> <p>Eliminates the need for server management tasks.</p> </li> <li> <p>Supports event-driven architectures.</p> </li> </ul>"},{"location":"openshift/applications/understanding-openshift-serverless/#containers-vs-serverless","title":"Containers vs Serverless","text":"Technology Description Containers Containers enable more efficient resource utilization than virtual machines because they do not include a full operating system per container. They have a smaller resource footprint, and container orchestration makes scaling up or down easier. Serverless Provides all the resource-utilization benefits of containers, but resources are released once the application function completes. A serverless model can scale up and down rapidly as needed."},{"location":"openshift/applications/understanding-openshift-serverless/#features-of-a-serverless-model","title":"Features of a Serverless Model","text":"<p>The following are key features of a serverless model:</p> Feature Description Stateless Functions Serverless applications are ideally single-purpose and stateless. For example, a function may query a database and return the results without retaining state. Simplified Deployment Functions can be deployed using GitOps, a command-line interface (CLI), or Kubernetes manifests, without requiring changes to existing DevOps practices. Event-Driven A serverless model relies on triggers to execute code, such as API requests or events from a message queue. It processes demand alerts or user requests as they arrive, reducing idle resource costs. Scale to Zero Automatic scaling ensures that services run only when needed, making this model ideal for bursty traffic. The ability to scale to zero means code executes only in response to events - and resources are automatically released once execution completes. Improved Response Times The application starts faster when a user accesses it, reducing wait time. Enhanced Resilience Stateless functions, autoscaling, and built-in failover mean fewer single points of failure - and more robust uptime."},{"location":"openshift/applications/understanding-openshift-serverless/#benefits-of-serverless","title":"Benefits of Serverless","text":"<p>Each evolution of a deployment model brings advantages over previous approaches, most notably cost savings and more efficient use of CPU, memory, and storage resources.</p> Category Serverless Cost Code is executed only when needed, with no idle time. If no requests are received, there is no cost. You pay only for execution time. No more paying for idle pods - each component scales to zero when not needed. Infrastructure Management There are no servers to manage. Patching, security updates, and most monitoring responsibilities are handled by the platform. Scalability Because the underlying platform typically has ample resources, scaling is simpler. There is less need to plan or manage capacity in advance. High Availability High availability is handled by the platform. Code can be executed concurrently as many times as needed to meet demand."},{"location":"openshift/applications/understanding-openshift-serverless/#knative-and-openshift-serverless","title":"Knative and OpenShift Serverless","text":"<p>Red Hat OpenShift Serverless is built on Knative, an open-source project that provides a serverless platform for deploying and managing applications.</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#introduction-to-knative","title":"Introduction to Knative","text":"<p>Knative is an open-source project that enables the deployment and management of modern serverless workloads on OpenShift/Kubernetes. Knative bridges the gap between the power of Kubernetes and the simplicity of serverless, event-driven computing. It adds a serverless application layer on top of OpenShift/Kubernetes, allowing you to deploy any modern application workload - from monolithic applications and microservices to small, single-purpose functions.</p> <p>It has three primary components:</p> <ul> <li> <p>Serving: Enables rapid deployment and     automatic scaling of containers through a request-driven model, serving workloads     based on demand.</p> </li> <li> <p>Eventing: Provides the infrastructure     to consume and produce events that trigger applications. Applications can be     triggered by internal event sources, cloud services, or Red Hat AMQ streams.</p> </li> <li> <p>Functions: Offers a flexible approach     to building source code into containers.</p> </li> </ul>"},{"location":"openshift/applications/understanding-openshift-serverless/#what-is-openshift-serverless","title":"What is OpenShift Serverless?","text":"<p>Red Hat OpenShift Serverless is an enterprise-grade serverless platform built on Knative, offering developers a complete set of tools to build, deploy, and manage serverless applications on OpenShift. OpenShift Serverless enables applications to scale up or down to zero on demand. It allows developers to run Kubernetes-native, event-driven, stateless workloads for microservices, containers, and Function-as-a-Service (FaaS) implementations, while automatically handling infrastructure management and resource scaling.</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#openshift-serverless-architecture","title":"OpenShift Serverless Architecture","text":"<p>The following illustration depicts the architecture of Red Hat OpenShift Serverless:</p> <p></p>"},{"location":"openshift/applications/understanding-openshift-serverless/#traditional-kubernetes-vs-knative-deployments","title":"Traditional Kubernetes vs Knative Deployments","text":"<p>Traditional Kubernetes Deployment:</p> <p>In a traditional Kubernetes deployment, a container image is hosted in a registry, and a YAML file defines the deployment. When applied to the cluster, the deployment creates a ReplicaSet, which then launches the specified number of pods. A service is also created to match the labels of these pods.</p> <p>Additional steps may be required depending on the scenario, such as creating routes if the service cannot access a cloud load balancer. Overall, a traditional Kubernetes deployment involves multiple steps, lengthy YAML files, and several moving components.</p> <p></p> <p>Knative Deployment:</p> <p>With serverless, infrastructure management is handled for you. You simply provide a container image and run it on the cluster - everything else is automatically managed.</p> <p>In Knative, deployment is even simpler: you define a small resource file called a Knative Service, which specifies the container image to run. When applied to the cluster, the Knative Operator automatically creates all necessary resources, including the deployment, service, route (if needed), and a configuration resource. The configuration resource also manages revisions, making it easy to roll back to previous versions when required.</p> <p></p>"},{"location":"openshift/applications/understanding-openshift-serverless/#examples-of-openshift-serverless-workloads","title":"Examples of OpenShift Serverless Workloads","text":"<p>OpenShift Serverless workloads typically follow this workflow:</p> <ul> <li> <p>A request is received.</p> </li> <li> <p>A pod is created to handle the request.</p> </li> <li> <p>The pod processes the request.</p> </li> <li> <p>The pod is terminated.</p> </li> </ul>"},{"location":"openshift/applications/understanding-openshift-serverless/#components-apis","title":"Components &amp; APIs","text":"<p>The following illustration shows the CRDs set up by OpenShift Serverless to enable Knative serverless application components:</p> <p></p> <p>The main components of the OpenShift Serverless architecture are:</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#knative-serving","title":"Knative Serving","text":"<p>Enables developers to create cloud-native applications using a serverless architecture. It provides Custom Resource Definitions (CRDs) that developers can use to deploy serverless containers, manage pod scaling, and more.</p> <p>Knative Serving is responsible for:</p> <ul> <li> <p>Deploying applications</p> </li> <li> <p>Updating applications</p> </li> <li> <p>Routing traffic to applications</p> </li> <li> <p>Auto-scaling applications</p> </li> </ul> <p>Knative Serving provides autoscaling, route management, and container lifecycle management for HTTP based workloads. Built on Kubernetes, it allows deploying and serving applications and functions as serverless containers, simplifying deployment, dynamically scaling with traffic, and supporting traffic-split rollouts.</p> <p>Watch a video about Knative Serving</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#knative-eventing","title":"Knative Eventing","text":"<p>Provides the infrastructure for building and deploying event-driven applications. It allows developers to define event sources and sinks and offers mechanisms to route events to functions, applications, or other event sinks.</p> <p>Knative Eventing is an event-driven platform that uses CloudEvents with a simple HTTP interface. It enables late-binding of event sources and consumers, allowing services to be loosely coupled, independently deployed, and connected without modifying producers or consumers.  </p> <p>Event Mesh: Supports responsive, scalable, and resilient architectures by decoupling components and enabling asynchronous communication.</p> <p>Watch a video about Knative Eventing</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#knative-functions","title":"Knative Functions","text":"<p>Knative Functions lets developers focus on business logic by writing small, executable serverless functions deployed as Knative Services. These functions use Knative Serving and Eventing to provide efficient, scalable, and fast development for stateless, event-driven workloads on OpenShift.</p> <p>Watch a video about Knative Functions</p>"},{"location":"openshift/applications/understanding-openshift-serverless/#serverless-workflows","title":"Serverless Workflows","text":"<p>Simplify complex workflows with Serverless Logic, built on the Serverless Workflow foundation. Define and run event-driven applications, automate tasks, integrate services, and scale workflows without managing infrastructure.</p> <p>Watch a video about OpenShift Serverless Logic</p> <p>OpenShift Serverless applications can be integrated with other OpenShift services, such as OpenShift Pipelines, and Service Mesh, delivering a complete serverless application development and deployment experience.</p> <p>For more information, see the documentation on Red Hat OpenShift Serverless.</p>"},{"location":"openshift/decommission/decommission-openshift-resources/","title":"Decommission OpenShift Resources","text":""},{"location":"openshift/decommission/decommission-openshift-resources/#decommission-openshift-resources","title":"Decommission OpenShift Resources","text":"<p>You can decommission all of your NERC OpenShift resources sequentially as outlined below.</p>"},{"location":"openshift/decommission/decommission-openshift-resources/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Backup: Back up any critical data or configurations stored on the resources     that going to be decommissioned. This ensures that important information is not     lost during the process.</p> </li> <li> <p>Kubernetes Objects (Resources): Please review all OpenShift Kubernetes Objects     (Resources) to ensure they are not actively used and ready to be decommissioned.</p> </li> <li> <p>Install and configure the OpenShift CLI (oc), see How to Setup the     OpenShift CLI Tools     for more information.</p> </li> </ul>"},{"location":"openshift/decommission/decommission-openshift-resources/#delete-all-resources-from-the-nerc-openshift-and-openshift-ai","title":"Delete all resources from the NERC OpenShift and OpenShift AI","text":"<p>Run <code>oc login</code> in your local machine's terminal using your own token to authenticate and access all your projects on the NERC OpenShift as described here. Please ensure you have already selected the correct project that needs to be decommissioned, as shown below:</p> <pre><code>oc login --token=&lt;your_token&gt; --server=https://api.shift.nerc.mghpcc.org:6443\nLogged into \"https://api.shift.nerc.mghpcc.org:6443\" as \"test1_user@fas.harvard.edu\" using the token provided.\n\nYou have access to the following projects and can switch between them with 'oc project &lt;projectname&gt;':\n\n    test-project-1\n* test-project-2\n    test-project-3\n\nUsing project \"test-project-2\".\n</code></pre> <p>Switching to your project that need to be decommissioned by running <code>oc project &lt;projectname&gt;</code> command:</p> <pre><code>oc project &lt;your_openshift_project_to_decommission&gt;\nUsing project \"&lt;your_openshift_project_to_decommission&gt;\" on server \"https://api.shift.nerc.mghpcc.org:6443\".\n</code></pre> <p>Please confirm the correct project is being selected by running <code>oc project</code>, as shown below:</p> <pre><code>oc project\nUsing project \"&lt;your_openshift_project_to_decommission&gt;\" on server \"https://api.shift.nerc.mghpcc.org:6443\".\n</code></pre> <p>Important Note: Best Practice for Specifying Namespace in <code>oc</code> Commands.</p> <p>The best practice is to specify the namespace in each <code>oc</code> command using the <code>-n</code> option, e.g., <code>-n &lt;your_openshift_project_to_decommission&gt;</code>. This ensures that your commands are always executed in the intended project, minimizing the risk of affecting the wrong resources.</p> <p>For example, the <code>oc get all</code> command can also be executed by specifying the namespace using the <code>-n</code> option, like this: <code>oc get all -n &lt;your_openshift_project_to_decommission&gt;</code>.</p> <p>Please review all resources currently being used by your project by running <code>oc get all</code>, as shown below:</p> <pre><code>oc get all\n\nNAME                                                                  READY   STATUS             RESTARTS       AGE\npod/ds-pipeline-persistenceagent-pipelines-definition-868665f7z9lpm   1/1     Running            0              141m\n...\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                               AGE\nservice/ds-pipeline-pipelines-definition   ClusterIP   172.30.133.168   &lt;none&gt;        8443/TCP,8888/TCP,8887/TCP            141m\n...\n\nNAME                                                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ds-pipeline-persistenceagent-pipelines-definition    1/1     1            1           141m\n...\n\nNAME                                                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ds-pipeline-persistenceagent-pipelines-definition-868665f748    1         1         1       141m\n...\n\nNAME                                                 IMAGE REPOSITORY\n                                                TAGS   UPDATED\nimagestream.image.openshift.io/simple-node-app-git   image-registry.openshift-image-registry.svc:5000/test-project-gpu-dc1e23/simple-node-app-git\n\nNAME                                                        HOST/PORT\n                                                PATH   SERVICES                           PORT            TERMINATION          WILDCARD\nroute.route.openshift.io/ds-pipeline-pipelines-definition   ds-pipeline-pipelines-definition-test-project-gpu-dc1e23.apps.shift.nerc.mghpcc.org          ds-pipeline-pipelines-definition   oauth           reencrypt/Redirect   None\n...\n</code></pre> <p>To list all Resources with their Names only.</p> <p>To list all resources with their names only, you can run this command: <code>oc get all -oname</code>.</p> <p>Here, <code>-oname</code> flag specifies the output format. In this case, it instructs the command to output only the names of the resources.</p> <p>Run the <code>oc delete</code> command to delete all resource objects specified as parameters after <code>--all</code> within your selected project (namespace).</p> <p>Danger</p> <p>The <code>oc delete</code> operation will cause all resources specfied will be deleted. This command can be very powerful and should be used with caution as it will delete all resources in the specified project.</p> <p>Always ensure that you are targeting the correct project (namespace) when using this command to avoid unintentional deletion of resources. If you're unsure which namespace you're currently in, run the oc project command to display the current project. To be safe, you can also specify the namespace in all <code>oc</code> commands by using the <code>-n</code> option, e.g., <code>-n &lt;your_openshift_project_to_decommission&gt;</code>.</p> <p>Make sure to backup any important data or configurations before executing this command to prevent accidental data loss.</p> <pre><code>oc delete pod,deployment,deploymentconfig,pvc,route,service,build,buildconfig,\nstatefulset,replicaset,replicationcontroller,job,cronjob,imagestream,revision,\nconfiguration,notebook --all\n</code></pre> <p>How to Delete All Allocations on a Project at Once</p> <p>If you have multiple allocations within a ColdFront project and want to delete all allocations associated with that project, you can run the following script.</p> <pre><code># pattern used for the \"Allocated Project Name\" attribute for the Allocation\n# that is based on the ColdFront Project Title\npattern=\"^&lt;your_openshift_project_to_decommission&gt;\"\nfor proj in $(oc get projects -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}' | grep \"$pattern\"); do\n    echo \"deleting resources\"\n    oc -n \"$proj\" delete pod,deployment,deploymentconfig,pvc,route,service,build,buildconfig,statefulset,replicaset,replicationcontroller,job,cronjob,imagestream,revision,configuration,notebook --all --ignore-not-found --wait=true || true\ndone\n</code></pre> <p>Please check all the resources currently being used by your project by running <code>oc get all</code>, as shown below:</p> <pre><code>oc get all\nNAME                        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                               AGE\nservice/modelmesh-serving   ClusterIP   None         &lt;none&gt;        8033/TCP,8008/TCP,8443/TCP,2112/TCP   7m4s\n</code></pre> <p>Important Note</p> <p>The last remaining service, i.e., <code>service/modelmesh-serving</code>, shown when running the <code>oc get all</code> command, is a REQUIRED resource, and so you don't need to clean it up.</p>"},{"location":"openshift/decommission/decommission-openshift-resources/#optional-remove-users-from-your-coldfront-project","title":"Optional: Remove User(s) from your ColdFront Project","text":"<p>Removing users from your ColdFront project via the NERC's ColdFront interface is straightforward. Simply click the \"Remove Users\" button for the corresponding project. This will display the following interface:</p> <p></p> <p>PI or project managers can select the user(s) and then click on the \"Remove Selected Users From Project\" button.</p> <p>Very Important</p> <p>If you remove a user (or users) from a project, they will automatically be removed from all allocations they were previously assigned to within that project.</p>"},{"location":"openshift/decommission/decommission-openshift-resources/#use-coldfront-to-reduce-the-storage-quota-to-zero","title":"Use ColdFront to reduce the Storage Quota to Zero","text":"<p>Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC-OCP (OpenShift) Resource Allocations, storage quotas are specified by the \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" allocation attributes.</p> <p>Even if you have deleted all Persistent Volume Claims (PVC) in your OpenShift project. It is very essential to adjust the approved values for your NERC-OCP (OpenShift) resource allocations to zero (0) otherwise you will still be incurring a charge for the approved storage as explained in Billing FAQs.</p> <p>To achieve this, you must submit a final change request to reduce the Storage Quotas for \"OpenShift Request on NESE Storage Quota (GiB)\" and \"OpenShift Limit on Ephemeral Storage Quota (GiB)\" to zero (0) for your NERC-OCP (OpenShift) resource type. You can review and manage these resource allocations by visiting the resource allocations. Here, you can filter the allocation of your interest and then proceed to request a change request.</p> <p>Very Important Note</p> <p>Although other allocated resources i.e. CPU, RAM, GPU, etc. operate on a pay-as-you-go model, wherein charges are incurred solely based on usage, Active (Needs Renewal) allocations after \"End Date\" will remain accessible to the users assigned under the allocation. It is advisable to set all other allocation quota attributes to zero (0) during the change request. This measure ensures that existing users will not accidentally use the resources from the project.</p> <p>Alternatively, PIs can control access to the allocation by removing users assigned to their NERC-OCP (OpenShift) allocation. This ensures that even if the allocation ends, users will not have access to the unused resources.</p> <p>Please make sure your change request looks like this:</p> <p></p> <p>Wait until the requested resource allocation gets approved by the NERC's admin.</p> <p>After approval, kindly review and verify that the quotas are accurately reflected in your resource allocation and OpenShift project. Please ensure that the approved quota values are accurately displayed as explained here.</p>"},{"location":"openshift/decommission/decommission-openshift-resources/#review-your-project-usage","title":"Review your Project Usage","text":"<p>Run the <code>oc describe quota</code> command to obtain detailed information about the resource quotas for all Resources defined within your selected project (namespace). Please note the name of the resource quota in the output of this command, i.e., <code>&lt;your_openshift_project_resource_quota_name&gt;</code>.</p> <pre><code>oc get quota\n\nNAME                              AGE   REQUEST                                                                               LIMIT\n&lt;your_openshift_project_resource_quota_name&gt;   105s   persistentvolumeclaims: 0/0, requests.nvidia.com/gpu: 0/0, requests.storage: 0/0   limits.cpu: 0/0, limits.ephemeral-storage: 0/0, limits.memory: 0/0\n</code></pre> <p>Very Important: Ensure No Resources that will be Billed are Used</p> <p>Most importantly, ensure that there is no active usage for any of your currently allocated project resources.</p> <p>To review the resource quota usage for your project, you can run <code>oc describe quota &lt;your_openshift_project_resource_quota_name&gt;</code>.</p> <p>Please ensure the output appears as follows, with all Used and Hard resources having a value of zero (0) as shown below:</p> <pre><code>oc describe quota &lt;your_openshift_project_resource_quota_name&gt;\n\nName:                     &lt;your_openshift_project_resource_quota_name&gt;\nNamespace:                &lt;your_openshift_project_to_decommission&gt;\nResource                  Used  Hard\n--------                  ----  ----\nlimits.cpu                0     0\nlimits.ephemeral-storage  0     0\nlimits.memory             0     0\npersistentvolumeclaims    0     0\nrequests.nvidia.com/gpu   0     0\nrequests.storage          0     0\n</code></pre> <p>Important Information</p> <p>Make sure to replace <code>&lt;your_openshift_project_resource_quota_name&gt;</code> with the actual name you find in the output, which is typically in this format: <code>&lt;your_openshift_project_to_decommission&gt;-project</code>.</p>"},{"location":"openshift/decommission/decommission-openshift-resources/#review-your-projects-resource-quota-from-the-openshift-web-console","title":"Review your Project's Resource Quota from the OpenShift Web Console","text":"<p>After removing all OpenShift resources and updating all resource quotas to set them to zero (0), you can review and verify that these changes are reflected in your OpenShift Web Console as well.</p> <p>When you are logged-in to the NERC's OpenShift Web Console, you can also view the resource quota for your project in the web console by navigating to Administration -&gt; ResourceQuotas.</p> <p>Click on your appropriate project name, i.e., <code>&lt;your_openshift_project_to_decommission&gt;</code>, to view the Resource Quota details.</p> <p></p> <p>Very Important Note</p> <p>It should also indicate that all resources have NO usage, i.e., zero (0), and also NO maximum set, i.e., zero (0), as shown below:</p> <p></p>"},{"location":"openshift/decommission/decommission-openshift-resources/#finally-archive-your-coldfront-project","title":"Finally, Archive your ColdFront Project","text":"<p>As a PI, you will now be able to Archive your ColdFront Project via accessing NERC's ColdFront interface. Please refer to these intructions on how to archive your projects that need to be decommissioned.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/","title":"Set up Domain Name for Your Application","text":""},{"location":"openshift/domain-name-system/domain-name-for-your-application/#set-up-domain-name-for-your-application","title":"Set up Domain Name for Your Application","text":"<p>Prerequisites:</p> <p>Your application should be running in Pods within NERC OpenShift, with a Service created for the port(s) you want to expose. By default, an HTTPS-enabled route is set up for each deployed application, following this format: <code>https://&lt;your-application-name&gt;-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.</p> <p>In this example, we have set up Credit Card Fraud Detection Application based on this Predictive AI tutorial.</p> <p>The Topology view shows application details as shown below:</p> <p></p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#enabling-https-for-web-applications","title":"Enabling HTTPS for Web Applications","text":"<p>HTTPS provides data-in-motion encryption for web applications. To enable HTTPS, you need to generate TLS certificates and have them signed by a trusted Certificate Authority (CA).</p> <p>Let's Encrypt is a free CA that supports automatic certificate issuing using the ACME protocol.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#what-is-dns","title":"What is DNS?","text":"<p>You can learn more about DNS by reading this page.</p> <p>Unlike DNS services on NERC OpenStack, where the Domain Name requires an A Record pointing to the public floating IP of your NERC VM, on NERC OpenShift, you need to point your unique public Route url i.e. <code>&lt;your-application-name&gt;-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code> of your deployed application to the public DNS by creating a CNAME Record.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#what-is-a-cname-record","title":"What is a CNAME Record?","text":"<p>A CNAME (Canonical Name) record: CNAME is a type of DNS record that maps one domain name (alias) to another domain name (canonical name). This allows multiple domain names to point to the same target without needing separate IP addresses.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#how-to-get-user-friendly-domain-names-hostnames","title":"How to get user-friendly domain names (hostnames)?","text":"<p>NERC does not currently offer integrated domain name service management.</p> <p>You can use one of the following methods to configure name resolution (DNS) for your NERC's virtual instances.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#1-using-freely-available-free-dynamic-dns-services","title":"1. Using freely available free Dynamic DNS services","text":"<p>Get a free domain or host name from no-ip.com or other</p> <p>free Dynamic DNS services.</p> <p>Here we will describe how to use No-IP to configure dynamic DNS.</p> <p>Step 1: Create your No-IP Account.</p> <p></p> <p>During this process you can add your desired unique hostname with pre-existing domain name or you can choose to create your hostname later on.</p> <p></p> <p>Step 2: Confirm Your Account by verifing your email address.</p> <p></p> <p>Step 3: Log In to Your Account to view your dashboard.</p> <p>Step 4: Create a New Hostname.</p> <p></p> <p>Give a Hostname and select one of the available Domain names. This will create your full hostname like `. Also, choose DNS Alias (CNAME) as the Record Type and add your application's unique Public Route [without HTTP(S) protocol] in the Target textbox.</p> <p></p> <p>Click Create Hostname.</p> <p>Then, browse your host or domain name as you setup during registration or later i.e. http://nerc.hopto.org on above example.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#how-to-create-an-ingress-with-tlsssl-certificates-from-lets-encrypt","title":"How to create an Ingress with TLS/SSL certificates from Let's Encrypt","text":"<p>To use your own custom domain with HTTPS, you need to create an Ingress with annotations for cert-manager.</p> <p>To expose your application securely with HTTPS on OpenShift, you need to create an Ingress resource per port that is used by Service to serve public facing. An Ingress allows external HTTP and HTTPS traffic to access services within your cluster.</p> <ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>In the Navigation Menu, navigate to the Networking -&gt; Ingresses menu:</p> <p></p> </li> <li> <p>Click Create Ingress.</p> </li> <li> <p>You will need to:</p> <ul> <li>i. Change:</li> </ul> <p><code>metadata.name</code> i.e. <code>&lt;your-unique-ingress-name&gt;</code></p> <p><code>spec.rules[0].host</code> i.e. <code>&lt;your-hostname&gt;</code></p> <p><code>spec.rules[0].http.paths[*].backend.service</code> i.e. path, servicename and service exposed port number</p> <ul> <li>ii. Add:</li> </ul> <p><code>metadata.annotations</code> i.e.</p> <pre><code>annotations:\n  acme.cert-manager.io/http01-ingress-class: openshift-default\n  cert-manager.io/cluster-issuer: letsencrypt-production-http01\n</code></pre> <p><code>spec.ingressClassName</code> i.e.</p> <pre><code>spec:\n  ingressClassName: openshift-default\n</code></pre> <p><code>spec.tls</code> i.e.</p> <pre><code>tls:\n  - hosts:\n      - &lt;your-hostname&gt;\n    secretName: &lt;unique-secret-name&gt;\n</code></pre> <p>For example, your Ingress template should look like similar to shown below:</p> <pre><code>kind: Ingress\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: &lt;your-unique-ingress-name&gt;\n  namespace: &lt;your-namespace&gt;\n  annotations:\n    acme.cert-manager.io/http01-ingress-class: openshift-default\n    cert-manager.io/cluster-issuer: letsencrypt-production-http01\nspec:\n  ingressClassName: openshift-default\n  tls:\n    - hosts:\n        - nerc.hopto.org\n      secretName: &lt;your-unique-tls-secret-name&gt;\n  rules:\n    - host: nerc.hopto.org\n      http:\n        paths:\n          - path: &lt;your-path-to-service&gt;\n            pathType: Prefix\n            backend:\n              service:\n                name: &lt;your-service-name&gt;\n                port:\n                  number: &lt;your-service-exposed-port&gt;\n</code></pre> </li> <li> <p>Click Create.</p> </li> <li> <p>In the Navigation Menu, navigate to the Workloads -&gt; Topology menu.</p> </li> <li> <p>Click on your deployed application to see the Overview panel to the right.</p> </li> <li> <p>Click the Resources tab to view the Routes used by the component. You will see a new Route that has been added by the Ingress configuration you just set up above as shown below:</p> </li> </ol> <p></p> <ul> <li>Click on the Route link it will load the web application with the HTTPS and domain name you defined in your ingress: <code>https://&lt;your-hostname&gt;</code> i.e. <code>https://nerc.hopto.org</code></li> </ul> <p>To Troubleshoot Certificate Requests</p> <p>Naviage to Search Menu and search for certificate and select CertificateRequest. Here you will be able to see all requested certificates in your namespaces as shown below:</p> <p></p> <p>You can click on it and check the Conditions of the certificate request as shown below:</p> <p></p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#2-using-your-local-research-computing-rc-department-or-academic-institutions-central-it-services","title":"2. Using your local Research Computing (RC) department or academic institution's Central IT services","text":"<p>You need to contact and work with your Research Computing department or academic institution's Central IT services to create CNAME record for your hostname that maps to the Public Route url i.e. <code>&lt;your-application-name&gt;-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code> of your deployed application.</p>"},{"location":"openshift/domain-name-system/domain-name-for-your-application/#3-using-commercial-dns-providers","title":"3. Using commercial DNS providers","text":"<p>Alternatively, you can purchase a fully registered domain name or host name from commercial hosting providers and then register DNS records for your route url i.e. <code>&lt;your-application-name&gt;-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code> from commercial cloud servies i.e. AWS Route53, Azure DNS, CloudFlare, Google Cloud Platform, GoDaddy, etc.</p>"},{"location":"openshift/get-started/openshift-overview/","title":"OpenShift Overview","text":""},{"location":"openshift/get-started/openshift-overview/#openshift-overview","title":"OpenShift Overview","text":"<p>OpenShift is a multifaceted, container orchestration platform from Red Hat. OpenShift Container Platform is a cloud-based Kubernetes container platform. NERC offers a cloud development Platform-as-a-Service (PaaS) solution based on Red Hat's OpenShift Container Platform that provides isolated, multi-tenant containers for application development and deployment. This is optimized for continuous containerized application development and multi-tenant deployment which allows you and your team to focus on solving your research problems and not infrastructure management.</p>"},{"location":"openshift/get-started/openshift-overview/#basic-components-and-glossary-of-common-terms","title":"Basic Components and Glossary of common terms","text":"<p>OpenShift is a container orchestration platform that provides a number of components and tools to help you build, deploy, and manage applications. Here are some of the basic components of OpenShift:</p> <ul> <li> <p>Project: A project is a logical grouping of resources in the NERC's OpenShift     platform that provides isolation from others resources.</p> </li> <li> <p>Nodes: Nodes are the physical or virtual machines that run the applications     and services in your OpenShift cluster.</p> </li> <li> <p>Image: An image is a non-changing, definition of file structures and programs     for running an application.</p> </li> <li> <p>Container: A container is an instance of an image with the addition of other     operating system components such as networking and running programs. Containers     are used to run applications and services in OpenShift.</p> </li> <li> <p>Pods: Pods are the smallest deployable units defined, deployed, and managed     in OpenShift, that group related one or more containers that need to share resources.</p> </li> <li> <p>Services: Services are logical representations of a set of pods that provide     a network endpoint for access to the application or service. Services can be     used to load balance traffic across multiple pods, and they can be accessed     using a stable DNS name. Services are assigned an IP address and port and proxy     connections to backend pods. This allows the pods to change while the connection     details of the service remain consistent.</p> </li> <li> <p>Volume: A volume is a persistent file space available to pods and containers     for storing data. Containers are immutable and therefore upon a restart any     contents are cleared and reset to the original state of the image used to create     the container. Volumes provide storage space for files that need to persist     through container restarts.</p> </li> <li> <p>Routes: Routes can be used to expose services to external clients to connections     outside the platform. A route is assigned a name in DNS when set up to make it     easily accessible. They can be configured with custom hostnames and TLS certificates.</p> </li> <li> <p>Replication Controllers: A replication controller (rc) is a built-in mechanism     that ensures a defined number of pods are running at all times. An asset that     indicates how many pod replicas are required to run at a time. If a pod unexpectedly     quits or is deleted, a new copy of the pod is created and started. Additionally,     if more pods are running than the defined number, the replication controller     will delete the extra pods to get down to the defined number.</p> </li> <li> <p>Namespace: A Namespace is a way to logically isolate resources within the     Cluster. In our case every project gets an unique namespace.</p> </li> <li> <p>Role-based access control (RBAC): A key security control to ensure that cluster     users and workloads have only access to resources required to execute their roles.</p> </li> <li> <p>Deployment Configurations: A deployment configuration (dc) is an extension     of a replication controller that is used to push out a new version of application     code. Deployment configurations are used to define the process of deploying     applications and services to OpenShift. Deployment configurations     can be used to specify the number of replicas, the resources required by the     application, and the deployment strategy to use.</p> </li> <li> <p>Application URL Components: When an application developer adds an application     to a project, a unique DNS name is created for the application via a Route. All     application DNS names will have a hyphen separator between your application name     and your unique project namespace. If the application is a web application, this     DNS name is also used for the URL to access the application. All names are in     the form of <code>&lt;your-application-name&gt;-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.     For example: <code>mytestapp-mynamespace.apps.shift.nerc.mghpcc.org</code>.</p> </li> </ul>"},{"location":"openshift/gpus/intro-to-gpus-on-nerc-ocp/","title":"Introduction to GPUs on NERC OpenShift","text":""},{"location":"openshift/gpus/intro-to-gpus-on-nerc-ocp/#introduction-to-gpus-on-nerc-openshift","title":"Introduction to GPUs on NERC OpenShift","text":"<p>NERC OCP clusters leverage the NVIDIA GPU Operator as well as the Node Feature Discovery Operator to manage and deploy GPU worker nodes to clusters.</p> <p>GPU nodes on NERC clusters are also managed via taints according to their GPU device. This ensures that only workloads explicitly requesting GPUs will consume GPU resources.</p>"},{"location":"openshift/gpus/intro-to-gpus-on-nerc-ocp/#nerc-gpu-worker-node-architecture","title":"NERC GPU Worker Node Architecture","text":"<p>The NERC OpenShift environment currently supports three different NVIDIA GPU  products:</p> <ol> <li>NVIDIA-A100-SXM4-40GB (A100)</li> <li>NVIDIA-H100-80GB-HBM3 (H100)</li> <li>Tesla-V100-PCIE-32GB (V100)</li> </ol> <p>NERC GPU Worker Nodes Info</p> <p>A100 worker nodes also contain 4 individual GPUs, but each has 40\u202fGB of memory. H100 worker nodes contain 4 individual GPUs, each with 80\u202fGB of memory. In contrast, V100 worker nodes contain a single GPU with 32\u202fGB of memory.</p>"},{"location":"openshift/gpus/intro-to-gpus-on-nerc-ocp/#accessing-gpu-resources","title":"Accessing GPU Resources","text":"<p>In order to use GPU resources on your pod, you must specify the number of GPUs you want to use in the \"OpenShift Request on GPU Quota\" attribute that has been approved for your \"NERC-OCP (OpenShift)\" resource allocation on NERC's ColdFront as described here.</p>"},{"location":"openshift/gpus/intro-to-gpus-on-nerc-ocp/#deploying-workloads-to-gpus","title":"Deploying Workloads to GPUs","text":"<p>There are two ways to deploy workloads on GPU nodes:</p> <ol> <li> <p>Deploy directly in your OCP namespace:</p> <p>In your project namespace you can deploy a GPU workload by explicitely requesting a GPU in your pod manifest, see: How to specify pod to use GPU.</p> </li> <li> <p>Deploy through NERC RHOAI</p> <p>See Populate the data science project with a Workbench for selecting GPU options.</p> <p>GPU Accelerator Available on NERC RHOAI</p> <p>The different options for GPU accelerator are \"None\", \"NVIDIA A100 GPU\", \"NVIDIA H100 GPU\", and \"NVIDIA V100 GPU\" as shown below:</p> <p></p> </li> </ol>"},{"location":"openshift/logging-in/access-the-openshift-web-console/","title":"Access the NERC's OpenShift Web Console","text":""},{"location":"openshift/logging-in/access-the-openshift-web-console/#access-the-nercs-openshift-web-console","title":"Access the NERC's OpenShift Web Console","text":"<p>The NERC's OpenShift Container Platform web console is a user interface that can be accessed via the web.</p> <p>You can find it at https://console.apps.shift.nerc.mghpcc.org.</p> <p>Very Important: What is the Web Console URL for NERC Academic (EDU) OpenShift?</p> <p>Access the NERC Academic (EDU) OpenShift web console at: https://console.apps.edu.nerc.mghpcc.org</p> <p>The NERC Authentication supports CILogon using Keycloak for gateway authentication and authorization that provides federated login via your institution accounts and it is the recommended authentication method.</p> <p>Make sure you are selecting \"mss-keycloak\" as shown here:</p> <p></p> <p>Next, you will be redirected to CILogon welcome page as shown below:</p> <p></p> <p>MGHPCC Shared Services (MSS) Keycloak will request approval of access to the following information from the user:</p> <ul> <li> <p>Your CILogon user identifier</p> </li> <li> <p>Your name</p> </li> <li> <p>Your email address</p> </li> <li> <p>Your username and affiliation from your identity provider</p> </li> </ul> <p>which are required in order to allow access your account on NERC's OpenStack web console.</p> <p>From the \"Selected Identity Provider\" dropdown option, please select your institution's name. If you would like to remember your selected institution name for future logins please check the \"Remember this selection\" checkbox this will bypass the CILogon welcome page on subsequent visits and proceed directly to the selected insitution's identity provider(IdP). Click \"Log On\". This will redirect to your respective institutional login page where you need to enter your institutional credentials.</p> <p>Important Note</p> <p>The NERC does not see or have access to your institutional account credentials, it points to your selected insitution's identity provider and redirects back once authenticated.</p> <p>Once you successfully authenticate, you will see a graphical user interface displaying a list of projects on the Projects page based on your ColdFront allocations, as shown below:</p> <p></p> <p>To visualize your project data and perform administrative, management, or troubleshooting tasks, select a project from the list to open its detailed view along with the Overview pane, as shown below:</p> <p></p> <p>The Workload section provides a comprehensive overview of details relevant to project administration, including resource utilization, configuration, and security settings, as shown below:</p> <p></p> <p>I can't find my project</p> <p>If you are a member of several projects i.e. ColdFront NERC-OCP (OpenShift) allocations, you may need to switch the project before you can see and use OpenShift resources you or your team has created. Clicking on the Home -&gt; Projects menu which is displayed near the top left side will show the list of projects you are in. You can search and select the project by clicking on the project name in that list as shown below:</p> <p></p>"},{"location":"openshift/logging-in/setup-the-openshift-cli/","title":"How to Setup the OpenShift CLI Tools","text":""},{"location":"openshift/logging-in/setup-the-openshift-cli/#how-to-setup-the-openshift-cli-tools","title":"How to Setup the OpenShift CLI Tools","text":"<p>The most commonly used command-line client tool for the NERC's OpenShift is OpenShift CLI (<code>oc</code>). It is available for Linux, Windows, or macOS and allows you to create applications and manage OpenShift Container Platform projects from a terminal.</p>"},{"location":"openshift/logging-in/setup-the-openshift-cli/#installing-the-openshift-cli","title":"Installing the OpenShift CLI","text":"<p>Installation options for the CLI vary depending on your Operating System (OS). You can install the OpenShift CLI (oc) either by downloading the binary or by using an Package Manager (RPM).</p> <p>Unlike the web console, it allows the user to work directly with the project source code using command scripts once they are authenticated using token.</p> <p>You can download the latest <code>oc</code> CLI client tool binary from web console as shown below:</p> <p></p> <p>Then add it to your path environment based on your OS choice by following this documentation.</p>"},{"location":"openshift/logging-in/setup-the-openshift-cli/#configuring-the-openshift-cli","title":"Configuring the OpenShift CLI","text":"<p>You can configure the <code>oc</code> command tool to enable tab completion to automatically complete oc commands or suggest options when you press Tab for the Bash or Zsh shells by following these steps.</p>"},{"location":"openshift/logging-in/setup-the-openshift-cli/#first-time-usage","title":"First Time Usage","text":"<p>Before you can use the oc command-line tool, you will need to authenticate to the NERC's OpenShift platform by running built-in login command obtained from the NERC's OpenShift Web Console. This will allow authentication and enables you to work with your NERC's OpenShift Container Platform projects. It will create a session that will last approximately 24 hours.</p> <p>To get the oc login command with your own unique token, please login to the NERC's OpenShift Web Console and then under your user profile link located at the top right corner, click on Copy login command as shown below:</p> <p></p> <p>It will once again ask you to provide your KeyCloak login and then once successful it will redirect you to a static page with a link to Display Token as shown below:</p> <p></p> <p>Clicking on that \"Display Token\" link it will show a static page with Login command with token as shown below:</p> <p></p> <p>Copy and run the generated command on your terminal to authenticate yourself to access the project from your terminal i.e. <code>oc login --token=&lt;Your-Token&gt; --server=https://&lt;NERC-OpenShift-Server&gt;</code></p> <p>If you try to run an oc command and get a permission denied message, your login session has likely expired and you will need to re-generate the oc login command from your NERC's OpenShift Web Console and then run the new oc login command with new token on your terminal.</p>"},{"location":"openshift/logging-in/setup-the-openshift-cli/#other-useful-oc-commands","title":"Other Useful oc Commands","text":"<p>This reference document provides descriptions and example commands for OpenShift CLI (oc) developer commands.</p> <p>Important Note</p> <p>Run <code>oc help</code> to list all commands and run <code>oc &lt;command&gt; --help</code> or <code>oc &lt;command&gt; -h</code> to get additional details for a specific command.</p> <p>You can always use <code>oc explain &lt;resource&gt;</code>, if you need help with resources.</p>"},{"location":"openshift/logging-in/the-openshift-cli/","title":"OpenShift CLI Tools Overview","text":""},{"location":"openshift/logging-in/the-openshift-cli/#openshift-command-line-interface-cli-tools-overview","title":"OpenShift command-line interface (CLI) Tools Overview","text":"<p>With the OpenShift CLI, the oc command, you can create applications and manage OpenShift Container Platform projects from a terminal.</p> <p>The web console provides a comprehensive set of tools for managing your projects and applications. There are, however, some tasks that can only be performed using a command-line tool called oc.</p> <p>The OpenShift CLI is ideal in the following situations:</p> <ul> <li> <p>Working directly with project source code</p> </li> <li> <p>Scripting OpenShift Container Platform operations</p> </li> <li> <p>Managing projects while restricted by bandwidth resources and the web console     is unavailable</p> </li> </ul> <p>It is recommended that developers should be comfortable with simple command-line tasks and the the NERC's OpenShift command-line tool.</p>"},{"location":"openshift/logging-in/web-console-overview/","title":"Web Console Overview","text":""},{"location":"openshift/logging-in/web-console-overview/#web-console-overview","title":"Web Console Overview","text":"<p>The NERC's OpenShift Container Platform (OCP) provides a web-based console that enables users to perform common management tasks such as building and deploying applications. It now features a single unified view within the OpenShift console, offering a more comprehensive and cohesive interface for all tasks. This enhancement simplifies navigation and allows developers and administrators to manage their environments seamlessly from one centralized dashboard.</p> <p>You can find it at https://console.apps.shift.nerc.mghpcc.org.</p> <p>Very Important: What is the Web Console URL for NERC Academic (EDU) OpenShift?</p> <p>Access the NERC Academic (EDU) OpenShift web console at: https://console.apps.edu.nerc.mghpcc.org</p> <p>The web console provides tools to access and manage your application code and data.</p> <p>Below is a sample screenshot of the web interface with labels describing different sections of the NERC's OpenShift Web Console:</p> <p></p> <ol> <li> <p>Navigation Menu - Menu options to access different tools and settings for a project.    The list will change depending on which Perspective view you are in.</p> </li> <li> <p>Project List - Drop-down to select a different project. Based on user's active     and approved resource allocations this projects list will be updated.</p> <p>This allows you to view the overview of the currently selected project from the drop-down list and also details about it including resource utilization and resource quotas, as shown below:</p> <p></p> <p>Important Note</p> <p>You can identify the currently selected project with tick mark and also you can click on star icon to keep the project under your Favorites list:</p> <p></p> </li> <li> <p>User Preferences - Shown the option to get and copy the OpenShift Command Line    oc login command and set your individual console preferences including default    views, language, import settings, and more.</p> </li> <li> <p>View Switcher - This three dot menu is used to switch between List View    and Graph view of all your applications.</p> </li> <li> <p>Main Panel - Displays basic application information. Clicking on the application    names in the main panel expands the Details Panel (6).</p> </li> <li> <p>Details Panel - Displays additional information about the application selected    from the Main Panel. This includes detailed information about the running application,    applications builds, routes, and more. Tabs at the top of this panel will change    the view to show additional information such as Details and Resources.</p> </li> </ol>"},{"location":"openshift/logging-in/web-console-overview/#navigation-menu","title":"Navigation Menu","text":"<p>On the left side of the navigation pane, the web console provides a comprehensive set of tools for managing your projects and applications.</p>"},{"location":"openshift/logging-in/web-console-overview/#home","title":"Home","text":""},{"location":"openshift/logging-in/web-console-overview/#projects","title":"Projects","text":"<p>Clicking on the Home -&gt; Projects menu which is displayed near the top left side will show the list of projects you are in. You can search and select the project by clicking on the project name in that list as shown below:</p> <p></p>"},{"location":"openshift/logging-in/web-console-overview/#search","title":"Search","text":"<p>Clicking on the Home -&gt; Search menu allows you to search any resources based on search criteria like Label or Name.</p>"},{"location":"openshift/logging-in/web-console-overview/#software-catalog","title":"Software Catalog","text":"<p>Easily discover, explore, and deploy tools from a centralized catalog built to streamline your workflow</p> <ul> <li> <p>Centralizes all available software for faster discovery and access.</p> </li> <li> <p>Improved categorization for easier browsing.</p> </li> <li> <p>Supports consistent deployment flows.</p> </li> </ul> <p></p>"},{"location":"openshift/logging-in/web-console-overview/#events","title":"Events","text":"<p>Clicking on the Home -&gt; Events menu provides you with a Dashboard to view the resource usage and also other metrics and events that occured on your project. Here you can identify, monitor, and inspect the usage of Memory, CPU, Network, and Storage in your project.</p>"},{"location":"openshift/logging-in/web-console-overview/#favorites","title":"Favorites","text":"<p>Pin frequently used items to the top for quick access whenever you need them.</p> <p></p> <ul> <li> <p>Reduce time spent searching for commonly used pages and resources.</p> </li> <li> <p>Customize the interface to match your personal workflow.</p> </li> </ul> <p></p>"},{"location":"openshift/logging-in/web-console-overview/#helm","title":"Helm","text":"<p>You can enable the Helm Charts here. Helm Charts is the pacakge manager that help to easily manage definitions, installations and upgrades of you complex application. It also shows catalog of all available helm charts for you to use by installing them.</p>"},{"location":"openshift/logging-in/web-console-overview/#workloads","title":"Workloads","text":""},{"location":"openshift/logging-in/web-console-overview/#topology","title":"Topology","text":"<p>The Topology view in the the web console provides a visual representation of all the applications within a project, their build status, and the components and services associated with them. If you have no workloads or applications in the project, the Topology view displays the available options to create applications.</p> <p></p> <p>If you have existing workloads, the Topology view graphically displays your workload nodes. The Topology view provides you the option to monitor your applications using the List view. Use the List view icon () to see a list of all your applications and use the Graph view icon () to switch back to the graph view.To read more about how to view the topology of your application please read this official documentation from Red Hat.</p>"},{"location":"openshift/logging-in/web-console-overview/#secrets","title":"Secrets","text":"<p>This allows you to view or create Secrets that allows to inject sensitive data into your application as files or environment variables.</p>"},{"location":"openshift/logging-in/web-console-overview/#configmaps","title":"ConfigMaps","text":"<p>This menu allows you to view or create a new ConfigMap by entering manually YAML or JSON definitions, or by dragging and dropping a file into the editor.</p>"},{"location":"openshift/logging-in/web-console-overview/#storage","title":"Storage","text":"<p>Read more about Storage here.</p>"},{"location":"openshift/logging-in/web-console-overview/#builds","title":"Builds","text":"<p>This menu provides tools for building and deploying applications. You can use it to create and manage build configurations using YAML syntax, as well as view the status and logs of your builds.</p>"},{"location":"openshift/logging-in/web-console-overview/#administration","title":"Administration","text":""},{"location":"openshift/logging-in/web-console-overview/#resource-quotas","title":"Resource Quotas","text":"<p>You can also view the resource quota for your project in the web console by navigating to Administration -&gt; ResourceQuotas, as shown below:</p> <p></p> <p>If you scroll down to the Resource Quota Details section for the pods resource type, you can view the used and maximum resources as shown below:</p> <p></p>"},{"location":"openshift/logging-in/web-console-overview/#limit-ranges","title":"Limit Ranges","text":"<p>You can also be able to view the current limit range for your project by going into the Administration -&gt; LimitRange menu as shown below:</p> <p></p>"},{"location":"openshift/storage/Rclone/","title":"Rclone","text":""},{"location":"openshift/storage/Rclone/#rclone","title":"Rclone","text":"<p>Rclone is a program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Different cloud storage products support rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols. Rclone is mature, open-source software originally inspired by rsync and written in Go.</p>"},{"location":"openshift/storage/Rclone/#deployment","title":"Deployment","text":""},{"location":"openshift/storage/Rclone/#deploying-as-a-workbench-using-a-data-science-project-dsp-on-nerc-rhoai","title":"Deploying as a Workbench Using a Data Science Project (DSP) on NERC RHOAI","text":"<ol> <li> <p>Navigating to the OpenShift AI dashboard.</p> <p>Please follow these steps to access the NERC OpenShift AI dashboard.</p> </li> <li> <p>Please ensure that you start your Rclone Web server with options as depicted     in the following configuration screen. This screen provides you with the opportunity     to select a notebook image and configure its options, including the Accelerator     and Number of accelerators (GPUs).</p> <p></p> <p>For our example project, let's name it \"Rclone Workbench\". We'll select the Rclone image, choose a Deployment size of Small, choose Accelerator of None, and allocate a Cluster storage space of 100Mi.</p> <p>Tip</p> <p>The dashboard currently enforces a minimum storage volume size of 20GB, which exceeds the requirements for the Rclone configuration. Please ensure that you change it to 100Mi in Cluster Storage.</p> </li> <li> <p>If this procedure is successful, you have started your Rclone web server. When     your workbench is ready and the status changes to Running, click the open     icon () next to your workbench's name, or     click the workbench name directly to access your environment:</p> <p></p> </li> <li> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when     prompted, as shown below:</p> <p></p> <p>Next, you should see the Rclone WebUI Login page, as shown below:</p> <p></p> <p>What are the Username and Password for Accessing the Rclone Web Interface?</p> <p>No username or password is required for this setup. Simply click on \"Login\" to access the Rclone Web Interface.</p> </li> </ol>"},{"location":"openshift/storage/Rclone/#deploying-rclone-on-nerc-openshift-as-a-standalone-application","title":"Deploying Rclone on NERC OpenShift as a Standalone Application","text":"<ul> <li> <p>Prerequisites:</p> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI to enable <code>oc</code> commands. Refer to this user guide.</p> </li> </ul>"},{"location":"openshift/storage/Rclone/#steps","title":"Steps","text":"<ol> <li> <p>Clone or navigate to this repository.</p> <p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/rclone-web-on-openshift.git\ncd rclone-web-on-openshift\n</code></pre> <p>In the <code>standalone/deploy</code> folder, you will find the following YAML files:</p> <p>i. <code>01-pvc.yaml</code>: Creates a persistent volume to store the configuration.</p> <p>ii. <code>02-deployment.yaml</code>: Deploys the application.</p> <p>Very Important: Change provided admin username and password</p> <p>Modify the admin account and password to restrict access (highly recommended).</p> <p>iii. <code>03-service.yaml</code>, <code>04-route.yaml</code>: Set up external access to connect to the Web UI.</p> </li> <li> <p>Run this <code>oc</code> command: <code>oc apply -f ./standalone/deploy/.</code> to execute all YAML files located in the standalone/deploy folder.</p> </li> </ol>"},{"location":"openshift/storage/Rclone/#configuration","title":"Configuration","text":"<ul> <li> <p>You will see the <code>rclone-app</code> application on OpenShift Web Dashboard.</p> </li> <li> <p>Once the application is deployed and the route is set up, it can be accessed     at a web URL similar to <code>https://rclone-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>     by clicking any one of the links as shown here:</p> <p></p> </li> <li> <p>Users will be prompted to log in with a username and password set in     <code>02-deployment.yaml</code> as environment variables     before gaining access to the Rclone Login page, as shown below:</p> <p> </p> </li> <li> <p>Once authenticated, log in to the Rclone WebUI Login page again using the same     username and password.</p> <p></p> </li> <li> <p>Create a configuration for your endpoints.</p> <p>Rclone to connect the NERC OpenStack Object Storage</p> <p>For example, here we are setting up an S3 configuration to connect to a container (bucket) on the NERC OpenStack. You must have already created this container and gathered all the necessary details: the endpoint, access key, secret key, and container name. Refer to this guide to learn how to retrieve this information from your NERC OpenStack project.</p> <p>i. In Rclone, click on \"Configs\" Menu.  </p> <p>ii. Click on \"Create a New Config\" button to create a new Remote.</p> <p></p> <p>iii. Create a new configuration, give it a name i.e. <code>myBucket</code>, and select \"Amazon S3 Compliant Storage Providers...\", which support the NERC OpenStack Object Storage/ Swift.</p> <p></p> <p>iv. Enter the connection details by providing the AWS Access Key ID, AWS Secret Access Key, and the S3 Endpoint under \"Endpoint for S3 API\" (e.g., <code>https://stack.nerc.mghpcc.org:13808</code>, the default S3 Endpoint URL for NERC OpenStack Object Storage). The final value is automatically copied into other fields - this is expected behavior.</p> <p>Very Important: Endpoint for S3 API for the NERC Object Storage/ Swift</p> <p>The default endpoint for S3 API for the NERC Object Storage/ Swift is <code>https://stack.nerc.mghpcc.org:13808</code>.</p> <p></p> <p>v. Finalize the config by clicking on \"Next\" at the bottom.</p> <p>Now that you have completed the remote setup, go to the Explorer menu, select the remote, and start browsing!</p> <p></p> </li> </ul>"},{"location":"openshift/storage/Rclone/#usage","title":"Usage","text":"<p>In this simple example, we will transfer a sample dump from Wikipedia. Wikimedia publishes these dumps daily, and they are mirrored by various organizations.</p> <p>In a \"standard\" setup, loading this data into your object store wouldn't be very practical, as it often requires downloading it locally first before uploading it to your storage.</p> <p>This is how we can do it with Rclone:</p> <ul> <li> <p>Create another remote of type \"HTTP\" named as <code>wikidump</code>.</p> <p></p> </li> <li> <p>Enter the address of one of the mirrors. Here we use <code>https://dumps.wikimedia.your.org/wikidatawiki/</code>.</p> <p></p> </li> <li> <p>Open the Explorer view, set it in dual-pane layout:</p> <p></p> <p>In the first pane, open your first remote, i.e., <code>myBucket</code>, and in the other pane, open the remote <code>wikidump</code>, which should look like this:</p> <p></p> <p>Browse to the folder you want, select a file or folder, and simply drag and drop it from <code>wikidump</code> to your NERC OpenStack Object Storage container/bucket.</p> <p>For a more interesting test, try selecting a larger file!</p> </li> <li> <p>Click on the \"Dashboard\" menu, where you will see the file transfer happening     in the background.</p> <p></p> </li> </ul> <p>That's it! No installation required, high-speed optimized transfer, and you can even perform multiple transfers in the background...</p>"},{"location":"openshift/storage/minio/","title":"Minio","text":""},{"location":"openshift/storage/minio/#minio","title":"Minio","text":"<p>Minio is an open-source, high-performance object storage server compatible with the Amazon S3 API. It provides reliable and scalable storage for cloud-native applications, big data, and AI workloads.</p> <p>MinIO offers several key features:</p> <ul> <li> <p>High Performance: MinIO is optimized for high throughput and low latency, making it suitable for large-scale object storage deployments.</p> </li> <li> <p>Scalability: It can handle petabytes of data and thousands of concurrent users, allowing you to scale your storage needs as required.</p> </li> <li> <p>Reliability: MinIO ensures data durability and availability with built-in replication and erasure coding capabilities.</p> </li> <li> <p>Security: It supports encryption at rest and in transit, ensuring the confidentiality and integrity of your data.</p> </li> <li> <p>Integration: MinIO is compatible with the Amazon S3 API, making it easy to migrate existing applications and workflows to MinIO.</p> </li> </ul>"},{"location":"openshift/storage/minio/#deploying-minio-in-your-openshift-project","title":"Deploying Minio in Your OpenShift Project","text":"<p>You can run a script that automates the setup of a local S3 storage (MinIO) by completing the following tasks:  </p> <ul> <li> <p>Deploys a MinIO instance in your project namespace.</p> </li> <li> <p>Generates a random user ID and password for the MinIO Console.</p> </li> <li> <p>Installs all required network policies.</p> </li> </ul> <p>Procedure:</p> <ol> <li> <p>From the OpenShift AI dashboard, you can return to OpenShift Web Console     by using the application launcher icon (the black-and-white icon that looks     like a grid), and choosing the \"OpenShift Console\" as shown below:</p> <p></p> </li> <li> <p>From your NERC's OpenShift Web Console,     navigate to your project corresponding to the NERC RHOAI Data Science Project     and select the \"Import YAML\" button, represented by the \"+\" icon in the top     navigation bar as shown below:</p> <p></p> </li> <li> <p>Verify that you selected the correct project.</p> <p></p> </li> <li> <p>Copy the following code and paste it into the Import YAML editor.</p> Local S3 storage (MinIO) Creation YAML Script <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: minio-setup\n  labels:\n    app: minio\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: minio-setup-edit\n  labels:\n    app: minio\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\nsubjects:\n- kind: ServiceAccount\n  name: minio-setup\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-pvc\n  labels:\n    app: minio\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi # Adjust the size according to your needs\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - minio server /data --console-address :9090\n        command:\n        - /bin/bash\n        - -c\n        envFrom:\n        - secretRef:\n            name: minio-root-user\n        image: quay.io/minio/minio:latest\n        name: minio\n        ports:\n        - containerPort: 9000\n          name: api\n          protocol: TCP\n        - containerPort: 9090\n          name: console\n          protocol: TCP\n        resources:\n          limits:\n            cpu: \"2\"\n            memory: 2Gi\n          requests:\n            cpu: 200m\n            memory: 1Gi\n        volumeMounts:\n        - mountPath: /data\n          name: minio-volume\n      volumes:\n      - name: minio-volume\n        persistentVolumeClaim:\n          claimName: minio-pvc\n      - emptyDir: {}\n        name: empty\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-minio-root-user\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          if [ -n \"$(oc get secret minio-root-user -oname 2&gt;/dev/null)\" ]; then\n            echo \"Secret already exists. Skipping.\" &gt;&amp;2\n            exit 0\n          fi\n          genpass() {\n              &lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c\"${1:-32}\"\n          }\n          id=$(genpass 16)\n          secret=$(genpass)\n          cat &lt;&lt; EOF | oc apply -f-\n          apiVersion: v1\n          kind: Secret\n          metadata:\n            name: minio-root-user\n          type: Opaque\n          stringData:\n            MINIO_ROOT_USER: ${id}\n            MINIO_ROOT_PASSWORD: ${secret}\n          EOF\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: create-minio-root-user\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\n  labels:\n    app: minio\nspec:\n  ports:\n  - name: api\n    port: 9000\n    targetPort: api\n  - name: console\n    port: 9090\n    targetPort: 9090\n  selector:\n    app: minio\n  sessionAffinity: None\n  type: ClusterIP\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: minio-console\n  labels:\n    app: minio\nspec:\n  port:\n    targetPort: console\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  wildcardPolicy: None\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: minio-s3\n  labels:\n    app: minio\nspec:\n  port:\n    targetPort: api\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  wildcardPolicy: None\n</code></pre> <p>Very Important Note</p> <p>In this YAML file, the size of the storage is set as 10Gi. Change it if you need to.</p> </li> <li> <p>Click Create.</p> </li> </ol> <p>Verification:</p> <p>i. Once Resource is successfully created, you will see a \"Resources successfully created\" message and the following resources listed:</p> <p></p> <p>ii. Once the deployment is successful, you will be able to see all resources are created and grouped under \"minio\" application grouping on the Workloads -&gt; Topology menu, as shown below:</p> <p></p> <p>Once successfully initiated, click on the minio deployment and select the \"Resources\" tab to review created Pods, Services, and Routes.</p> <p></p> <p>Please note the minio-console route URL by navigating to the \"Routes\" section under the Location path. When you click on the minio-console route URL, it will open the MinIO web console that looks like below:</p> <p></p> <p>MinIO Web Console Login Credential</p> <p>For this, you need to install and configure the OpenShift CLI by following the setup instructions.</p> <p>Once the OpenShift CLI is set up, the Username and Password for the MinIO web console can be retrieved by running the following <code>oc</code> commands:</p> <p>i. To get Access key run:</p> <p><code>oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_USER}}' | base64 --decode</code></p> <p>ii. And to get Secret key run:</p> <p><code>oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_PASSWORD}}' | base64 --decode</code> </p> <p>Return to the MinIO Web Console using the provided URL. Enter the Access Key as the Username and the Secret Key as the Password.</p> <p>Alternatively, Running a script to install local MinIO object storage</p> <p>Alternatively, you can run a script to install local object storage using the OpenShift CLI (<code>oc</code>). Once the OpenShift CLI is set up, execute the following command to install MinIO object storage along with local S3 storage (MinIO):</p> <p><code>oc apply -f https://raw.githubusercontent.com/nerc-project/llm-on-nerc/main/setup/s3-basic.yaml</code></p> <p>Clean Up</p> <p>To delete all resources if not necessary just run <code>oc delete -f https://raw.githubusercontent.com/nerc-project/llm-on-nerc/main/setup/s3-basic.yaml</code> or <code>oc delete all,sa,rolebindings,pvc,job -l app=minio</code>.</p> <p>Important Note</p> <p>The script is based on a guide for deploying MinIO. The MinIO-based Object Storage that the script creates is not meant for production usage.</p>"},{"location":"openshift/storage/storage-overview/","title":"Storage Overview","text":""},{"location":"openshift/storage/storage-overview/#storage-overview","title":"Storage Overview","text":"<p>The NERC OCP supports multiple types of storage.</p>"},{"location":"openshift/storage/storage-overview/#glossary-of-common-terms-for-ocp-storage","title":"Glossary of common terms for OCP storage","text":"<p>This glossary defines common terms that are used in the storage content.</p>"},{"location":"openshift/storage/storage-overview/#compute-resources","title":"Compute Resources","text":"<p>For more details, read the Compute Resources page.</p>"},{"location":"openshift/storage/storage-overview/#storage","title":"Storage","text":"<p>OCP supports many types of storage, both for on-premise and cloud providers. You can manage container storage for persistent and non-persistent data in an OCP cluster.</p>"},{"location":"openshift/storage/storage-overview/#storage-class","title":"Storage class","text":"<p>A storage class provides a way for administrators to describe the classes of storage they offer. Different classes might map to quality of service levels, backup policies, arbitrary policies determined by the cluster administrators.</p> <p></p> <p>Very Important Note</p> <p>On the NERC OCP cluster, the default <code>StorageClass</code> is <code>ocs-external-storagecluster-ceph-rbd</code>. If you don't specify <code>storageClassName</code> in your PersistentVolumeClaim (PVC), that default will be used.</p>"},{"location":"openshift/storage/storage-overview/#storage-types","title":"Storage types","text":"<p>OCP storage is broadly classified into two categories, namely ephemeral storage and persistent storage.</p>"},{"location":"openshift/storage/storage-overview/#ephemeral-storage","title":"Ephemeral storage","text":"<p>Pods and containers are ephemeral or transient in nature and designed for stateless applications. Ephemeral storage allows administrators and developers to better manage the local storage for some of their operations. For more information about ephemeral storage overview, types, and management, see Understanding ephemeral storage.</p> <p>Pods and containers can require temporary or transient local storage for their operation. The lifetime of this ephemeral storage does not extend beyond the life of the individual pod, and this ephemeral storage cannot be shared across pods.</p> <p>For more information on how to specify ephemeral storage (deleted with the pod) to a pod please read this.</p>"},{"location":"openshift/storage/storage-overview/#using-in-memory-emptydir-tmpfs-on-the-nerc-ocp","title":"Using in-memory <code>emptyDir</code> (tmpfs) on the NERC OCP","text":"<p>You can also use an in-memory <code>emptyDir</code> (RAM-backed tmpfs) on OpenShift that is fast, shared by all containers in the Pod, and wiped when the Pod goes away.</p> <p>For a Pod that defines an <code>emptyDir</code> volume, the volume is created when the Pod is scheduled onto a node. As the name implies, it starts empty. All containers in the Pod can read and write the same files in the <code>emptyDir</code>, which can be mounted at the same or different paths in each container. When the Pod is removed from the node for any reason, the <code>emptyDir</code> and its data are permanently deleted.</p>"},{"location":"openshift/storage/storage-overview/#what-it-is-and-why","title":"What it is (and why)","text":"<ul> <li> <p>Set <code>emptyDir.medium: \"Memory\"</code> to mount a tmpfs inside the Pod.</p> </li> <li> <p>Fast, node-RAM\u2013backed storage that's wiped when the Pod goes away.</p> </li> <li> <p>The volume is shared by all containers in the Pod.</p> </li> <li> <p>Files written there count against the container's memory limit (not <code>ephemeral-storage</code>). To learn more about defining CPU and memory, read this user guide.</p> </li> </ul> <p>Common uses for an <code>emptyDir</code> volume include:</p> <ul> <li> <p>Scratch space (e.g., for a disk-based merge sort)</p> </li> <li> <p>Checkpointing long-running computations for crash recovery</p> </li> <li> <p>Staging files that a content-fetcher/manager container downloads while a web server container serves them</p> </li> </ul> <p>Note</p> <p>A container crashing does not remove a Pod from a node. The data in an <code>emptyDir</code> volume is safe across container crashes.</p>"},{"location":"openshift/storage/storage-overview/#yaml-example","title":"YAML example","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mem-cache-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels: { app: mem-cache-demo }\n  template:\n    metadata:\n      labels: { app: mem-cache-demo }\n    spec:\n      containers:\n        - name: app\n          image: registry.access.redhat.com/ubi9/ubi-minimal\n          command: [\"/bin/sh\",\"-c\"]\n          args: [\"python3 -m http.server 8080\"]\n          resources:\n            requests:\n              memory: \"256Mi\"\n            limits:\n              memory: \"512Mi\"   # leave headroom for tmpfs usage\n          volumeMounts:\n            - name: fast-cache\n              mountPath: /cache\n      volumes:\n        - name: fast-cache\n          emptyDir:\n            medium: Memory\n            sizeLimit: 256Mi    # strongly recommended\n</code></pre> <p>Sizing &amp; safety tips</p> <ul> <li> <p>Always set <code>sizeLimit</code> to prevent the tmpfs from growing until the node or pod runs out of memory.</p> </li> <li> <p>Because it's RAM-backed, <code>ephemeral-storage</code> limits do not apply - use memory requests/limits and <code>sizeLimit</code>.</p> </li> <li> <p>Leave headroom: if total memory (processes + tmpfs) exceeds the memory limit, the container can be OOM-killed.</p> </li> </ul>"},{"location":"openshift/storage/storage-overview/#persistent-storage","title":"Persistent storage","text":"<p>Stateful applications deployed in containers require persistent storage. OCP uses a pre-provisioned storage framework called persistent volumes (PV) to allow cluster administrators to provision persistent storage. The data inside these volumes can exist beyond the lifecycle of an individual pod. Developers can use persistent volume claims (PVCs) to request storage requirements. For more information about persistent storage overview, configuration, and lifecycle, see Understanding persistent storage.</p> <p>Pods and containers can require permanent storage for their operation. OpenShift Container Platform uses the Kubernetes persistent volume (PV) framework to allow cluster administrators to provision persistent storage for a cluster. Developers can use PVC to request PV resources without having specific knowledge of the underlying storage infrastructure.</p>"},{"location":"openshift/storage/storage-overview/#persistent-volumes-pv","title":"Persistent volumes (PV)","text":"<p>OCP uses the Kubernetes persistent volume (PV) framework to allow cluster administrators to provision persistent storage for a cluster. Developers can use PVC to request PV resources without having specific knowledge of the underlying storage infrastructure.</p>"},{"location":"openshift/storage/storage-overview/#persistent-volume-claims-pvcs","title":"Persistent volume claims (PVCs)","text":"<p>You can use a PVC to mount a PersistentVolume into a Pod. You can access the storage without knowing the details of the cloud environment.</p> <p>Important Note</p> <p>A PVC is in active use by a pod when a Pod object exists that uses the PVC.</p>"},{"location":"openshift/storage/storage-overview/#access-modes","title":"Access modes","text":"<p>Volume access modes describe volume capabilities. You can use access modes to match persistent volume claim (PVC) and persistent volume (PV). The following are the examples of access modes:</p> Storage Class Description ReadWriteOnce (RWO) Allows read-write access to the volume by a single node at a time. ReadOnlyMany (ROX) Allows multiple nodes to read from the volume simultaneously, but only one node can write to it. ReadWriteMany (RWX) Allows multiple nodes to read from and write to the volume simultaneously. ReadWriteOncePod (RWOP) Allows read-write access to the volume by multiple pods running on the same node simultaneously. <p>Very Important: ReadWriteMany (RWX) storage is currently not available.</p> <p>If your workload requires shared storage across multiple pods, you may use one of the following supported alternatives on NERC:</p> <p>1. NERC OpenStack Object Storage (Swift):</p> <ul> <li> <p>Prerequisites:</p> <p>You must have at least one active NERC (OpenStack) type resource allocation. You can refer to this documentation on how to get allocation and request \"NERC (OpenStack)\" type resource allocations on your project. The \"OpenStack Swift Quota (GiB)\" attribute defines the total object storage quota assigned to your allocation.</p> </li> </ul> <p>Very Important Note</p> <p>For any NERC (OpenStack) resource allocation, Object Storage (Swift) and Block Storage (Volumes/Cinder) are managed through separate allocation quota attributes: \"OpenStack Swift Quota (GiB)\" for object storage and \"OpenStack Volume Quota (GiB)\" for block storage.</p> <p>For more details on how to set up persistent storage using OpenStack Object Storage (Swift), follow this user guide.</p> <p>2. MinIO-based Local S3-Compatible Object Storage on NERC OpenShift (OCP):</p> <ul> <li> <p>Prerequisites:</p> <p>You must have at least one active NERC-OCP (OpenShift) type resource allocation. You can refer to this documentation on how to get allocation and request \"NERC-OCP (OpenShift)\" type resource allocations on your project. The \"OpenShift Request on NESE Storage Quota (GiB)\" attribute defines the total storage quota assigned to your allocation.</p> </li> </ul> <p>Very Important Note</p> <p>For any NERC-OCP (OpenShift) resource allocation, the \"OpenShift Request on NESE Storage Quota (GiB)\" attribute defines the total storage capacity available to your allocation. This quota is used by MinIO-based storage, which mounts a PersistentVolume through a PVC into the running Pod.</p> <p>For more details on setting up persistent storage using MinIO-based storage, follow this user guide. You may also review how it is configured in the following example projects:</p> <ul> <li> <p>Credit Card Fraud Detection Application </p> </li> <li> <p>Object Detection Application Using YOLOv5 Model</p> </li> </ul> <p>These options offer reliable and scalable alternatives until official ReadWriteMany (RWX) persistent storage class support becomes available soon on the NERC OCP.</p>"},{"location":"openshift/storage/storage-overview/#how-to-specify-persistent-storage-using-yaml","title":"How to specify Persistent storage using YAML?","text":"<p>Create a PersistentVolumeClaim and mount it in your Pod/Deployment. The cluster's StorageClass handles provisioning.</p> <pre><code># 1) Create a PVC\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce     # or other Access Mode if your storage supports sharing\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: ocs-external-storagecluster-ceph-rbd  # omit to use the default\n---\n# 2) Use the PVC in a Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels: { app: my-app }\n  template:\n    metadata:\n      labels: { app: my-app }\n    spec:\n      securityContext:\n        fsGroup: 1001   # helps with write permissions on many storage backends\n      containers:\n        - name: app\n          image: registry.access.redhat.com/ubi9/ubi-minimal\n          command: [\"/bin/sh\",\"-c\"]\n          args: [\"python3 -m http.server 8080\"]\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/data\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: data-pvc\n</code></pre> <p>Quick CLI equivalent (creates &amp; mounts a PVC to <code>/var/lib/data</code>)</p> <pre><code>oc set volume deploy/my-app \\\n    --add --name=data --type pvc \\\n    --claim-name=data-pvc --claim-size=10Gi \\\n    --mount-path=/var/lib/data \\\n    --claim-class=ocs-external-storagecluster-ceph-rbd\n</code></pre>"},{"location":"openshift/storage/storage-overview/#per-replica-storage-statefulset","title":"Per-replica storage (StatefulSet)","text":"<p>StatefulSets can auto-create one PVC per pod via <code>volumeClaimTemplates</code> as shown below:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: db\nspec:\n  serviceName: db\n  replicas: 3\n  selector:\n    matchLabels: { app: db }\n  template:\n    metadata:\n      labels: { app: db }\n    spec:\n      securityContext:\n        fsGroup: 1001\n      containers:\n        - name: db\n          image: registry.access.redhat.com/rhel9/postgresql-15\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/pgsql/data\n  volumeClaimTemplates:\n    - metadata:\n        name: data\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 20Gi\n        storageClassName: ocs-external-storagecluster-ceph-rbd  # omit to use the default\n</code></pre> <p>Tips &amp; gotchas</p> <ul> <li> <p>Permissions (arbitrary UIDs): OpenShift runs containers with a random UID by default. Use <code>spec.securityContext.fsGroup</code> (or an image that supports arbitrary UIDs) so the pod can write to the volume.</p> </li> <li> <p>Access modes: Use ReadWriteOnce (RWO) for a single writer; ReadWriteMany (RWX) if multiple pods must write concurrently (requires an RWX-capable StorageClass).</p> </li> <li> <p>StorageClass: If you omit <code>storageClassName</code>, the NERC OCP cluster's Default StorageClass i.e. <code>ocs-external-storagecluster-ceph-rbd</code> is used. Specify one explicitly to control performance/features.</p> </li> <li> <p>SubPath mounts: To mount only a subdirectory of a volume, set <code>subPath: &lt;dir&gt;</code> under <code>volumeMounts</code>.</p> </li> </ul>"},{"location":"openshift-ai/","title":"NERC RHOAI","text":""},{"location":"openshift-ai/#red-hat-openshift-ai-rhoai-tutorial-index","title":"Red Hat OpenShift AI (RHOAI) Tutorial Index","text":"<p>If you're just starting out, we recommend starting from Red Hat OpenShift AI (RHOAI) Overview and going through the tutorial in order.</p> <p>If you just need to review a specific step, you can find the page you need in the list below.</p>"},{"location":"openshift-ai/#nerc-openshift-ai-getting-started","title":"NERC OpenShift AI Getting Started","text":"<ul> <li>NERC Red Hat OpenShift AI (RHOAI) Overview &lt;&lt;-- Start Here</li> </ul>"},{"location":"openshift-ai/#nerc-openshift-ai-dashboard","title":"NERC OpenShift AI dashboard","text":"<ul> <li> <p>Access the NERC's OpenShift AI dashboard</p> </li> <li> <p>The NERC's OpenShift AI dashboard Overview</p> </li> </ul>"},{"location":"openshift-ai/#using-data-science-project-in-the-nerc-rhoai","title":"Using Data Science Project in the NERC RHOAI","text":"<ul> <li> <p>Using Your Data Science Project (DSP)</p> </li> <li> <p>Explore the JupyterLab Environment</p> </li> <li> <p>Model Serving in the NERC RHOAI</p> </li> <li> <p>Test the Model in the NERC RHOAI</p> </li> </ul>"},{"location":"openshift-ai/#other-example-projects","title":"Other Example Projects","text":"<ul> <li> <p>How to access, download, and analyze data for S3 usage</p> </li> <li> <p>Configure a Jupyter Notebook to use GPUs for AI/ML modeling</p> </li> </ul>"},{"location":"openshift-ai/#predictive-generative-ai-on-nerc","title":"Predictive &amp; Generative AI on NERC","text":"<ul> <li>Predictive &amp; Generative AI (LLM &amp; RAG) in Action</li> </ul>"},{"location":"openshift-ai/#predictive-ai-tutorial","title":"Predictive AI tutorial","text":"<ul> <li> <p>Credit Card Fraud Detection Application</p> </li> <li> <p>Object Detection Application Using YOLOv5 Model</p> </li> </ul>"},{"location":"openshift-ai/#generative-ai-tutorial","title":"Generative AI tutorial","text":"<ul> <li> <p>Large Language Model (LLM) - Chat</p> </li> <li> <p>Retrieval Augumented Generation (RAG) - Talk with your PDF</p> </li> <li> <p>Deploying a Llama model with KServe</p> </li> <li> <p>Serving Text Generation Inference Service (TGIS) and FLAN-T5 Small Model</p> </li> </ul>"},{"location":"openshift-ai/#llm-clients","title":"LLM Clients","text":"<ul> <li>LLM Client - AnythingLLM</li> </ul>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/","title":"Explore the JupyterLab Environment","text":""},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#explore-the-jupyterlab-environment","title":"Explore the JupyterLab Environment","text":"<p>When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment:</p> <p></p> <p>How can I start or stop a Workbench?</p> <p>If the status of the workbench is <code>Stopped</code>, in the Status column for the workbench, click <code>Start</code>. The Status column changes from <code>Stopped</code> to <code>Starting</code> when the workbench server is starting, and then to <code>Running</code> when the workbench has successfully started.</p> <p>Make sure you are selecting \"mss-keycloak\" once shown:</p> <p></p> <p>Authorize the requested permissions if needed:</p> <p></p> <p>This will initiate your JupyterLab environment based on the Jupyter Image you have selected. JupyterLab offers a shared interactive integrated development environment.</p> <p>Once you successfully authenticate, you should see the NERC RHOAI JupyterLab Web Interface as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#clone-a-git-repository","title":"Clone a Git repository","text":"<p>You can clone a Git repository in JupyterLab through the left-hand toolbar or the Git menu option in the main menu as shown below:</p> <p></p> <p>Let's clone a repository using the left-hand toolbar. Click on the Git icon, shown in below:</p> <p></p> <p>Then click on Clone a Repository as shown below:</p> <p></p> <p>Enter the git repository URL, which points to the end-to-end ML workflows demo project i.e. https://github.com/nerc-project/nerc_rhoai_mlops.</p> <p>Then click Clone button, as shown below:</p> <p></p> <p>Alternatively, you can clone any Public Git repository directly by clicking on the Git Clone icon in the toolbar, as shown below:</p> <p></p> <p>What is MLOps?</p> <p>Machine learning operations (MLOps) are a set of practices that automate and simplify machine learning (ML) workflows and deployments. MLOps encompasses the tools, platforms, and processes required to build, train, deploy, monitor, and continuously improve AI/ML models for cloud-native applications.</p> <p>To learn more about the end-to-end reference design for MLOps, read the blog: Enterprise MLOps Reference Design.</p> <p>Cloning takes a few seconds, after which you can double-click and navigate to the newly-created folder that contains your cloned Git repository.</p>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#exploring-the-example-nerc-mlops-project","title":"Exploring the Example NERC MLOps Project","text":"<p>You will be able to find the newly-created folder named <code>nerc_rhoai_mlops</code> based on the Git repository name, as shown below:</p> <p></p>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#working-with-notebooks","title":"Working with notebooks","text":""},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#whats-a-notebook","title":"What's a notebook?","text":"<p>A notebook is an environment where you have cells that can display formatted text, or code.</p> <p>This is an empty cell:</p> <p></p> <p>And a cell where we have entered some Python code:</p> <p></p> <ul> <li> <p>Code cells contain Python code that can be run interactively. It means that you     can modify the code, then run it, but only for this cell, not for the whole     content of the notebook! The code will not run on your computer or in the browser,     but directly in the environment you are connected to NERC RHOAI.</p> </li> <li> <p>To run a code cell, you simply select it (select the cell, or on the left side     of it), and select the Run/Play button from the toolbar (you can also press     <code>CTRL+Enter</code> to run a cell, or <code>Shift+Enter</code> to run the cell and automatically     select the following one).</p> </li> </ul> <p>The Run button on the toolbar:</p> <p></p> <p>As you will see, you then get the result of the code that was run in that cell (if the code produces some output), as well as information on when this particular cell has been run.</p> <p>When you save a notebook, the code as well as all the results are saved! So you can always reopen it to look at the results without having to run all the program again, while still having access to the code that produced this content.</p> <p>More about Notebook</p> <p>Notebooks are so named because they are just like a physical Notebook. It is exactly like if you were taking notes about your experiments (which you will do), along with the code itself, including any parameters you set. You see the output of the experiment inline (this is the result from a cell once it is run), along with all the notes you want to take (to do that, you can switch the cell type from the menu from <code>Code</code> to <code>Markup</code>).</p>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#sample-jupyter-notebook-files","title":"Sample Jupyter Notebook files","text":"<p>In your Jupyter environment, you can navigate and select any Jupyter notebook files by double-clicking them in the file explorer on the left side. Double-click the notebook file to launch it. This action will open another tab in the content section of the environment, on the right.</p> <p>Here, you can find three primary starter notebooks for setting up the intelligent application: <code>01_sandbox.ipynb</code>, <code>02_model_training_basics.ipynb</code>, <code>03_remote_inference_grpc.ipynb</code>, and <code>04-remote_inference_rest.ipynb</code> within the root folder path of <code>nerc_rhoai_mlops</code>.</p> <p>You can click and run <code>01_sandbox.ipynb</code> to verify the setup JupyterLab environment can run python code properly.</p> <p>Also, you can find the \"samples\" folder within the root folder path of <code>nerc_rhoai_mlops</code>. For learning purposes, double-click on the \"samples\" folder under the newly-created folder named <code>nerc_rhoai_mlops</code>. Within the \"samples\" folder, you'll find some starter Jupyter notebook files: <code>Intro.ipynb</code>, <code>Lorenz.ipynb</code>, and <code>gpu.ipynb</code>. These files can be used to test basic JupyterLab functionalities. You can explore them at your own pace by running each of them individually. Please feel free to experiment, run the different cells, add some more code. You can do what you want - it is your environment, and there is no risk of breaking anything or impacting other users. This environment isolation is also a great advantage brought by NERC RHOAI.</p> <p>How to get access to the NERC RHOAI Dashboard from JupyterLab Environment?</p> <p>If you had closed the NERC RHOAI dashboard, you can access it from your currently opened JupyterLab IDE by clicking on File -&gt; Hub Control Panel as shown below:</p> <p></p>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#testing-for-gpu-code","title":"Testing for GPU Code","text":"<p>As we have setup the workbench specifing the desired Accelerator as NVIDIA V100 GPU and Number of accelerators: \"1\", we will be able to test GPU based code running <code>gpu.ipynb</code> notebook file as shown below:</p> <p></p>"},{"location":"openshift-ai/data-science-project/explore-the-jupyterlab-environment/#training-a-model","title":"Training a model","text":"<p>Within the root folder path of <code>nerc_rhoai_mlops</code>, find a sample Jupyter notebook file <code>02_model_training_basics.ipynb</code> that demonstrates how to train a model within the NERC RHOAI. To run it you need to double click it and execute the \"Run\" button to run all notebook cells at once. This is used to train your model for \"Basic classification of clothing images\" by importing the publicly available Fashion MNIST dataset and using TensorFlow. This process will take some time to complete. At the end, it will generate and save the model <code>my-model.keras</code> within the root folder path of <code>nerc_rhoai_mlops</code>.</p> <p>The Machine Learning Model File Hosted on A Bucket.</p> <p>The model we are going to use is an object detection model that is able to isolate and recognize T-shirts, bottles, and hats in pictures. Although the process is globally the same one as what we have seen in the Training a model section, this model has already been trained as it takes a few hours with the help of a GPU to do it.</p> <p>The resulting model has been saved in the ONNX format, an open standard for machine learning interoperability, which is one we can use with OpenVINO and RHOAI model serving.</p> <p>In this tutorial, the fine-tuned model has been stored and is available for download on the NERC OpenStack Object Storage container, as described here. However, you can also set up your own S3 backend to store the model by using a script to configure local S3 storage (MinIO), as explained here.</p> <p>Visualizing the Machine Learning Model</p> <p>You can use Netron, a cross-platform machine learning model visualization tool, to explore, analyze, and share the structure of your model by simply uploading a supported machine learning model file.</p> <p>For example:</p> <p></p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/","title":"Model Serving in the NERC RHOAI","text":""},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#model-serving-in-the-nerc-rhoai","title":"Model Serving in the NERC RHOAI","text":""},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#model-serving-features","title":"Model Serving Features","text":""},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#model-serving-workflow","title":"Model Serving Workflow","text":"<p>Prerequisites:</p> <p>To run a model server and deploy a model on it, you need to have:</p> <ul> <li>Select the correct data science project and create workbench, see Populate     the data science project     for more information.</li> </ul>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#create-a-model-server","title":"Create a model server","text":"<p>After creating the connection, you can add your model server. In the OpenShift AI dashboard, navigate to the data science project details page and click the Models tab. If this is the first time, then you will be able to choose the model serving type, either a Single-model serving platform or a Multi-model serving platform to be used when deploying from this project. The model-serving UI is integrated into the OpenShift AI dashboard and project workspaces, and cluster resources scale up or down with demand.</p> <p></p> <p>OpenShift AI offers two options for model serving:</p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#1-single-model-serving","title":"1. Single-model Serving","text":"<p>Each model is deployed on its own dedicated model server. This approach is ideal for:</p> <ul> <li> <p>Large language models (LLMs)</p> </li> <li> <p>Generative AI</p> </li> <li> <p>Models that require dedicated resources</p> </li> </ul> <p>The single-model serving platform is based on the KServe component.</p> <p>Important Note</p> <p>If you want to deploy each model on its own runtime server, or use a serverless deployment, select the Single-model serving platform. This option is recommended for production use.</p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#setting-up-the-single-model-server","title":"Setting up the Single-model Server","text":"<ol> <li> <p>In the left menu, click Data science projects.</p> <p>The Data science projects page opens.</p> </li> <li> <p>Click the name of the project that you want to deploy a model in.</p> <p>A project details page opens.</p> </li> <li> <p>Click the Models tab.</p> </li> <li> <p>Perform one of the following actions:</p> <ul> <li> <p>If you see a \u200b\u200bSingle-model serving platform tile, click Select single-model     on the tile and then click the Deploy model button.</p> <p></p> </li> <li> <p>If you don't see any tiles, ensure that you've already selected the     \"Single-model serving platform\" then click the Deploy model button.</p> <p></p> </li> </ul> </li> <li> <p>The Deploy model dialog opens.</p> <p>In the pop-up window that appears, you can specify the following details:</p> <ul> <li> <p>Model deployment name: This is the name of the inference service created     when the model is deployed.</p> </li> <li> <p>Serving runtime: Select a model-serving runtime framework from the available     options in your OpenShift Data Science deployment. This framework is used     to deploy and serve machine learning models. For LLMs that need maximum     scalability and throughput, OpenShift AI offers parallelized, multi-node     serving with vLLM runtimes to handle high volumes of concurrent, real-time     requests.</p> </li> <li> <p>Model framework (name - version): This will be auto selected based on     your Serving runtime selection.</p> </li> <li> <p>Deployment mode: Deployment modes determine the technology stack used     to deploy a model, providing different levels of management, flexibility,     and scalability. You can read more about KServe deployment modes     here.</p> <p>The options available are:</p> <p>i. Advanced: Advanced deployment mode uses Knative Serverless. By     default, KServe integrates with Red Hat OpenShift Serverless and     Red Hat OpenShift Service Mesh to deploy models on the single-model     serving platform.</p> <ul> <li> <p>Number of model server replicas to deploy: This defines the number     of instances of the model server engine you want to deploy.</p> <p>Using the \"Advanced\" deployment mode, you can scale it up as needed by specifying the Minimum replicas and Maximum replicas, depending on the expected number of incoming requests.</p> <p>Auto-Scaling &amp; Scale-to-Zero for Significant Cost Savings</p> <p>Once you deploy your model and obtain the inference endpoints, you can edit the deployment and set the Minimum replicas to 0. This enables intelligent auto-scaling of your model's compute resources (CPU, GPU, RAM, etc.), allowing replicas to scale up during high traffic and scale down when idle. With <code>scale-to-zero</code> enabled, the system reduces pods to zero during inactivity, eliminating idle compute costs\u2014especially beneficial for GPU workloads. The model then scales back up instantly as soon as a new request arrives.</p> <p></p> </li> </ul> <p>ii. Standard: Alternatively, you can use standard deployment mode, which         uses KServe RawDeployment mode.</p> <ul> <li> <p>Number of model server replicas to deploy: This defines the number     of instances of the model server engine you want to deploy.</p> <p>In \"Standard\" deployment mode, you cannot scale the number of replicas up or down, unlike in \"Advanced\" deployment mode.</p> <p></p> </li> </ul> </li> <li> <p>Model server size: This is the amount of resources, CPU, and RAM that     will be allocated to your server. Select the appropriate configuration for     size and the complexity of your model.</p> <p>Custom Model Server Size Option</p> <p>With the Custom option, you can tailor the CPU and Memory allocations of your model server to match the specific needs of your workload.</p> <p></p> <p>If you select Custom from the dropdown menu, you can configure the following settings in the Model server size section to customize your model server:</p> <p></p> <ol> <li> <p>CPUs requested \u2013 Specify the number of CPUs to allocate for your model server. Use the dropdown beside this field to choose the value in cores or millicores.</p> </li> <li> <p>CPU limit \u2013 Set the maximum number of CPUs the model server can use. Use the dropdown beside this field to specify the value in cores or millicores.</p> </li> <li> <p>Memory requested \u2013 Specify the amount of memory requested for the model server in gibibytes (Gi).</p> </li> <li> <p>Memory limit \u2013 Set the maximum memory limit for the model server in gibibytes (Gi).</p> </li> </ol> </li> <li> <p>Accelerator: This allows you to add a GPU to your model server,     enabling it to leverage optimized hardware for faster inference and improved     efficiency.</p> <p>Serving Runtime and Accelerator Compatibility</p> <p>If you need to use an Accelerator, it is recommended to select a compatible Serving runtime for optimal performance. Also, Number of accelerators (GPUs) is based on your available quota for GPUs for your project.</p> </li> <li> <p>Model route: select the     Make deployed models available through an external route checkbox, if     you want the serving endpoint (the model serving API) to be accessible outside     of the OpenShift cluster through an external route.</p> </li> <li> <p>Token authorization: To require token authentication for inference requests     to the deployed model, perform the following actions:</p> <p>i. Select Require token authentication.</p> <p>ii. In the Service account name field, enter the service account name that the token will be generated for.</p> <p>iii. To add an additional service account, click Add a service account and enter another service account name.</p> </li> <li> <p>Source model location: To specify the location of your model, either     select an existing connection you previously created or create a new one.</p> <p>Very Important</p> <p>If your connection type is an S3-compatible object storage, you must provide the folder path that contains your data file. The OpenVINO Model Server runtime has specific requirements for how you specify the model path. For more information, see known issue RHOAIENG-3025 in the OpenShift AI release notes.</p> </li> <li> <p>Optional: Customize the runtime parameters in the Configuration parameters     section.</p> </li> </ul> <p>After adding and selecting options within the Deploy model pop-up window, click Deploy to create the model server.</p> </li> </ol> <p>Serving vLLM and Large Language Models (LLMs) with Red Hat OpenShift AI</p> <p>To learn how to deploy a Llama 3.2 3B quantized model - or any other Large Language Model (LLM) - with GPU acceleration, refer to the Deploying Llama 3.2 with KServe Guide.</p> <p>This guide also includes examples that demonstrate how to serve models using different Connection types, such as:</p> <ul> <li> <p>A locally hosted S3-compatible object storage system like MinIO - to store and access your model artifacts directly within your own project.</p> </li> <li> <p>A URI that references a ModelCar Container Image.</p> </li> </ul> <p>All deployment scenarios use the vLLM runtime, ensuring efficient and scalable model serving on OpenShift AI.</p> <p>To deploy a lightweight FLAN-T5 Small model without GPU acceleration using the TGIS Standalone ServingRuntime for KServe, refer to the Serving Text Generation Inference Service (TGIS) and FLAN-T5 Small Model.</p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#2-multi-model-serving","title":"2. Multi-model Serving","text":"<p>Very Important</p> <p>Starting with OpenShift AI version 2.19, the multi-model serving platform based on ModelMesh is deprecated. You can continue to deploy models on the multi-model serving platform, but it is recommended that you migrate to the single-model serving platform.</p> <p>All models within the project are deployed on a shared model server. This setup is best suited for:</p> <ul> <li> <p>Efficient resource sharing among models</p> </li> <li> <p>Lightweight models with lower resource demands</p> </li> </ul> <p>The multi-model serving platform is based on the ModelMesh component.</p> <p>Important Note</p> <p>If you want to deploy multiple models using a single runtime server, select the Multi-model serving platform. This option is ideal when deploying more than 1,000 small or medium-sized models and aiming to reduce resource consumption.</p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#setting-up-the-multi-model-server","title":"Setting up the Multi-model Server","text":"<ol> <li> <p>In the left menu, click Data science projects.</p> <p>The Data science projects page opens.</p> </li> <li> <p>Click the name of the project that you want to deploy a model in.</p> <p>A project details page opens.</p> </li> <li> <p>Click the Models tab.</p> </li> <li> <p>Perform one of the following actions:</p> <ul> <li> <p>If you see a \u200bMulti-model serving platform tile, click Select multi-model.</p> <p></p> </li> <li> <p>If you don't see any tiles, ensure that you've already selected the     \"\u200bMulti-model serving platform\".  </p> </li> <li> <p>You will be able to create a new model server by clicking the     Add model server button, as shown below:</p> <p> </p> </li> </ul> </li> <li> <p>In the pop-up window that appears, you can specify the following details:</p> <ul> <li> <p>Model server name: Enables users to enter a unique name for the model     server.</p> </li> <li> <p>Serving runtime: Select a model-serving runtime framework from the available     options in your OpenShift Data Science deployment. This framework is used     to deploy and serve machine learning models.</p> </li> <li> <p>Number of model server replicas to deploy: This is the number of instances     of the model server engine that you want to deploy. You can scale it up     as needed, depending on the number of requests you will receive.</p> </li> <li> <p>Model server size: This is the amount of resources, CPU, and RAM that     will be allocated to your server. Select the appropriate configuration for     size and the complexity of your model.</p> <p>Custom Model Server Size Option</p> <p>With the Custom option, you can tailor the CPU and Memory allocations of your model server to match the specific needs of your workload.</p> <p></p> <p>If you select Custom from the dropdown menu, you can configure the following settings in the Model server size section to customize your model server:</p> <p></p> <ol> <li> <p>CPUs requested \u2013 Specify the number of CPUs to allocate for your model server. Use the dropdown beside this field to choose the value in cores or millicores.</p> </li> <li> <p>CPU limit \u2013 Set the maximum number of CPUs the model server can use. Use the dropdown beside this field to specify the value in cores or millicores.</p> </li> <li> <p>Memory requested \u2013 Specify the amount of memory requested for the model server in gibibytes (Gi).</p> </li> <li> <p>Memory limit \u2013 Set the maximum memory limit for the model server in gibibytes (Gi).</p> </li> </ol> </li> <li> <p>Accelerator: This allows you to add a GPU to your model server,     enabling it to leverage optimized hardware for faster inference and improved     efficiency.</p> <p>Serving Runtime and Accelerator Compatibility</p> <p>If you need to use an Accelerator, it is recommended to select a compatible Serving runtime for optimal performance. Also, Number of accelerators (GPUs) is based on your available quota for GPUs for your project.</p> </li> <li> <p>Model route: Check this box if you want the serving endpoint (the model     serving API) to be accessible outside of the OpenShift cluster through an     external route.</p> </li> <li> <p>Token authorization: Check this box if you want to secure or restrict     access to the model by forcing requests to provide an authorization token.</p> </li> </ul> <p>After adding and selecting options within the Add model server pop-up window, click Add to create the model server.</p> </li> </ol> <p>For our example project, we will choose \"Multi-model serving platform\" and then add a new model server and let's name the Model server \"coolstore-modelserver\". We'll select the OpenVINO Model Server in Serving runtime.</p> <p>Please leave the other fields with their default settings, such as replicas set to <code>1</code>, size set to <code>Small</code>, and Accelerator set to <code>None</code>. At this point, do not check Make model available via an external route, as shown below:</p> <p></p> <p>Once you've configured your model server, you can deploy your model by clicking on \"Deploy model\" located on the right side of the running model server as shown below:</p> <p></p> <p>Alternatively, you can also do this from the main RHOAI dashboard's \"Model Serving\" menu item as shown below:</p> <p></p> <p>If you wish to view details for the model server, click on the link corresponding to the Model Server's Name. You can also modify a model server configuration by clicking on the three dots on the right side, and selecting Edit model server. This will bring back the same configuration page we used earlier. This menu also has the option for you to delete model server.</p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#create-a-connection","title":"Create a connection","text":"<p>Once we have our workbench and cluster storage set up, we can create connections. Click the \"Create connection\" button to open the connection configuration window as shown below:</p> <p></p> <p>Connections are configurations for remote data location. In the Add connection modal, select a Connection type. The OCI-compliant registry, S3 compatible object storage, and URI options are pre-installed connection types. Please select \"S3 compatible object storage - v1\" as your Connection type as shown below:</p> <p></p> <p>Other Connection Types</p> <p>OpenShift AI supports three connection types for accessing model:</p> <ul> <li> <p>OCI-compliant registry: For proprietary images requiring authentication.     If you selected OCI-compliant registry, in the Registry host field,     enter the path to the OCI-compliant registry where model is stored. Deploying     models from OCI containers is also known as Modelcars in KServe.</p> </li> <li> <p>S3 compatible object storage: For cloud storage solutions, which is     the one used in this demonstration. For S3-compatible object storage,     in the Path field, enter the folder path that contains the model in     your specified data source.</p> </li> <li> <p>URI: For publicly available resources. If you selected URI in the     preceding step, in the URI field, enter the model Uniform Resource Identifier     (URI) where the model is located.</p> </li> </ul> <p>Within this window, enter the information about the S3-compatible object bucket where the model is stored. Enter the following information:</p> <ul> <li> <p>Connection name: Enter a unique name for the connection. A resource name     is generated based on the name of the connection. A resource name is the label     for the underlying resource in OpenShift.</p> <p>Optional: Edit the default resource name. Note that you cannot change the resource name after you create the connection.</p> <p>Optional: Provide a description of the connection.</p> </li> <li> <p>Access Key: The access key to the bucket.</p> </li> <li> <p>Secret Key: The secret for the access key.</p> </li> <li> <p>Endpoint: The endpoint to connect to the storage.</p> </li> <li> <p>Region: The region to connect to the storage.</p> </li> <li> <p>Bucket: The name of the bucket.</p> </li> </ul> <p>NOTE: However, you are not required to use the S3 service from Amazon Web Services (AWS). Any S3-compatible storage i.e. NERC OpenStack Container (Ceph), Minio, AWS S3, etc. is supported.</p> <p></p> <p>For our example project, let's name it \"ocp-nerc-container-connect\", we'll select the \"us-east-1\" as Region, choose \"ocp-container\" as Bucket.</p> <p>The API Access EC2 credentials can be downloaded and accessed from the NERC OpenStack Project as described here. This credential file contains information regarding Access Key, Secret Key, and Endpoint.</p> <p>Very Important Note: If you are using an AWS S3 bucket, the Endpoint needs to be set as <code>https://s3.amazonaws.com/</code>. However, for the NERC Object Storage container, which is based on the Ceph backend, the Endpoint needs to be set as <code>https://stack.nerc.mghpcc.org:13808</code>, and the Region should be set as <code>us-east-1</code>.</p> <p>How to store &amp; connect to the model file in the object storage bucket?</p> <p>The model file(s) should have been saved into an S3-compatible object storage bucket (NERC OpenStack Container [Ceph], Minio, or AWS S3) for which you must have the connection information, such as location and credentials. You can create a bucket on your active project at the NERC OpenStack Project by following the instructions in this guide.</p> <p>The API Access EC2 credentials can be downloaded and accessed from the NERC OpenStack Project as described here.</p> <p>For our example project, we are creating a bucket named \"ocp-container\" in one of our NERC OpenStack project's object storage. Inside this bucket, we have added a folder or directory called \"coolstore-model\", where we will store the model file in ONNX format, as shown here:</p> <p></p> <p>ONNX: An open standard for machine learning interoperability.</p> <p>After completing the required fields, click Create. You should now see the connection displayed in the main project window as shown below:</p> <p></p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#deploy-the-model","title":"Deploy the model","text":"<p>To add a model to be served, click the Deploy model button. Doing so will initiate the Deploy model pop-up window as shown below:</p> <p></p> <p>Enter the following information for your new model:</p> <ul> <li> <p>Model Name: The name you want to give to your model (e.g., \"coolstore\").</p> </li> <li> <p>Model framework (name-version): The framework used to save this model.     At this time, OpenVINO IR or ONNX or Tensorflow are supported.</p> </li> <li> <p>Model Location: Select the connection you created as described here     to store the model. Alternatively, you can create a new connection directly     from this menu.</p> </li> <li> <p>Folder path: If your model is not located at the root of the bucket of your     connection, you must enter the path to the folder it is in.</p> </li> </ul> <p>For our example project, let's name the Model as \"coolstore\", select \"onnx-1\" for the framework, select the Data location you created before for the Model location, and enter \"coolstore-model\" as the folder path for the model (without leading /).</p> <p>When you are ready to deploy your model, select the Deploy button.</p> <p>When you return to the Deployed models page, you will see your newly deployed model. You should click on the 1 on the Deployed models tab to see details. When the model has finished deploying, the status icon will be a green checkmark indicating the model deployment is complete as shown below:</p> <p></p> <p>Important Note</p> <p>When you delete a model server, all models hosted on it are also removed, making them unavailable to applications.</p>"},{"location":"openshift-ai/data-science-project/model-serving-in-the-rhoai/#check-the-model-api","title":"Check the Model API","text":"<p>The deployed model is now accessible through the API endpoint of the model server. The information about the endpoint is different, depending on how you configured the model server.</p> <p>If you did not expose the model externally through a route, click on the \"Internal endpoint details\" link in the Inference endpoint section. A popup will display the address for the gRPC and the REST URLs for the inference endpoints as shown below:</p> <p></p> <p>Your model is now deployed and ready to use!</p> <p>Notes:</p> <ul> <li> <p>The REST URL displayed is only the base address of the endpoint. You must     append <code>/v2/models/name-of-your-model/infer</code> to it to have the full address.     Example: <code>http://modelmesh-serving.name-of-your-project-namespace:8008/v2/models/coolstore/infer</code></p> </li> <li> <p>The full documentation of the API (REST and gRPC) is available here.</p> </li> <li> <p>The gRPC proto file for the Model Server is available here.</p> </li> <li> <p>If you have exposed the model through an external route, the Inference endpoint     displays the full URL that you can copy.</p> </li> </ul> <p>Important Note</p> <p>Even when you expose the model through an external route, the internal ones are still available. They use this format:</p> <ul> <li> <p>REST: <code>http://modelmesh-serving.name-of-your-project:8008/v2/models/name-of-your-model/infer</code></p> </li> <li> <p>gRPC: <code>grpc://modelmesh-serving.name-of-your-project:8033</code>. Please make note of the grpcURL value, we will need it later.</p> </li> </ul>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/","title":"Test the Model in the NERC RHOAI","text":""},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#test-the-model-in-the-nerc-rhoai","title":"Test the Model in the NERC RHOAI","text":"<p>Now that the model server is ready to receive requests, we can test it.</p> <p>How to get access to the NERC RHOAI Dashboard from JupyterLab Environment?</p> <p>If you had closed the NERC RHOAI dashboard, you can access it from your currently opened JupyterLab IDE by clicking on File -&gt; Hub Control Panel as shown below:</p> <p></p>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#using-the-model-server-inference-endpoint","title":"Using the Model Server Inference Endpoint","text":"<p>As described here, once your model is successfully deployed using Models, the model is accessible through the inference API endpoints as shown below:</p> <p></p> <p>The Inference endpoint field contains an Internal Service link, click the link to open a popup that shows the URL details, and then take note of the grpcUrl and restUrl values.</p>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#using-the-model-server-to-do-an-inference-using-grpc","title":"Using the model server to do an inference using gRPC","text":"<ul> <li> <p>In your project in JupyterLab, open the notebook <code>03_remote_inference_grpc.ipynb</code>     and follow the instructions to see how the model can be queried.</p> </li> <li> <p>Update the <code>grpc_url</code> with your own grpcUrl value as previously noted.</p> <p></p> </li> <li> <p>Once you've completed the notebook's instructions, the object detection model     can isolate and recognize T-shirts, bottles, and hats in pictures, as shown below:</p> <p></p> </li> </ul>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#using-the-model-server-to-do-an-inference-using-rest","title":"Using the model server to do an inference using REST","text":"<ul> <li> <p>In your project in JupyterLab, open the notebook <code>04-remote_inference_rest.ipynb</code>     and follow the instructions to see how the model can be queried.</p> </li> <li> <p>Update the <code>rest_url</code> with the restUrl as previously noted.</p> <p></p> </li> <li> <p>Once you've completed the notebook's instructions, the object detection model     can isolate and recognize T-shirts, bottles, and hats in pictures, as shown below:</p> <p></p> </li> </ul>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#building-and-deploying-an-intelligent-application","title":"Building and deploying an intelligent application","text":"<p>The application we are going to deploy is a simple example of how you can add an intelligent feature powered by AI/ML to an application. It is a webapp that you can use on your phone to discover coupons on various items you can see in a store, in an augmented reality way.</p>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#architecture","title":"Architecture","text":"<p>The different components of this intelligent application are:</p> <p>\u2022 The Frontend: a React application, typically running in your phone's browser or a web browser on your computer.</p> <p>\u2022 The Backend: a NodeJS server, serving the application and relaying API calls,</p> <p>\u2022 The Pre-Post Processing Service: a Python FastAPI service, doing the image pre-processing, calling the model server API, and doing the post-processing before sending the results back.</p> <p>\u2022 The Model Server: the RHOAI component serving the model as an API to do the inference.</p>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#application-workflow-steps","title":"Application Workflow Steps","text":"<ol> <li> <p>Pass the image to the pre-post processing service</p> </li> <li> <p>Pre-process the image and call the model server</p> </li> <li> <p>Send back the inference result</p> </li> <li> <p>Post-process the inference and send back the result</p> </li> <li> <p>Pass the result to the frontend for display</p> </li> </ol>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#deploy-the-application-on-nerc-openshift","title":"Deploy the Application on NERC OpenShift","text":"<p>The deployment of the application is really easy, as we already created for you the necessary YAML files. They are included in the Git project we used for this example project. You can find them in the deployment folder inside your JupyterLab environment, or directly here.</p> <p>To deploy the Pre-Post Processing Service service and the Application:</p> <ul> <li> <p>From your NERC's OpenShift Web Console,     navigate to your project corresponding to the NERC RHOAI Data Science Project     and select the \"Import YAML\" button, represented by the \"+\" icon in the top     navigation bar as shown below:</p> <p></p> </li> <li> <p>Verify that you selected the correct project.</p> <p></p> </li> <li> <p>Either drag and drop <code>https://github.com/nerc-project/nerc_rhoai_mlops/blob/main/deployment/pre_post_processor_deployment.yaml</code>     or copy and paste the contents of this <code>pre_post_processor_deployment.yaml</code>     file into the opened Import YAML editor box. If you have named your model     coolstore as instructed, you're good to go. If not, modify the value on     line # 35     with the name you set. You can then click the Create button, as shown below:</p> <p></p> </li> <li> <p>Once Resource is successfully created, you will see the following screen:</p> <p></p> </li> <li> <p>Click on \"Import more YAML\" and Copy/Paste the content of the file <code>intelligent_application_deployment.yaml</code>     inside the opened YAML editor. Nothing to change here, you can then click the     Create button, as shown below:</p> <p></p> </li> <li> <p>If both deployments are successful, you will be able to see both of them grouped     under \"intelligent-application\" on the Workloads -&gt; Topology menu,     as shown below:</p> <p></p> </li> </ul>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#use-the-application","title":"Use the application","text":"<p>The application is relatively straightforward to use. Click on the URL for the Route <code>ia-frontend</code> that was created.</p> <p>You have first to allow it to use your camera, this is the interface you get:</p> <p></p> <p>You have:</p> <ul> <li> <p>The current view of your camera.</p> </li> <li> <p>A button to take a picture as shown here:</p> <p></p> </li> <li> <p>A button to switch from front to rear camera if you are using a phone:</p> <p></p> </li> <li> <p>A QR code that you can use to quickly open the application on a phone     (much easier than typing the URL!):</p> <p></p> </li> </ul> <p>When you take a picture, it will be sent to the <code>inference</code> service, and you will see which items have been detected, and if there is a promotion available as shown below:</p> <p></p>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#tweak-the-application","title":"Tweak the application","text":"<p>There are two parameters you can change on this application:</p> <ul> <li> <p>On the <code>ia-frontend</code> Deployment, you can modify the <code>DISPLAY_BOX</code> environment     variable from <code>true</code> to <code>false</code>. It will hide the bounding box and the inference     score, so that you get only the coupon flying over the item.</p> </li> <li> <p>On the <code>ia-inference</code> Deployment, the one used for pre-post processing, you     can modify the <code>COUPON_VALUE</code> environment variable. The format is simply an     Array with the value of the coupon for the 3 classes: bottle, hat, shirt. As     you see, these values could be adjusted in real time, and this could even be     based on another ML model!</p> </li> </ul>"},{"location":"openshift-ai/data-science-project/testing-model-in-the-rhoai/#model-performance-metrics","title":"Model Performance Metrics","text":"<p>Access a comprehensive set of model performance metrics to build custom visualizations or integrate with your observability stack - complete with out-of-the-box performance and operations dashboards - by clicking the deployed model's name.</p> <p></p>"},{"location":"openshift-ai/data-science-project/using-projects-the-rhoai/","title":"Using Your Data Science Project (DSP)","text":""},{"location":"openshift-ai/data-science-project/using-projects-the-rhoai/#using-your-data-science-project-dsp","title":"Using Your Data Science Project (DSP)","text":"<p>You can access your current projects by navigating to the \"Data Science Projects\" menu item on the left-hand side, as highlighted in the figure below:</p> <p></p> <p>If you have any existing projects, they will be displayed here. These projects correspond to your NERC-OCP (OpenShift) resource allocations.</p> <p>Why we need Data Science Project (DSP)?</p> <p>To implement a data science workflow, you must use a data science project. Projects allow you and your team to organize and collaborate on resources within separated namespaces. From a project you can create multiple workbenches, each with their own Jupyter notebook environment, and each with their own data connections and cluster storage. In addition, the workbenches can share models and data with pipelines and model servers.</p>"},{"location":"openshift-ai/data-science-project/using-projects-the-rhoai/#selecting-your-data-science-project","title":"Selecting your data science project","text":"<p>Here, you can click on specific projects corresponding to the appropriate allocation where you want to work. This brings you to your selected data science project's details page, as shown below:</p> <p></p> <p>Within the data science project, you can add the following configuration options:</p> <ul> <li> <p>Workbenches: are instances of your development and experimentation environment.     They typically contain IDEs, such as JupyterLab, RStudio, and Visual Studio Code.</p> </li> <li> <p>Pipelines: A list of created and configured data science pipeline servers     within the project.</p> <p>Pipelines allow you to run multiple steps in a data science workflow, where each step can be represented by an individual Jupyter notebook.</p> <p>For example, a typical workflow might begin with data cleaning, followed by model training and prediction generation. Pipelines connect these notebooks together and ensure that each step runs automatically in the correct sequence.</p> </li> <li> <p>Models: A list of models and model servers used within your project. Models     enable you to serve trained models for real-time inference. You can configure     multiple model servers per data science project. Once a model is built, you     can use it by sending input data\u2014typically through an API\u2014and receiving     predictions or results in response.  </p> <p>Models are generally stored as files in storage systems such as an S3 bucket or similar data store, which are accessed through configured connections.</p> <p>More information about Model Serving in NERC RHOAI can be found here.</p> </li> <li> <p>Cluster storage: Storage for your project in your OpenShift cluster. Cluster     storage uses a Persistent Volume Claim (PVC) to store your Jupyter notebooks     and associated data, ensuring that your work remains saved and accessible even     if the notebook server restarts.</p> </li> <li> <p>Connections: A list of data sources that your project uses, such as an S3     object bucket. Connections allow you to link your project to external     storage systems and services. Typically, models are saved using     S3-compatible storage through a connection, but connections can also     serve other purposes, such as:</p> <p>i. OCI-compliant registries \u2013 Integrate with container registries.</p> <p>ii. S3-compatible object storage \u2013 Save models, datasets, and other files.</p> <p>iii. URIs \u2013 Connect to various external data sources.</p> </li> <li> <p>Permissions: define which users and groups can access the project.</p> </li> </ul> <p>As you can see in the project's details figure, our selected data science project currently has no workbenches, pipelines, models, storage, connections, and permissions.</p>"},{"location":"openshift-ai/data-science-project/using-projects-the-rhoai/#populate-the-data-science-project-with-a-workbench","title":"Populate the data science project with a Workbench","text":"<p>Add a workbench by clicking the Create workbench button, as shown below:</p> <p></p> <p>What are Workbenches?</p> <p>Workbenches are development environments. They can be based on JupyterLab, but also on other types of IDEs, like VS Code or RStudio. You can create as many workbenches as you want, and they can run concurrently.</p> <p>On the Create workbench page, complete the following information.</p> <p>Note: Not all fields are required.</p> <ul> <li> <p>Name</p> </li> <li> <p>Description</p> </li> <li> <p>Notebook image (Image selection)</p> </li> <li> <p>Deployment size (Container size, Accelerator and Number of accelerators)</p> </li> <li> <p>Environment variables</p> </li> <li> <p>Cluster storage name</p> </li> <li> <p>Cluster storage description</p> </li> <li> <p>Persistent storage size</p> </li> <li> <p>Connections</p> </li> </ul> <p>How to specify CPUs, Memory, and GPUs for your JupyterLab workbench?</p> <p>You have the option to select different container sizes to define compute resources, including CPUs and memory. Each container size comes with pre-configured CPU and memory resources.</p> <p>Optionally, you can specify the desired Accelerator and Number of accelerators (GPUs), depending on the nature of your data analysis and machine learning code requirements. However, this number should not exceed the GPU quota specified by the value of the \"OpenShift Request on GPU Quota\" attribute that has been approved for this \"NERC-OCP (OpenShift)\" resource allocation on NERC's ColdFront, as described here.</p> <p>The different options for GPU accelerator are \"None\", \"NVIDIA A100 GPU\", \"NVIDIA H100 GPU\", and \"NVIDIA V100 GPU\" as shown below:</p> <p></p> <p>If you need to increase this quota value, you can request a change as explained here.</p> <p>Once you have entered the information for your workbench, click Create.</p> <p></p> <p>For our example project, let's name it \"Tensorflow Workbench\". We'll select the TensorFlow image, choose a Deployment size of Small, Accelerator of NVIDIA A100 GPU, Number of accelerators as 1 and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>After creating the workbench, you will return to your project page. It shows the status of the workbench as shown below:</p> <p></p> <p>When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment.</p> <p>How can I start or stop a Workbench?</p> <p>If the workbench status is <code>Stopped</code>, click the Start button in the Status column. The status will change from <code>Stopped</code> to <code>Starting</code> while the workbench server is initializing, and then to <code>Running</code> once it has successfully started.</p> <p></p> <p>Notice that under the status indicator the workbench is Running. However, if any issues arise, such as an \"exceeded quota\" error, a red exclamation mark will appear under the Status indicator, as shown in the example below:</p> <p></p> <p>You can hover over that icon to view details. Upon closer inspection of the error message and the \"Event log\", you will receive details about the issue, enabling you to resolve it accordingly.</p> <p></p> <p>Any cluster storage that you associated with the workbench during the creation process appears on the \"Cluster storage\" tab for the project. Also, you can review the attached cluster storage by expanding the workbench as shown above.</p> <p>More About Cluster Storage</p> <p>Cluster storage consists of Persistent Volume Claims (PVCs), which are persistent storage spaces available for storing your notebooks and data. You can create PVCs directly from here and mount them in your workbenches as needed. It's worth noting that a default cluster storage (PVC) is automatically created with the same name as your workbench to save your work.</p> <p>Note: Once persistent storage is created, you can only increase its size by editing it. Adjusting the storage size will restart the workbench, making it temporarily unavailable, with the downtime typically depending on the size increase.</p> <p></p>"},{"location":"openshift-ai/data-science-project/using-projects-the-rhoai/#launching-a-workbench-using-your-existing-storage","title":"Launching a Workbench Using Your Existing Storage","text":"<p>If you want to use previously created persistent storage that isn't attached to any existing workbench cluster, first detach the storage configured for the new workbench, as shown below:</p> <p></p> <p>Once detached, click the Attach existing storage button. A popup will appear allowing you to select from the available storage options, as shown below:</p> <p></p> <p>After selecting the appropriate storage from the available options, click the Attach storage button.</p> <p>Once the workbench is successfully created and in Running status, it will include all previously stored data and code mapped to the newly created workbench.</p> <p>Note</p> <p>If your data science work requires changes to your workbench image, container size, or identifying information, you can update your project's workbench settings accordingly. For workloads involving large datasets, you can enhance performance by assigning accelerators to your workbench.</p> <p></p> <p>Before clicking the action menu (\u22ee) at the end of the selected workbench row, make sure to change the workbench status from Running to Stopped by clicking the Stop button. Updating the workbench will cause it to restart, so ensure that you save all your current work to avoid losing any unsaved data before stopping your workbench.</p>"},{"location":"openshift-ai/data-science-project/using-projects-the-rhoai/#how-to-solve-the-deleted-workbench-image-issue","title":"How to Solve the Deleted Workbench Image Issue?","text":"<p>While navigating to your previously set up workbenches in your RHOAI project's Workbenches section, you may encounter a deleted image error message, as shown below:</p> <p></p> <p>The recommended approach is to Edit Workbench. You can select the appropriate image and version (if available) from the available options.</p> <p>Before clicking the action menu (\u22ee) at the end of the selected workbench row, make sure to change the workbench status from Running to Stopped by clicking the Stop button. Updating the workbench will cause it to restart, so ensure that you save all your current work to avoid losing any unsaved data before stopping your workbench.</p> <p></p> <p>Select the equivalent image and applicable version (if available) from the drop-down list under the Workbench Image section, as shown below:</p> <p></p> <p>Click the Update Workbench button.</p> <p>It may take some time for the changes to reflect and for the dashboard to return to the Workbenches page. Once the update is complete, click the Start button to restart the workbench.  </p> <p>Once the workbench status changes from Stopped to Running, you can click on its name link and access your workbench without any issues.</p>"},{"location":"openshift-ai/get-started/rhoai-overview/","title":"NERC RHOAI Overview","text":""},{"location":"openshift-ai/get-started/rhoai-overview/#red-hat-openshift-ai-rhoai-overview","title":"Red Hat OpenShift AI (RHOAI) Overview","text":"<p>NERC's Red Hat OpenShift AI (RHOAI) offers a versatile and scalable MLOps solution equipped with tools for rapid constructing, deploying, and overseeing AI-driven applications. Integrating the proven features of both Red Hat OpenShift AI and Red Hat OpenShift creates a comprehensive enterprise-grade artificial intelligence and machine learning (AI/ML) application platform, facilitating collaboration among data scientists, engineers, and app developers. NERC's RHOAI provides features such as multi-tenancy support and robust security for model serving, as well as integration with data services.</p> <p>OpenShift AI streamlines workflows for data ingestion, model training, model serving, and observability, enabling seamless collaboration between teams. This consolidated platform promotes consistency, security, and scalability, fostering seamless teamwork across disciplines and empowering teams to quickly explore, build, train, deploy, test machine learning models, and scale AI-enabled intelligent applications.</p> <p></p> <p>Formerly known as Red Hat OpenShift Data Science, OpenShift AI facilitates the complete journey of AI/ML experiments and models. OpenShift AI enables data acquisition and preparation, model training and fine-tuning, model serving and model monitoring, hardware acceleration, and distributed workloads using graphics processing unit (GPU) resources.</p>"},{"location":"openshift-ai/get-started/rhoai-overview/#ai-for-all","title":"AI for All","text":""},{"location":"openshift-ai/get-started/rhoai-overview/#mlops-with-red-hat-openshift-ai","title":"MLOps with Red Hat OpenShift AI","text":"<p>Machine learning operations (MLOps) are a set of practices that automate and simplify machine learning (ML) workflows and deployments. MLOps encompasses the tools, platforms, and processes required to build, train, deploy, monitor, and continuously improve AI/ML models for cloud-native applications.</p> <p>To learn more about the end-to-end reference design for MLOps, read the blog: Enterprise MLOps Reference Design.</p>"},{"location":"openshift-ai/get-started/rhoai-overview/#what-is-the-ml-lifecycle","title":"What is the ML lifecycle?","text":"<p>The ML lifecycle is a multi-phase process that turns large, diverse datasets and ample compute - together with open-source ML tools - into intelligent applications.</p> <p>At a high level, it includes four stages:</p> <p></p> <ol> <li> <p>Gather &amp; prepare data \u2013 Ensure data completeness and quality (cleaning, labeling, feature engineering).</p> </li> <li> <p>Develop the model \u2013 Train, validate, and select the model with the best performance.</p> </li> <li> <p>Integrate &amp; infer \u2013 Deploy the model into applications/services and serve predictions.</p> </li> <li> <p>Monitor &amp; manage \u2013 Track business/performance metrics, detect data/concept drift, and retrain as needed.</p> </li> </ol> <p>Recent enhancements to Red Hat OpenShift AI include:</p> <p></p> <ul> <li> <p>Model building and fine-tuning: Data scientists can explore and develop     models in a JupyterLab interface using secure, prebuilt     notebook images that include popular Python libraries (e.g. TensorFlow,     PyTorch) and GPU support via CUDA. Organizations can     also supply custom notebook images, enabling teams to create and collaborate     on notebooks while organizing work across projects and workbenches.</p> </li> <li> <p>Implementation Deployment pipelines for monitoring AI/ML experiments and     automating ML workflows accelerate the iteration process for data scientists     and developers of intelligent applications. This integration facilitates swift     iteration on machine learning projects and embeds automation into application     deployment and updates.</p> </li> <li> <p>Model evaluation: During model exploration and development, the LM Evaluation     (LM-Eval) component provides clear signals about the model quality. It benchmarks     large language models (LLMs) across a range of tasks - such as logical and     mathematical reasoning and adversarial natural-language challenges - using     industry-standard benchmark suites.</p> </li> <li> <p>Model serving now incorporates GPU assistance for inference tasks and custom     model serving runtimes, enhancing inference performance and streamlining the     deployment of foundational models. For generative AI workloads, OpenShift AI     provides vLLM-powered model inferencing, offering industry-leading performance     and efficiency across the most popular open source large language models (LLMs).</p> </li> <li> <p>With Model monitoring, organizations can oversee performance and operational     metrics through a centralized dashboard, enhancing management capabilities.     Data scientists can use out-of-the-box visualizations for performance and operations     metrics or integrate data with other observability services.</p> </li> </ul>"},{"location":"openshift-ai/get-started/rhoai-overview/#red-hat-openshift-ai-ecosystem","title":"Red Hat OpenShift AI ecosystem","text":"Name Description AI/ML modeling and visualization tools JupyterLab UI with prebuilt notebook images and common Python libraries and packages; TensorFlow; PyTorch, CUDA; and also support for custom notebook images Data engineering Support for different Data Engineering third party tools (optional) Data ingestion and storage Supports Amazon Simple Storage Service (S3) and NERC OpenStack Object Storage GPU support Available NVIDIA GPU Devices (with GPU operator): NVIDIA-A100-SXM4-40GB (A100), NVIDIA-H100-80GB-HBM3 (H100), and Tesla-V100-PCIE-32GB (V100) Model serving and monitoring Model serving (KServe with user interface), model monitoring, OpenShift Source-to-Image (S2I), Red Hat OpenShift API Management (optional add-on), Intel Distribution of the OpenVINO toolkit Data science pipelines Data science pipelines (Kubeflow Pipelines) chain together processes like data preparation, build models, and serve models"},{"location":"openshift-ai/logging-in/access-the-rhoai-dashboard/","title":"Access the NERC's OpenShift AI dashboard","text":""},{"location":"openshift-ai/logging-in/access-the-rhoai-dashboard/#access-the-nercs-openshift-ai-dashboard","title":"Access the NERC's OpenShift AI dashboard","text":"<p>Access the NERC's OpenShift Web Console via the web browser as described here.</p> <p>Make sure you are selecting \"mss-keycloak\" as shown here:</p> <p></p> <p>Once you successfully authenticate, you will see a graphical user interface displaying a list of projects on the Projects page based on your ColdFront allocations, as shown below:</p> <p></p> <p>After logging in to the NERC OpenShift console, access the NERC's Red Hat OpenShift AI dashboard by clicking the application launcher icon (the black-and-white icon that looks like a grid), located on the header as shown below:</p> <p></p> <p>OpenShift AI uses the same credentials as OpenShift for the dashboard, notebooks, and all other components. When prompted, log in to the OpenShift AI dashboard by using your OpenShift credentials by clicking \"Log In With OpenShift\" button as shown below:</p> <p></p> <p>Once the NERC OpenShift AI dashboard is launched, you can view a list of the installed OpenShift AI components, as shown below:</p> <p></p> <p>An overview of NERC's OpenShift AI dashboard can be found here.</p> <p>You can return to OpenShift Web Console by using the application launcher icon (the black-and-white icon that looks like a grid), and choosing the \"OpenShift Console\" as shown below:</p> <p></p>"},{"location":"openshift-ai/logging-in/the-rhoai-dashboard-overview/","title":"The NERC's OpenShift AI dashboard Overview","text":""},{"location":"openshift-ai/logging-in/the-rhoai-dashboard-overview/#the-nercs-openshift-ai-dashboard-overview","title":"The NERC's OpenShift AI dashboard Overview","text":"<p>In the NERC's RHOAI dashboard, you can see multiple links on your left hand side.</p> <p></p> <ol> <li> <p>Data Science Projects: View your existing projects. This will show different     projects corresponding to your NERC-OCP (OpenShift) resource allocations. Here,     you can choose specific projects corresponding to the appropriate allocation     where you want to work. Within these projects, you can create workbenches,     deploy various development environments (such as Jupyter Notebooks, VS Code,     RStudio, etc.), create connections, or serve models.</p> <p>More information about Data Science Projects (DSP) can be found here.</p> <p>What are Workbenches?</p> <p>Workbenches are development environments. They can be based on JupyterLab, but also on other types of IDEs, like VS Code or RStudio. You can create as many workbenches as you want, and they can run concurrently.</p> </li> <li> <p>Models: Manage and view the health and performance of your deployed     models across different projects corresponding to your NERC-OCP (OpenShift)     resource allocations.</p> <ul> <li> <p>Model registry: RHOAI provides a central place to view and manage registered     models, helping data scientists share, version, deploy and track predictive     and gen AI models, metadata and model artifacts. Model registries provide     a structured and organized way to store, share, version, deploy, and track     models. Users can select a model registry to view and manage your registered     models.</p> <p>Important Note</p> <p>This feature is not yet available to users.</p> </li> <li> <p>Model deployments: Manage and view the health and performance of your     deployed models. Also, you can \"Deploy Model\" to a specific project selected     from the dropdown menu here. Models enable you to quickly serve trained     models for real-time inference. Each data science project can have multiple     model servers, and each server can host models. More information about     Model Serving in NERC RHOAI can be found here.</p> </li> </ul> </li> <li> <p>Data Science Pipelines:</p> <ul> <li> <p>Pipelines: Manage your pipelines for a specific project selected from the     dropdown menu.  </p> </li> <li> <p>Runs: Manage and view your runs for a specific project selected from the     dropdown menu.</p> </li> </ul> </li> <li> <p>Experiments: An Experiment in RHOAI is a logical grouping of multiple     pipeline runs that share a common objective or context. For example, you might     create an experiment to test different hyperparameter configurations, preprocessing     strategies, or model architectures for the same dataset.</p> </li> <li> <p>Distributed workloads: Distributed workloads help teams accelerate data     processing and the full ML lifecycle-training, tuning, and serving - by prioritizing     and distributing jobs for optimal node utilization. Advanced GPU support ensures     the performance and scalability required for foundation-model workloads.</p> <p>Important Note</p> <p>This feature is not yet available to users.</p> </li> <li> <p>Applications:</p> <ul> <li> <p>Enabled: Launch your enabled applications, view documentation, or get     started with quick start instructions and tasks.</p> </li> <li> <p>Explore: View optional applications for your RHOAI instance.</p> </li> </ul> <p>Important Note</p> <p>Most of Applications are disabled by default on NERC RHOAI right now.</p> </li> <li> <p>Resources: Access all learning resources that Resources showcases various     tutorials or demos helping your onboarding to the RHOAI platform.</p> </li> </ol>"},{"location":"openshift-ai/other-projects/LLM-chat/","title":"Large Language Model (LLM) - Chat","text":""},{"location":"openshift-ai/other-projects/LLM-chat/#large-language-model-llm-chat","title":"Large Language Model (LLM) - Chat","text":"<p>llama.cpp is an open-source software library, primarily written in C++, designed for performing inference on various large language models, including Llama. It is developed in collaboration with the GGML project, a general-purpose tensor library.</p> <p>The library includes command-line tools as well as a server featuring a simple web interface.</p>"},{"location":"openshift-ai/other-projects/LLM-chat/#standalone-deployment-of-llamacpp-model-server","title":"Standalone Deployment of <code>llama.cpp</code> Model Server","text":"<ul> <li> <p>Prerequisites:</p> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI to enable <code>oc</code> commands. Refer to this user guide.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/LLM-chat/#deployment-steps","title":"Deployment Steps","text":"<ol> <li> <p>Clone or navigate to this repository.</p> <p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/llm-on-nerc.git\ncd llm-on-nerc/llm-servers/llama.cpp/\n</code></pre> <p>Read More</p> <p>For more details, check out the documentation.</p> <p>In the <code>standalone</code> folder, you will find the following YAML files:</p> <p>i. <code>01-llama-cpp-pvc.yaml</code>: Creates a persistent volume to store the model file. Adjust the storage size according to your needs.</p> <p>ii. <code>02-llama-cpp-deployment.yaml</code>: Deploys the application.</p> <p>iii. <code>03-llama-cpp-service.yaml</code>, <code>04-llama-cpp-route.yaml</code>: Set up external access to connect to the Inference runtime Web UI.</p> </li> <li> <p>Run this <code>oc</code> command: <code>oc apply -f ./standalone/.</code> to execute all YAML files located in the standalone folder.</p> </li> </ol> <p>This deployment sets up a ready-to-use container runtime that pulls the Mistral-7B-Instruct-v0.3.Q4_K_M.gguf pre-trained foundational model from Hugging Face Hub.</p> <p>About Mistral-7B Model</p> <p>Mistral-7B is a high-performance, relatively lightweight LLM, fully open-source under the Apache 2.0 license. The Mistral-7B-Instruct-v0.3 LLM is an instruct fine-tuned version of the Mistral-7B-v0.3.</p>"},{"location":"openshift-ai/other-projects/LLM-chat/#opening-the-inference-runtime-ui","title":"Opening the Inference runtime UI","text":"<ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>In the Navigation Menu, navigate to the Workloads -&gt; Topology menu.</p> </li> <li> <p>Click the button to open the llama-cpp-server UI:</p> <p></p> <p></p> <p></p> <p>For a better experience customize the Chat UI and Prompt Style:</p> <p></p> </li> <li> <p>Test your inferencing by querying the inferencing runtime at the \"Say Something\"     box:</p> <p></p> </li> <li> <p>Start Chatting:</p> <p>You can begin interacting with the LLM.</p> <p></p> </li> </ol>"},{"location":"openshift-ai/other-projects/LLM-chat/#clean-up","title":"Clean Up","text":"<p>To delete all resources if not necessary just run <code>oc delete -f ./standalone/.</code> or <code>oc delete all,pvc -l app=llama-cpp</code>.</p> <p>For more details, refer to this documentation.</p> <p>Another LLM Server with Open WebUI to Chat</p> <p>Similar to <code>llama.cpp</code>, you can set up an example deployment of the Ollama server on the NERC OpenShift environment by following these steps.</p> <p>Once successfully deployed, you can access the Open WebUI for Ollama, allowing you to log in, download new models, and start chatting!</p> <p></p> <p>Connecting LLM Clients to the Deployed LLM Providers</p> <p>To learn more about how to use LLM clients i.e. AnythingLLM to connect with the locally deployed LLM providers on NERC OpenShift, please refer to this detailed guide.</p>"},{"location":"openshift-ai/other-projects/LLM-client-AnythingLLM/","title":"LLM Client - AnythingLLM","text":""},{"location":"openshift-ai/other-projects/LLM-client-AnythingLLM/#llm-client-anythingllm","title":"LLM Client - AnythingLLM","text":"<p>LLM clients provide user-friendly interfaces for interacting with LLMs. These clients help streamline the deployment and usage of LLMs for various applications, such as chatbots, document processing, and AI-powered automation.</p>"},{"location":"openshift-ai/other-projects/LLM-client-AnythingLLM/#anythingllm","title":"AnythingLLM","text":"<p>AnythingLLM is an open-source framework that enables users to connect and interact with LLMs efficiently. It supports integration with multiple AI models, providing a web-based UI for seamless communication, ensuring you're not limited to a single provider. To integrate an LLM, simply provide the endpoint URL and, if necessary, an authentication token.</p> <p>It facilitates conversations with documents in various formats, including PDFs, TXT, and CSVs. This tool is especially useful for creating private, localized versions of ChatGPT, allowing users to upload files and receive context-aware responses based on their content.</p>"},{"location":"openshift-ai/other-projects/LLM-client-AnythingLLM/#deploying-as-a-workbench-using-a-data-science-project-dsp-on-nerc-rhoai","title":"Deploying as a Workbench Using a Data Science Project (DSP) on NERC RHOAI","text":"<p>Here, we'll guide you through deploying AnythingLLM as a workbench on NERC RHOAI to create a private chatbot for internal users. AnythingLLM enables teams to securely interact with documents and knowledge bases by integrating various LLMs and LLM servers with private data in a controlled environment.</p> <p>By leveraging NERC OpenShift's powerful AI platform and built-in security features, we can deploy AnythingLLM as an efficient and secure internal chatbot solution.</p> <p>Prerequisites:</p> <ul> <li>Before proceeding, confirm that you have a locally running Ollama Model     Serving instance on your NERC OpenShift environment setup by following     these instructions     for a Standalone Deployment.</li> </ul> <p>Procedure:</p> <ol> <li> <p>Navigating to the OpenShift AI dashboard.</p> <p>Please follow these steps to access the NERC OpenShift AI dashboard.</p> </li> <li> <p>Please ensure that you start your AnythingLLM server with options as depicted     in the following configuration screen. This screen provides you     with the opportunity to select a notebook image and configure its options,     including the Accelerator and Number of accelerators (GPUs).</p> <p></p> <p>For our example project, let's name it \"AnythingLLM Workbench\". We'll select the AnythingLLM image, choose a Deployment size of Small, Accelerator as None (no GPU is needed for this setup), and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Tip</p> <p>The dashboard currently enforces a minimum storage volume size of 20GB. Please ensure that you modify this based on your need in Cluster Storage.</p> </li> <li> <p>If this procedure is successful, you have started your AnythingLLM Workbench.     When your workbench is ready and the status changes to Running, click the     open icon () next to your workbench's name,     or click the workbench name directly to access your environment:</p> <p></p> </li> <li> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when     prompted, as shown below:</p> <p></p> <p>Next, you should see the AnythingLLM welcome splash screen, as shown below:</p> <p></p> <p>Click on Get started.</p> </li> <li> <p>Configure the LLM Endpoint to Connect the Workbench to your locally deployed     Ollama Model Serving instance:</p> <ul> <li> <p>Search LLM Providers: Scroll through the list and select Ollama     from the available options.</p> </li> <li> <p>Ollama Base URL: Enter the URL where Ollama is running. For your     locally deployed Ollama Model Serving instance, you have two main     options for the \"Ollama Base URL,\" as explained below:</p> <ul> <li> <p>Internal Service Endpoint:  </p> <p>Important Note</p> <p>This option is only accessible within the OpenShift cluster and cannot be accessed externally.</p> <ul> <li> <p>You can use the internal service URL for the Ollama service within     your OpenShift environment based on the service name and exposed     port, such as <code>http://ollama-service:11434</code>.</p> <p></p> </li> <li> <p>Alternatively, you can click on the service name to view details,     including the internal service routing Hostname and Port,     as shown below:</p> <p></p> <p>Thus, the internal service URL will be: <code>http://ollama-service.&lt;your-namespace&gt;.svc.cluster.local:11434</code>.</p> </li> </ul> </li> <li> <p>Public Route URL:</p> <ul> <li>The Ollama service can be accessed externally using the public     Route URL provided by OpenShift. The URL follows this format:     <code>https://ollama-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.</li> </ul> </li> </ul> <p>Choose the appropriate URL option based on your needs and whether the Ollama service is intended for internal use or external access.</p> </li> <li> <p>Ollama Model: Choose the specific Ollama model you want to use for     your conversations.</p> </li> <li> <p>Max Tokens: Specify the maximum number of tokens to be used for     context and responses. A good starting point is 4096, but you can adjust     this later within your workspaces.</p> </li> </ul> <p></p> </li> <li> <p>Set Up User Access by selecting <code>Just me</code> on the next screen. Since OpenShift's     authentication ensures that only you can access your workbench, this option     is appropriate.</p> <p>You will then be prompted to set up a secondary password. However, this step is generally not necessary, as access to the workbench is already secured by OpenShift authentication.</p> <p></p> </li> <li> <p>Then, on the Review Configuration screen, you will see a summary of your     settings. Take a moment to confirm that everything looks correct before proceeding.</p> <p></p> <p>You may encounter a brief survey during the setup process, which you can skip if you prefer.</p> <p></p> </li> <li> <p>Next, you'll set up your First Workspace, which serves as a project area     within AnythingLLM. Each workspace can have its own settings and data, enabling     you to organize different tasks or experiments independently.</p> <p></p> </li> <li> <p>The initial view will provide you with various information, but you can go     straight to your workspace and start interacting with the LLM immediately.</p> <p></p> </li> </ol> <p>There's a lot you can do with AnythingLLM, so be sure to explore its features! For more details, check out the documentation.</p>"},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/","title":"Retrieval Augumented Generation (RAG) - Talk with your PDF","text":""},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/#rag-application-talk-with-your-pdf","title":"RAG Application - Talk with your PDF","text":"<p>For this walkthrough, we will be using an application that is a RAG-based Chatbot. It will utilize a Qdrant vector store, Ollama for LLM serving, Langchain as the \"glue\" between these components, and Gradio as the UI engine.</p> <p>This setup enables efficient RAG by leveraging vector search, embedding and an optimized inference engine.</p>"},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/#model-serving","title":"Model Serving","text":"<p>Deploy Ollama Model Serving instance on the NERC OpenShift environment by following these intructions.</p>"},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/#pull-the-required-model-for-rag","title":"Pull the Required Model for RAG","text":"<p>Once the Ollama setup is successfully completed, you will be able to access the Open WebUI for Ollama as explained here.</p> <p>Using Open WebUI, you can download and manage LLM models as per your need. For our RAG application, we are going to use Phi-3 model which is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft. We are going to pull the Phi-3 model using Open WebUI as shown below:</p> <p></p> <p>Alternatively, you can Pull the Models using the \"Terminal\" connected to the Ollama pod.</p>"},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/#deploying-a-vector-database","title":"Deploying a Vector Database","text":"<p>For our RAG application, we need a Vector Database to store the embeddings of different documents. In this case, we are using Qdrant.</p> <p>You can deploy and run the Qdrant vector database directly on the NERC OpenShift environment by following these intructions.</p> <p>After you follow those instructions, you should have a Qdrant instance ready to be populated with documents.</p> <p>Facing Rate Limits While Pulling Container Image?</p> <p>If you encounter Rate Limits while pulling Container Images, refer to this guide for detailed steps on how to resolve the issue.</p>"},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/#deploying-as-a-workbench-using-a-data-science-project-dsp-on-nerc-rhoai","title":"Deploying as a Workbench Using a Data Science Project (DSP) on NERC RHOAI","text":"<p>Prerequisites:</p> <ul> <li>Before proceeding, confirm that you have an active GPU quota that has been approved     for your current NERC OpenShift Allocation through NERC ColdFront. Read     more about How to Access GPU Resources     on NERC OpenShift Allocation.</li> </ul> <p>Procedure:</p> <ol> <li> <p>Navigating to the OpenShift AI dashboard.</p> <p>Please follow these steps to access the NERC OpenShift AI dashboard.</p> </li> <li> <p>Please ensure that you start your Jupyter notebook server with options as     depicted in the following configuration screen. This screen provides you     with the opportunity to select a notebook image and configure its options,     including the Accelerator and Number of accelerators (GPUs).</p> <p></p> <p>For our example project, let's name it \"RAG Workbench\". We'll select the TensorFlow image with Recommended Version (selected by default), choose a Deployment size of Medium, Accelerator as None (no GPU is needed for this setup) and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Tip</p> <p>The dashboard currently enforces a minimum storage volume size of 20GB. Please ensure that you modify this based on your need in Cluster Storage.</p> <p>Here, you will use Environment Variables to define the key-value pairs required for connecting to the Qdrant vector database, specifically:</p> <ul> <li> <p><code>QDRANT_COLLECTION</code>: The name of the collection in Qdrant.</p> </li> <li> <p><code>QDRANT_API_KEY</code>: The authentication key for accessing the Qdrant database.</p> <p>To retrieve the value of <code>QDRANT__SERVICE__API_KEY</code> from the <code>qdrant-key</code> Secret using the <code>oc</code> command, run:</p> <pre><code>oc get secret qdrant-key -o jsonpath='{.data.QDRANT__SERVICE__API_KEY}' | base64 --decode\n</code></pre> </li> </ul> <p>Make sure these variables are properly set to establish a secure connection.</p> </li> <li> <p>If this procedure is successful, you have started your RAG Workbench. When your     workbench is ready and the status changes to Running, click the open icon     () next to your workbench's name, or click     the workbench name directly to access your environment:</p> <p></p> </li> <li> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when     prompted, as shown below:</p> <p></p> <p>Next, you should see the NERC RHOAI JupyterLab Web Interface, as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p> </li> </ol>"},{"location":"openshift-ai/other-projects/RAG-talk-with-your-pdf/#importing-the-tutorial-files-into-the-jupyter-environment","title":"Importing the tutorial files into the Jupyter environment","text":"<p>Bring the content of this tutorial inside your Jupyter environment:</p> <p>On the toolbar, click the Git Clone icon:</p> <p></p> <p>Enter the following Git Repo URL: https://github.com/nerc-project/llm-on-nerc</p> <p>Check the Include submodules option, and then click Clone.</p> <p></p> <p>In the file browser, double-click the newly-created llm-on-nerc folder.</p> <p></p> <p>Verification:</p> <p>In the file browser, you should see the notebooks that you cloned from Git. Navigate to the <code>llm-on-nerc/examples/notebooks/langchain</code> directory, where you will find the Jupyter notebook file <code>RAG_with_sources_Langchain-Ollama-Qdrant.ipynb</code>, as shown below:</p> <p></p> <p>Double-click on this file to open it.</p> <p>Very Important Note</p> <p>Update the Ollama's BASE_URL and Qdrant's QDRANT_HOST in this notebook to match your deployment settings.</p> <p>Integrating Qdrant with LangChain:</p> <p>Once you have Qdrant set up, the next step is to integrate it with LangChain. The LangChain library provides various tools to interact with vector databases, including Qdrant.</p> <p>The AI model, now enriched with additional data, including The Forgotten Lighthouse book pdf that is located at <code>llm-on-nerc/examples/notebooks/langchain/datasource/The_Forgotten_Lighthouse_Book.pdf</code>, can be queried. When asked, \"Who is the starfish, and how do you know?\", it processes the PDF and infers that Grandpa calls Sarah \"my little starfish\" in his letter. Since the model wasn't originally trained on this book, its response relies on RAG, demonstrating how AI can extract and infer new information without retraining.</p> <p>In real life, this means you can run a pre-trained AI model (e.g., the Phi-3 model in our application) on your own data without ever sending it outside your premises for training.</p> <p>The response to our query is as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/configure-jupyter-notebook-use-gpus-aiml-modeling/","title":"Configure a Jupyter Notebook to use GPUs for AI/ML modeling","text":""},{"location":"openshift-ai/other-projects/configure-jupyter-notebook-use-gpus-aiml-modeling/#configure-a-jupyter-notebook-to-use-gpus-for-aiml-modeling","title":"Configure a Jupyter notebook to use GPUs for AI/ML modeling","text":"<p>Prerequisites:</p> <p>Prepare your Jupyter notebook server for using a GPU, you need to have:</p> <ul> <li> <p>Before proceeding, confirm that you have an active GPU quota that has been approved     for your current NERC OpenShift Allocation through NERC ColdFront. Read more     about How to Access GPU Resources     on NERC OpenShift Allocation.</p> </li> <li> <p>Select the correct data science project and create workbench, see     Populate the data science project     for more information.</p> </li> </ul> <p>Please ensure that you start your Jupyter notebook server with options as depicted in the following configuration screen. This screen provides you with the opportunity to select a notebook image and configure its options, including the Accelerator and Number of accelerators (GPUs).</p> <p></p> <p>For our example project, let's name it \"PyTorch Workbench\". We'll select the PyTorch image, choose a Deployment size of Small, choose Accelerator of NVIDIA V100 GPU, Number of accelerators as 1, and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Hardware Acceleration using GPU</p> <p>A GPU is highly recommended for this type of training to significantly improve performance and reduce training time.</p> <p>If this procedure is successful, you have started your Jupyter notebook server. When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment:</p> <p></p> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when prompted, as shown below:</p> <p></p> <p>Next, you should see the NERC RHOAI JupyterLab Web Interface, as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p>"},{"location":"openshift-ai/other-projects/configure-jupyter-notebook-use-gpus-aiml-modeling/#clone-a-github-repository","title":"Clone a GitHub Repository","text":"<p>You can clone a Git repository in JupyterLab through the left-hand toolbar or the Git menu option in the main menu as shown below:</p> <p></p> <p>Let's clone a repository using the left-hand toolbar. Click on the Git icon, shown in below:</p> <p></p> <p>Then click on Clone a Repository as shown below:</p> <p></p> <p>Enter the git repository URL, which points to the end-to-end ML workflows demo project i.e. https://github.com/rh-aiservices-bu/getting-started-with-gpus.</p> <p>Then click Clone button, as shown below:</p> <p></p> <p>Cloning takes a few seconds, after which you can double-click and navigate to the newly-created folder i.e. <code>getting-started-with-gpus</code> that contains your cloned Git repository.</p> <p>You will be able to find the newly-created folder named <code>getting-started-with-gpus</code> based on the Git repository name, as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/configure-jupyter-notebook-use-gpus-aiml-modeling/#exploring-the-getting-started-with-gpus-repository-contents","title":"Exploring the <code>getting-started-with-gpus</code> repository contents","text":"<p>After you've cloned your repository, the <code>getting-started-with-gpus</code> repository contents appear in a directory under the Name pane. The directory contains several notebooks as <code>.ipnyb</code> files, along with a standard license and README file as shown below:</p> <p></p> <p>Double-click the <code>torch-use-gpu.ipynb</code> file to open this notebook.</p> <p>This notebook handles the following tasks:</p> <ol> <li> <p>Importing torch libraries (utilities).</p> </li> <li> <p>Listing available GPUs.</p> </li> <li> <p>Checking that GPUs are enabled.</p> </li> <li> <p>Assigning a GPU device and retrieve the GPU name.</p> </li> <li> <p>Loading vectors, matrices, and data onto a GPU.</p> </li> <li> <p>Loading a neural network model onto a GPU.</p> </li> <li> <p>Training the neural network model.</p> </li> </ol> <p>Start by importing the various <code>torch</code> and <code>torchvision</code> utilities:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n</code></pre> <p>Once the utilities are loaded, determine how many GPUs are available:</p> <pre><code>torch.cuda.is_available() # Do we have a GPU? Should return True.\n</code></pre> <pre><code>torch.cuda.device_count()  # How many GPUs do we have access to?\n</code></pre> <p>When you have confirmed that a GPU device is available for use, assign a GPU device and retrieve the GPU name:</p> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)  # Check which device we got\n</code></pre> <pre><code>torch.cuda.get_device_name(0)\n</code></pre> <p>Once you have assigned the first GPU device to your device variable, you are ready to work with the GPU. Let's start working with the GPU by loading vectors, matrices, and data:</p> <pre><code>X_train = torch.IntTensor([0, 30, 50, 75, 70])  # Initialize a Tensor of Integers with no device specified\nprint(X_train.is_cuda, \",\", X_train.device)  # Check which device Tensor is created on\n</code></pre> <pre><code># Move the Tensor to the device we want to use\nX_train = X_train.cuda()\n# Alternative method: specify the device using the variable\n# X_train = X_train.to(device)\n# Confirm that the Tensor is on the GPU now\nprint(X_train.is_cuda, \",\", X_train.device)\n</code></pre> <pre><code># Alternative method: Initialize the Tensor directly on a specific device.\nX_test = torch.cuda.IntTensor([30, 40, 50], device=device)\nprint(X_test.is_cuda, \",\", X_test.device)\n</code></pre> <p>After you have loaded vectors, matrices, and data onto a GPU, load a neural network model:</p> <pre><code># Here is a basic fully connected neural network built in Torch.\n# If we want to load it / train it on our GPU, we must first put it on the GPU\n# Otherwise it will remain on CPU by default.\n\nbatch_size = 100\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(784, 784)\n        self.fc2 = nn.Linear(784, 10)\n\n    def forward(self, x):\n        x = x.view(batch_size, -1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.softmax(x, dim=1)\n        return output\n</code></pre> <pre><code>model = SimpleNet().to(device)  # Load the neural network model onto the GPU\n</code></pre> <p>After the model has been loaded onto the GPU, train it on a data set. For this example, we will use the FashionMNIST data set:</p> <pre><code>\"\"\"\n    Data loading, train and test set via the PyTorch dataloader.\n\"\"\"\n# Transform our data into Tensors to normalize the data\ntrain_transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\ntest_transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n        ])\n\n# Set up a training data set\ntrainset = datasets.FashionMNIST('./data', train=True, download=True,\n                  transform=train_transform)\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=False, num_workers=2)\n\n# Set up a test data set\ntestset = datasets.FashionMNIST('./data', train=False,\n                  transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                        shuffle=False, num_workers=2)\n</code></pre> <p>Once the FashionMNIST data set has been downloaded, you can take a look at the dictionary and sample its content.</p> <pre><code># A dictionary to map our class numbers to their items.\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\n\n# Plotting 9 random different items from the training data set, trainset.\nfigure = plt.figure(figsize=(8, 8))\nfor i in range(1, 3 * 3 + 1):\n    sample_idx = torch.randint(len(trainset), size=(1,)).item()\n    img, label = trainset[sample_idx]\n    figure.add_subplot(3, 3, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.view(28,28), cmap=\"gray\")\nplt.show()\n</code></pre> <p>The following figure shows a few of the data set's pictures:</p> <p></p> <p>There are ten classes of fashion items (e.g. shirt, shoes, and so on). Our goal is to identify which class each picture falls into. Now you can train the model and determine how well it classifies the items:</p> <pre><code>def train(model, device, train_loader, optimizer, epoch):\n    \"\"\"Model training function\"\"\"\n    model.train()\n    print(device)\n    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n</code></pre> <pre><code>def test(model, device, test_loader):\n    \"\"\"Model evaluating function\"\"\"\n    model.eval()\n    test_loss = 0\n    correct = 0\n    # Use the no_grad method to increase computation speed\n    # since computing the gradient is not necessary in this step.\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n</code></pre> <pre><code># number of  training 'epochs'\nEPOCHS = 5\n# our optimization strategy used in training.\noptimizer = optim.Adadelta(model.parameters(), lr=0.01)\n</code></pre> <pre><code>for epoch in range(1, EPOCHS + 1):\n        print( f\"EPOCH: {epoch}\")\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n</code></pre> <p>As the model is trained, you can follow along as its accuracy increases from 63 to 72 percent. (Your accuracies might differ, because accuracy can depend on the random initialization of weights.)</p> <p>Once the model is trained, save it locally:</p> <pre><code># Saving the model's weights!\ntorch.save(model.state_dict(), \"mnist_fashion_SimpleNet.pt\")\n</code></pre>"},{"location":"openshift-ai/other-projects/configure-jupyter-notebook-use-gpus-aiml-modeling/#load-and-test-the-pytorch-model","title":"Load and Test the PyTorch model","text":"<p>Let's now determine how our simple torch model performs using GPU resources.</p> <p>In the <code>getting-started-with-gpus</code> directory, double click on the <code>torch-test-model.ipynb</code> file (highlighted as shown below) to open the notebook.</p> <p></p> <p>After importing the <code>torch</code> and <code>torchvision</code> utilities, assign the first GPU to your device variable. Prepare to import your trained model, then place the model on your GPU and load in its trained weights:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)  # let's see what device we got\n</code></pre> <pre><code># Getting set to import our trained model.\n\n# batch size of 1 so we can look at one image at time.\nbatch_size = 1\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(784, 784)\n        self.fc2 = nn.Linear(784, 10)\n\n    def forward(self, x):\n        x = x.view(batch_size, -1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.softmax(x, dim=1)\n        return output\n</code></pre> <pre><code>model = SimpleNet().to(device)\nmodel.load_state_dict(torch.load(\"mnist_fashion_SimpleNet.pt\"))\n</code></pre> <p>You are now ready to examine some data and determine how your model performs. The sample run as shown below shows that the model predicted a \"bag\" with a confidence of about 0.9192. Despite the % in the output, 0.9192 is very good because a perfect confidence would be 1.0.</p> <p></p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/","title":"Deploying a Llama model with KServe","text":""},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#deploying-a-llama-model-with-kserve-using-red-hat-openshift-ai","title":"Deploying a Llama model with KServe using Red Hat OpenShift AI","text":"<p>In this walkthrough, we will explore and demonstrate how to deploy a Llama language model using the intuitive interface of Red Hat OpenShift AI (RHOAI) and NERC's powerful infrastructure features, including GPU acceleration, automatic resource scaling, and support for distributed computing.</p> <p>Prerequisites:</p> <ul> <li> <p>You have enabled the Single-model Serving platform. For more information     about enabling the single-model serving platform, see     Setting up the Single-model Server platform.</p> </li> <li> <p>Before proceeding, confirm that you have an active GPU quota that has been approved     for your current NERC OpenShift Allocation through NERC ColdFront. Read     more about How to Access GPU Resources     on NERC OpenShift Allocation.</p> </li> <li> <p><code>Llama-3.2-3B-Instruct-FP8</code> model: Llama-3.2-3B-Instruct-FP8 is obtained by quantizing the weights of the Llama-3.2-3B-Instruct model to the FP8 data type. This optimization reduces the number of bits used to represent weights and activations from 16 to 8, lowering GPU memory requirements by approximately 50% and increasing matrix-multiply throughput by about 2\u00d7. Weight quantization also reduces disk storage requirements by roughly 50%.</p> <p>For our Llama model demonstration, we are using a publicly available container image from the Quay.io registry. Specifically, we will deploy the Llama 3.2 model with 3 billion parameters, fine-tuned for instruction-following and optimized with 8-bit floating-point precision to minimize memory usage.</p> </li> <li> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI to enable <code>oc</code> commands. Refer to this user guide.</p> </li> <li> <p>Helm installed locally.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#establishing-model-connections","title":"Establishing model connections","text":"<p>NERC's RHOAI provides flexible options for establishing model connections, allowing you to choose the approach that best fits your deployment environment. You can integrate with a locally hosted S3-compatible object storage system - such as MinIO - to store and access your model artifacts directly within your own project. Alternatively, you can reference an accessible Uniform Resource Identifier (URI) that points to a Model container image. Deploying models from OCI containers is referred to as using ModelCars in KServe. A curated set of prebuilt ModelCar images is available in the ModelCar Catalog registry on Quay.io, offering convenient access to a growing collection of ready-to-use models that can be deployed directly. This approach enables seamless retrieval of fully packaged model files without relying on locally managed storage. Together, these options give you the flexibility to manage model assets in a way that aligns with your operational, security, and performance requirements.</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#set-up-local-s3-compatible-object-storage-minio","title":"Set up local S3 compatible object storage (MinIO)","text":"<ol> <li> <p>Navigating to the OpenShift AI dashboard.</p> <p>Please follow these steps to access the NERC OpenShift AI dashboard.</p> </li> <li> <p>Using a script to set up local S3 storage (MinIO) on your Data Science Project     in the NERC RHOAI as described here.</p> <p>Verification:</p> <ul> <li> <p>This action automatically creates a Connection based on your local S3-compatible     object storage (MinIO), which will then appear under the Connections     tab.</p> <p>Navigate to the Connections tab. You should see one connection listed: My Storage as shown below:</p> <p></p> </li> <li> <p>Click on the newly created connection from the list and then click the action     menu (\u22ee) at the end of the selected connection row. Choose \"Edit\" from the     dropdown menu. This will open a pop-up window as shown below:</p> <p></p> </li> <li> <p>Note both <code>Access key</code> (by clicking eye icon near the end of the textbox)     and <code>Secret key</code>.</p> </li> <li> <p>Once successfully initiated, click on the minio deployment and select     the \"Resources\" tab to review created Pods, Services, and Routes.</p> <p></p> <p>The <code>minio-s3</code> route URL (found under \"Routes\" -&gt; <code>minio-s3</code> -&gt; Location path) is used to interact with the MinIO API programmatically and will serve as the <code>S3_ENDPOINT</code>. Make sure to note this S3_ENDPOINT, as it will be required when uploading the model to the S3 (MinIO) bucket using a Python script.</p> <p>The <code>minio-console</code> route URL, also under \"Routes\" -&gt; <code>minio-console</code> -&gt; Location, opens the MinIO web console when clicked as shown below:</p> <p></p> <p>MinIO Web Console Login Credential</p> <p>The Username and Password for the MinIO Web Console can be obtained from the Connection's Access key and Secret key that you noted earlier. Use the Access key as the Username and the Secret key as the Password when signing in.</p> </li> </ul> </li> <li> <p>Once you log in to the MinIO Web Console, the Object Browser will open.     From here, verify that the bucket my-storage is visible, as shown below:</p> <p></p> </li> </ol>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#downloading-model","title":"Downloading Model","text":"<p>While you're not strictly required to use object storage to serve models in OpenShift AI, doing so simplifies things in terms of scalability and flexibility. It also provides the advantage of keeping a static local copy within the cluster after a lengthy download, so you don't need to repeatedly fetch the model from the internet whenever you restart it.</p> <p>To download a model from Hugging Face Hub:</p> <ol> <li> <p>Navigate to https://huggingface.co/.</p> </li> <li> <p>Search for the model you'd like to deploy.</p> <p>For this example, we'll use the <code>Llama-3.2-3B-Instruct-FP8</code> model, available here: https://huggingface.co/RedHatAI/Llama-3.2-3B-Instruct-FP8/tree/main.</p> <p>Very Important</p> <p>Even though this example uses the <code>Llama-3.2-3B-Instruct-FP8</code> LLM, the same mechanism can be applied to any other LLMs as well. Explore the Red Hat AI validated models collections on Hugging Face Hub.</p> </li> <li> <p>First you need to generate an access token:</p> <ul> <li> <p>Go to https://huggingface.co/settings/tokens.</p> </li> <li> <p>Click on the \"Create new token\" button.</p> </li> <li> <p>Create a \"Read\" access token by selecting Read for Token type     and then give it a Token name.</p> </li> <li> <p>Copy the generated Access Token i.e. <code>Access_Token</code>.</p> </li> </ul> </li> </ol> <p>Now that you have an Access Token, you can download the model using that token by either using Git or using the Hugging Face Hub CLI as described below:</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#using-git-with-access-token","title":"Using Git with Access Token","text":"<pre><code>git clone https://&lt;your-username&gt;:&lt;Access_Token&gt;@huggingface.co/&lt;model_repo_path&gt;\n</code></pre> <p>For example, this looks like as shown below:</p> <pre><code>git clone https://&lt;your-username&gt;:&lt;Access_Token&gt;@huggingface.co/RedHatAI/Llama-3.2-3B-Instruct-FP8\n</code></pre>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#using-the-hugging-face-hub-cli","title":"Using the Hugging Face Hub CLI","text":"<p>Use our one-liner installers to set up the <code>hf</code> CLI without touching your Python environment:</p> <p>On macOS and Linux:</p> <pre><code>curl -LsSf https://hf.co/cli/install.sh | bash\n</code></pre> <p>On Windows:</p> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://hf.co/cli/install.ps1 | iex\"\n</code></pre> <p>Login with your token:</p> <pre><code>hf auth login\n</code></pre> <p>This command will prompt you for a token. Copy-paste yours Access Token and press Enter. Then, you'll be asked if the token should also be saved as a git credential. Press Enter again (default to yes) if you plan to use git locally. Finally, it will call the Hub to check that your token is valid and save it locally.</p> <pre><code>_|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n_|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n_|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n_|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n_|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\nTo log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nEnter your token (input will not be visible):\nAdd token as git credential? (Y/n)\nToken is valid (permission: write).\nYour token has been saved in your configured git credential helpers (store).\nYour token has been saved to /home/&lt;your_username&gt;/.cache/huggingface/token\nLogin successful\n</code></pre> <p>Then download a model using <code>hf download</code> command from the Hub directly.</p> <pre><code>hf download RedHatAI/Llama-3.2-3B-Instruct-FP8\n</code></pre> <p>More about the Hugging Face Hub CLI</p> <p>This guide highlights the key capabilities of the <code>hf</code> CLI. For a full list of commands, options, and advanced usage, refer to the complete CLI reference.</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#uploading-the-model-to-the-s3-storage-minio","title":"Uploading the Model to the S3 storage (MinIO)","text":"<ul> <li> <p>Select existing bucket named \"my-storage\".</p> </li> <li> <p>Click on the \"Create new path\" button and provide a new path i.e. \"models\" as     shown below:</p> <p></p> </li> <li> <p>Upload the downloaded <code>Llama-3.2-3B-Instruct-FP8</code> model to the the bucket path     i.e. \"my-storage/models\" by selecting Upload -&gt; Upload Folder, and     select the folder where the model was downloaded from huggingface.</p> <p></p> </li> <li> <p>Wait for the upload to finish, this will take a while.</p> <p></p> <p>Uploading the Model to the S3 storage (MinIO) Bucket using Python Script</p> <p>You can easily upload your locally downloaded model files to your local S3-compatible object storage (MinIO) bucket using Python. Configure the Python script below with your previously noted S3_ENDPOINT, Access Key, and Secret Key, then point it to your model folder downloaded from the Hugging Face Hub. The following Python script will automatically upload all files while preserving the folder structure. Save it locally as <code>&lt;script_name&gt;.py</code> and run it using:</p> <pre><code>python &lt;script_name&gt;.py\n</code></pre> Python Script to Upload Model to the S3 storage (MinIO) Bucket <pre><code>import os\nimport boto3\nimport botocore\n\n# ----------------- Configuration -----------------\nACCESS_KEY = '&lt;ACCESS_KEY&gt;'       # Replace with your **Access Key**\nSECRET_KEY = '&lt;SECRET_KEY&gt;'       # Replace with your **Secret Key**\nS3_ENDPOINT = '&lt;S3_ENDPOINT&gt;'     # Replace with your **S3_ENDPOINT**\nREGION_NAME = 'us-east-1'\nBUCKET_NAME = 'my-storage'  # Replace with your existing bucket where the model will be stored\n\nif not all([ACCESS_KEY, SECRET_KEY, S3_ENDPOINT, REGION_NAME, BUCKET_NAME]):\n    raise ValueError(\n        \"One or more S3 connection variables are empty. \"\n        \"Please check your S3 configuration.\"\n    )\n\n# ----------------- Initialize S3 -----------------\nsession = boto3.session.Session(\n    aws_access_key_id=ACCESS_KEY,\n    aws_secret_access_key=SECRET_KEY\n)\n\ns3_resource = session.resource(\n    's3',\n    endpoint_url=S3_ENDPOINT,\n    region_name=REGION_NAME,\n    config=botocore.client.Config(signature_version='s3v4')\n)\n\nbucket = s3_resource.Bucket(BUCKET_NAME)\n\n# ----------------- Helper Functions -----------------\ndef ensure_s3_prefix_exists(bucket, prefix: str):\n    \"\"\"\n    Ensure an S3 prefix exists by creating a zero-byte object\n    if no objects already exist under the prefix.\n    \"\"\"\n    prefix = prefix.rstrip(\"/\") + \"/\"\n    objs = list(bucket.objects.filter(Prefix=prefix))\n    if not objs:\n        bucket.put_object(Key=prefix)\n        print(f\"Created S3 prefix: {prefix}\")\n    else:\n        print(f\"S3 prefix already exists: {prefix}\")\n\n\ndef upload_file_to_s3(local_file: str, s3_prefix: str):\n    \"\"\"\n    Upload a single file to S3 under the specified prefix.\n    \"\"\"\n    filename = os.path.basename(local_file)\n    s3_key = os.path.join(s3_prefix, filename).replace(\"\\\\\", \"/\")\n    print(f\"Uploading {local_file} -&gt; {s3_key}\")\n    bucket.upload_file(local_file, s3_key)\n    return 1\n\n\ndef upload_directory_to_s3(local_directory: str, s3_prefix: str):\n    \"\"\"\n    Upload all files from a local directory to S3 under the given prefix.\n    Preserves folder structure.\n    \"\"\"\n    num_files = 0\n    for root, _, files in os.walk(local_directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            relative_path = os.path.relpath(file_path, local_directory)\n            s3_key = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n            print(f\"Uploading {file_path} -&gt; {s3_key}\")\n            bucket.upload_file(file_path, s3_key)\n            num_files += 1\n    return num_files\n\n\ndef list_objects(prefix: str):\n    \"\"\"List all objects under the given S3 prefix.\"\"\"\n    for obj in bucket.objects.filter(Prefix=prefix):\n        print(obj.key)\n\n\n# ----------------- Main Logic -----------------\nLOCAL_MODELS_DIR = \"Llama-3.2-3B-Instruct-FP8\"      # Replace with your source local model directory\nS3_MODELS_PREFIX = \"models/Llama-3.2-3B-Instruct-FP8\"  # Replace with your destination model directory on S3\n\nif not os.path.isdir(LOCAL_MODELS_DIR):\n    raise ValueError(\n        f\"The directory '{LOCAL_MODELS_DIR}' does not exist. \"\n        \"Did you finish downloading or training the model?\"\n    )\n\n# Ensure S3 \"directory\" exists\nensure_s3_prefix_exists(bucket, S3_MODELS_PREFIX)\n\n# Upload all model files\nnum_files_uploaded = upload_directory_to_s3(LOCAL_MODELS_DIR, S3_MODELS_PREFIX)\n\nif num_files_uploaded == 0:\n    raise ValueError(\n        \"No files were uploaded. Did you finish saving the model to the directory?\"\n    )\n\nprint(f\"Successfully uploaded {num_files_uploaded} files to S3.\")\n</code></pre> </li> </ul>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#set-up-uri","title":"Set up URI","text":"<p>Alternatively, for the <code>Llama-3.2-3B-Instruct-FP8</code> model, you can use a publicly available container image from the Quay.io registry: quay.io/jharmison/models:redhatai--llama-3_2-3b-instruct-fp8-modelcar.</p> <p>Create a Connection to a ModelCar container image, which is an OCI-compliant container that packages a machine learning model along with its runtime environment and dependencies for consistent deployment.</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#adding-a-connection-based-on-uri","title":"Adding a Connection based on URI","text":"<p>In your OpenShift AI project, navigate to the Connections tab. and click the \"Create Connection\" and then choose the URI connection type as shown below:</p> <p></p> <p>To create this connection in your project, enter the following URI:</p> <pre><code>oci://quay.io/jharmison/models:redhatai--llama-3_2-3b-instruct-fp8-modelcar\n</code></pre> <p>and use <code>Llama 3.2 3B Modelcar</code> as the connection name, as shown below:</p> <p></p> <p>Important Note: ModelCar Requirements &amp; Guidance</p> <p>You have several options for deploying models to your OpenShift AI cluster. We recommend using ModelCar because it removes the need to manually download models from Hugging Face Hub, upload them to S3, or manage access permissions. With ModelCar, you can package models as OCI images and pull them at runtime or precache them. This simplifies versioning, improves traceability, and integrates cleanly into CI/CD workflows. ModelCar images also ensure reproducibility and maintain versioned model releases.</p> <p>You can deploy our own model using a ModelCar container, which packages all model files into an OCI container image. To learn more about ModelCar containers, read this article Build and deploy a ModelCar container in OpenShift AI. It explains the benefits of ModelCar containers, how to build a ModelCar image, and how to deploy it with OpenShift AI.</p> <p>For additional patterns and prebuilt ModelCar images, explore the Red Hat AI Services ModelCar Catalog repository on GitHub. Prebuilt images from this catalog are also available in the ModelCar Catalog registry on Quay.io.</p> <p>Use Any Other Available Model from the ModelCar Catalog registry.</p> <p>You can use any model from the ModelCar Catalog registry in a similar way. For example, for the <code>Granite-3.3-8B-Instruct</code> model, you can use the publicly available container image from the Quay.io registry: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct.</p> <p>The Granite-3.3-8B-Instruct model is an 8-billion-parameter, 128K context-length language model fine-tuned for improved reasoning and instruction-following capabilities. It is built on top of the <code>Granite-3.3-8B-Base</code> model.</p> <p>To create a connection for the <code>Granite-3.3-8B-Instruct</code> model, use the following URI:</p> <pre><code>oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct\n</code></pre> <p>In the Additional serving runtime arguments field under Configuration parameters section, specify the following recommended arguments:</p> <pre><code>--dtype=half\n--max-model-len=20000\n--gpu-memory-utilization=0.95\n--enable-chunked-prefill\n--enable-auto-tool-choice\n--tool-call-parser=llama3_json\n--chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja\n</code></pre> <p>However, note that all these images are compiled for the x86 architecture. If you're targeting ARM, you'll need to rebuild these images on an ARM machine, as demonstrated in this guide.</p> <p>Additionally, you may find it helpful to read Optimize and deploy LLMs for production with OpenShift AI.</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#setting-up-single-model-server-and-deploy-the-model","title":"Setting up Single-model Server and Deploy the model","text":"<ol> <li> <p>In the left menu, click Data science projects.</p> <p>The Data science projects page opens.</p> </li> <li> <p>Click the name of the project that you want to deploy a model in.</p> <p>A project details page opens.</p> </li> <li> <p>Click the Models tab.</p> </li> <li> <p>Click the Deploy model button.</p> <p></p> </li> <li> <p>The Deploy model dialog opens.</p> <p>Enter the following information for your new model:</p> <ul> <li> <p>Model deployment name: Enter a unique name for the model that you are     deploying (e.g., \"mini-llama-demo\").</p> </li> <li> <p>Serving runtime: Select vLLM NVIDIA GPU ServingRuntime for KServe     runtime.</p> </li> <li> <p>Model framework (name - version): This is pre-selected as <code>vLLM</code>.</p> </li> <li> <p>Deployment mode: From the Deployment mode list, select Advanced     option - uses Knative Serverless.</p> </li> <li> <p>Number of model server replicas to deploy has Minimum replicas:     <code>1</code> and Maximum replicas:<code>1</code>.</p> </li> <li> <p>Model server size: This is the amount of resources, CPU, and RAM that     will be allocated to your server. Here, you can select <code>Medium</code> size.</p> </li> <li> <p>Accelerator: Select <code>NVIDIA A100 GPU</code>.</p> </li> <li> <p>Number of accelerators: <code>1</code>.</p> </li> <li> <p>Model route: Select the checkbox for \"Make deployed models available     through an external route\" this will enable us to send requests to the model     endpoint from outside the cluster.</p> </li> <li> <p>Token authentication: Select the checkbox for \"Require token authentication\"     if you want to secure or restrict access to the model by forcing requests     to provide an authorization token, which is important for security. While     selecting it, you can keep the populated Service account name i.e. <code>default-name</code>.</p> </li> <li> <p>Source model location:</p> <p>If you set up the Connection using a locally hosted S3-compatible object storage system (MinIO) - then configure the following:</p> <p>i.  Select the Connection option from the dropdown list that you     created as described here     to store the model by using the Existing connection option Connection     dropdown list i.e. <code>My Storage</code>.</p> <p>ii. Path: If your model is not located at the root of the bucket of     your connection, you must enter the path to the folder it is in i.e.     <code>models/Llama-3.2-3B-Instruct-FP8</code>.</p> <ul> <li>Configuration parameters: You can customize the runtime parameters     in the Additional serving runtime arguments field. You don't     need to add any arguments here.</li> </ul> <p>Alternatively, if you set up the Connection using the URI, then select the Connection option from the dropdown list that you created as described here to store the model by using the Existing connection option Connection dropdown list i.e. <code>Llama 3.2 3B Modelcar</code>.</p> <ul> <li> <p>Configuration parameters: In the Additional serving runtime arguments     field, specify the following recommended arguments:</p> <pre><code>--dtype=half\n--max-model-len=20000\n--gpu-memory-utilization=0.95\n--enable-chunked-prefill\n--enable-auto-tool-choice\n--tool-call-parser=llama3_json\n--chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja\n</code></pre> </li> </ul> <p>Creating a New Connection</p> <p>Alternatively, you can create a new connection directly from this menu by selecting Create connection option.</p> </li> </ul> </li> </ol> <p>For our example, set the Model deployment name to <code>mini-llama-demo</code>, and select Serving runtime as <code>vLLM NVIDIA GPU ServingRuntime for KServe</code>. Also, ensure that the Deployment mode is set to <code>Advanced</code> - uses Knative Serverless.</p> <p></p> <p>Please leave the other fields at their default settings. For example, the Number of model server replicas to deploy has Minimum replicas set to <code>1</code> and Maximum replicas set to <code>1</code>, and the Model server size is set to <code>Medium</code>.</p> <p>Choose <code>NVIDIA A100 GPU</code> as the Accelerator, with the Number of accelerators set to <code>1</code>.</p> <p>How to Use the NVIDIA V100 GPU Accelerator to Reduce Costs?</p> <p>You can use the NVIDIA V100 GPU to reduce costs when deploying your model. To do this, make sure you select the Serving Runtime as <code>(V100 Support) vLLM NVIDIA GPU ServingRuntime for KServe</code>, which is customized to support the NVIDIA V100 GPU architecture. Then, choose NVIDIA A100 GPU as the Accelerator and set the Number of accelerators to <code>1</code>.</p> <p>At this point, ensure that both Make deployed models available through an external route and Require token authentication are checked. Please leave the populated Service account name i.e. <code>default-name</code> as it is.</p> <p>If you set up the Connection using a locally hosted S3-compatible object storage system (MinIO), select <code>My Storage</code> as the Connection from Existing connections. For the model Path location, enter <code>models/Llama-3.2-3B-Instruct-FP8</code> as the folder path, as shown below:</p> <p></p> <p>Alternatively, if you set up the Connection using the URI, select <code>Llama 3.2 3B Modelcar</code> as the Connection from Existing connections. In the Additional serving runtime arguments field under Configuration parameters section, specify the following recommended arguments:</p> <pre><code>--dtype=half\n--max-model-len=20000\n--gpu-memory-utilization=0.95\n--enable-chunked-prefill\n--enable-auto-tool-choice\n--tool-call-parser=llama3_json\n--chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja\n</code></pre> <p>Ensure it appears as follows:</p> <p></p> <p>When you are ready to deploy your model, select the Deploy button.</p> <p>Confirm that the deployed model appears on the Models tab for your project. After some time, once the model has finished deploying, the model deployments page of the dashboard will display a green checkmark in the Status column, indicating that the deployment is complete.</p> <p>To view details for the deployed model, click the dropdown arrow icon to the left of your deployed model name (e.g., <code>mini-llama-demo</code>), as shown below:</p> <p></p> <p>You can also modify the configure properties for your deployed model configuration by clicking on the three dots on the right side, and selecting Edit. This will bring back the same configuration pop-up window we used earlier. This menu also has the option for you to Delete the deployed model.</p> <p>Intelligent Auto-Scaling and Scale-to-Zero for Significant Cost Savings</p> <p>Once you have deployed your model and obtained the inference endpoints, you can modify the model configuration to set the Minimum replicas to 0, then redeploy it as shown below:</p> <p></p> <p>This enables intelligent auto-scaling of your model's compute resources (CPU, GPU, RAM, etc.), allowing replicas to scale up during high traffic and scale down when idle. With <code>scale-to-zero</code> enabled, the system reduces pods to zero during inactivity, eliminating idle compute costs - especially beneficial for GPU workloads. The model then scales back up instantly as soon as a new request arrives.</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#check-the-model-api","title":"Check the Model API","text":"<p>The deployed model is now accessible through the API endpoint of the model server. The information about the endpoint is different, depending on how you configured the model server.</p> <p>As in this example, you have exposed the model externally through a route, click on the \"Internal and external endpoint details\" link in the Inference endpoint section. A popup will display the address for the url and the External (can be accessed from inside or outside the cluster) for the inference endpoints as shown below:</p> <p></p> <p>Notes:</p> <ul> <li> <p>The internal URL displayed is only the base address of the endpoint of the     following format: <code>https://name-of-your-model.name-of-your-project-namespace.svc.cluster.local</code>     that is accessible only within your cluster locally e.g. <code>https://mini-llama-demo.name-of-your-project-namespace.svc.cluster.local</code>.</p> </li> <li> <p>The External Inference endpoint displays the full URL of the following format:     <code>https://name-of-your-model-name-of-your-project.apps.shift.nerc.mghpcc.org</code>     that you can be easily accessed from outside the cluster e.g. <code>https://mini-llama-demo-name-of-your-project.apps.shift.nerc.mghpcc.org</code>.</p> </li> <li> <p>Get the Authorization Token for your deployed model by clicking on dropdown     arrow icon to the left of your deployed model name i.e. \"mini-llama-demo\".     Your Authorization Token is located at the \"Token authentication\" section under     \"Token secret\", you can just copy the token i.e. <code>YOUR_BEARER_TOKEN</code> directly     from the UI.</p> </li> </ul> <p>Now that you have the URL and Authorization Token, you can try querying the model endpoint. We will try multiple queries.</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#testing-your-deployment","title":"Testing your deployment","text":""},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#internal-testing","title":"Internal testing","text":"<p>Once deployed, navigate to Workloads &gt; Pods in the left-hand menu, then locate and click on the pod that corresponds to the model deployment name, as shown below:</p> <p></p> <p>Access the pod's terminal by clicking the Terminal tab, then run a curl command to test internal communication.</p> <p>The vLLM runtime uses OpenAI's API format, making integration straightforward. You can learn more in the OpenAI documentation.</p> <p>The following is an example command you can use to test the connection:</p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#v1models","title":"/v1/models","text":"<p>Let's start with the simplest query, the <code>/v1/models</code> API endpoint. This endpoint just returns information about the models being served, I use it to simply see if the model can accept a request and return with some information:</p> <pre><code>curl -k -X GET http://localhost:8080/v1/models \\\n    -H \"Content-Type: application/json\"\n</code></pre> <p>If your command output is successful, it should output something like this:</p> <p></p> <p>Output:</p> <pre><code>{\"object\":\"list\",\"data\":[{\"id\":\"mini-llama-demo\",\"object\":\"model\",\"created\":1765026654,\"owned_by\":\"vllm\",\"root\":\"/mnt/models\",\"parent\":null,\"max_model_len\":131072,\"permission\":[{\"id\":\"modelperm-34037d02d5734fbf9f00d607f0f82ad5\",\"object\":\"model_permission\",\"created\":1765026654,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}\n</code></pre>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#v1completions","title":"v1/completions","text":"<p>Now that we know that works, let's test whether the <code>/v1/completions</code> API endpoint works. This endpoint takes a text prompt and returns a completed text response.</p> <pre><code>curl -k -X POST http://localhost:8080/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"mini-llama-demo\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 7,\n        \"temperature\": 0.7\n    }'\n</code></pre> <p>Changing API Query Parameters</p> <p>You can change the <code>temperature</code> of the query. The temperature essentially controls the \"randomness\" of the model's response. The lower the temperature the more deterministic the reponse, the higher the temperature the more random/unpredictible the response. So if you set the temperature to 0, it would always return the same output since there would be no randomness.</p> <p>Running this command should return an output similar to the following:</p> <pre><code>{\"id\":\"cmpl-f266de8ae5f64e14a2f3fa53560e4fbf\",\"object\":\"text_completion\",\"created\":1765026778,\"model\":\"mini-llama-demo\",\"choices\":[{\"index\":0,\"text\":\" city that has a unique blend of\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":5,\"total_tokens\":12,\"completion_tokens\":7,\"prompt_tokens_details\":null},\"kv_transfer_params\":null}\n</code></pre>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#v1chatcompletions","title":"v1/chat/completions","text":"<p>Let's test whether the <code>v1/chat/completions</code> API endpoint works. This endpoint takes a text prompt and returns a completed text response.</p> <pre><code>curl -k -X POST http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n      {\"role\": \"user\", \"content\": \"Hello\"},\n      {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"What is 2 plus 2?\"}\n    ]\n  }'\n</code></pre> <p>Running this command should return an output similar to the following:</p> <pre><code>{\"id\":\"chatcmpl-578a75d6950c4a46b277ed345b7c8ae5\",\"object\":\"chat.completion\",\"created\":1765026799,\"model\":\"mini-llama-demo\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"2 + 2 = 4\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":67,\"total_tokens\":75,\"completion_tokens\":8,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"kv_transfer_params\":null}\n</code></pre>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#testing-external-access","title":"Testing external access","text":"<p>For external testing, use the token and external endpoint in your curl command.</p> <p>The following are some example commands you can use to test the connection:</p> <pre><code>curl -k -X GET https://&lt;external-url&gt;/v1/models \\\n    -H \"Content-Type: application/json\" -H \"Authorization: Bearer YOUR_BEARER_TOKEN\"\n</code></pre> <pre><code>curl -k -X POST https://&lt;external-url&gt;/v1/completions \\\n    -H \"Content-Type: application/json\" -H \"Authorization: Bearer YOUR_BEARER_TOKEN\" \\\n    -d '{\n        \"model\": \"mini-llama-demo\",\n        \"prompt\": \"San Francisco is a\",\n        \"max_tokens\": 7,\n        \"temperature\": 0.7\n    }'\n</code></pre> <pre><code>curl -k -X POST https://&lt;external-url&gt;/v1/chat/completions \\\n  -H \"Content-Type: application/json\" -H \"Authorization: Bearer YOUR_BEARER_TOKEN\" \\\n  -d '{\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n      {\"role\": \"user\", \"content\": \"Hello\"},\n      {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"What is 2 plus 2?\"}\n    ]\n  }'\n</code></pre> <p>Output:</p> <p></p>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#web-interface-integration-using-open-webui","title":"Web interface integration using Open WebUI","text":"<p>For a more user-friendly experience, integrate with Open WebUI as follows:</p> <ol> <li> <p>Clone or navigate to this repository.</p> <p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/llm-on-nerc.git\ncd llm-on-nerc/llm-clients/openwebui/charts/openwebui\n</code></pre> </li> <li> <p>Prepare <code>values.yaml</code> to connect the Open WebUI to the Deployed vLLM Model.</p> <p>Edit the <code>values.yaml</code> file and locate the following entries.</p> <pre><code>vllmEndpoint: http://vllm.example.svc:8000/v1\nvllmModel: granite-3.3-2b-instruct\nvllmToken: \"\"\n</code></pre> <p>Update them to specify your running external endpoint, vLLM model, and token:</p> <p>For e.g.:</p> <pre><code>vllmEndpoint: https://mini-llama-demo-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org/v1\nvllmModel: mini-llama-demo\nvllmToken: \"&lt;YOUR_BEARER_TOKEN&gt;\"\n</code></pre> </li> <li> <p>Install Helm chart.</p> <p>Deploy Open WebUI using Helm with your configuration:</p> <pre><code>helm install openwebui ./ -f values.yaml\n</code></pre> <p>Output:</p> <pre><code>NAME: openwebui\nLAST DEPLOYED: Tue Dec  2 22:52:06 2025\nNAMESPACE: &lt;your-namespace&gt;\nSTATUS: deployed\nREVISION: 1\nDESCRIPTION: Install complete\nTEST SUITE: None\nNOTES:\n1. Get the Open WebUI URL by running these commands:\nroute_hostname=$(kubectl get --namespace &lt;your-namespace&gt; route openwebui -o jsonpath='{.status.ingress[0].host}')\necho https://${route_hostname}\n</code></pre> </li> <li> <p>Access Open WebUI and Test vLLM integration.</p> <p>Ensure the clean web UI is connected to your vLLM endpoint by sending a simple prompt and verifying the response as shown below:</p> <p></p> </li> </ol> <p>To Remove the Open WebUI Helm Chart</p> <p>Run the following command to cleanly uninstall and delete the Open WebUI Helm release:</p> <pre><code>helm uninstall openwebui\n</code></pre>"},{"location":"openshift-ai/other-projects/deploying-a-llama-model-with-kserve/#summary","title":"Summary","text":"<ul> <li> <p>Deploying validated models from the     Red Hat AI Hugging Face Validated Models repository     in disconnected OpenShift AI environments can be done by selecting the     S3-compatible object storage option when creating a Connection.</p> <p>You can integrate with a locally hosted S3-compatible object storage system - such as MinIO - to store and access your model artifacts directly within your project. This approach typically involves the following steps:</p> <p>i. Set up local S3 storage (MinIO) and create a connection pointing to the bucket.</p> <p>ii. Select the desired model.</p> <p>iii. Download the model and upload it to the locally configured S3 storage bucket.</p> </li> <li> <p>Alternatively, you can reference a publicly available Uniform Resource Identifier     (URI) by selecting the URI option during Connection creation. This     allows you to point directly to a ModelCar container image available in     the ModelCar Catalog registry     on Quay.io.</p> </li> <li> <p>Identify the required serving runtime.</p> </li> <li> <p>Configure a single-model server and deploy the model using the connection.</p> </li> <li> <p>Verify and test the model's API inference endpoints.</p> </li> <li> <p>Web interface integration of the deployed model using Open WebUI.</p> </li> </ul> <p>This process ensures that AI workloads run seamlessly in restricted or disconnected environments, allowing you to securely leverage validated and optimized AI models.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/","title":"Credit Card Fraud Detection Application","text":""},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#credit-card-fraud-detection-application","title":"Credit Card Fraud Detection Application","text":"<p>Here you will use an example fraud detection model to complete the following tasks:</p> <ul> <li> <p>Train your own fraud detection model.</p> </li> <li> <p>Explore a pre-trained fraud detection model by using a Jupyter notebook.</p> </li> <li> <p>Deploy the model by using OpenShift AI model serving.</p> </li> <li> <p>Refine and train the model by using automated pipelines.</p> </li> <li> <p>Deploy the \"Credit Card Fraud Detection\" application on NERC OpenShift, which connects to the deployed model.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#about-the-example-fraud-detection-model","title":"About the example fraud detection model","text":"<p>The example fraud detection model monitors credit card transactions for potential fraudulent activity by analyzing the following details:</p> <ul> <li> <p>Distance from the cardholder's home and previous transaction.</p> </li> <li> <p>Purchase amount, compared to the user's median transaction price.</p> </li> <li> <p>Retailer history, indicating whether the merchant has been used before.</p> </li> <li> <p>Authentication method, such as PIN usage.</p> </li> <li> <p>Transaction type, distinguishing between online and in-person purchases.</p> </li> </ul> <p>Based on the above mentioned factors, the model determines the likelihood of a transaction being fraudulent.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#navigating-to-the-openshift-ai-dashboard","title":"Navigating to the OpenShift AI dashboard","text":"<p>Please follow these steps to access the NERC OpenShift AI dashboard.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#setting-up-your-data-science-project-in-the-nerc-rhoai","title":"Setting up your Data Science Project in the NERC RHOAI","text":"<p>Prerequisites:</p> <ul> <li>Before proceeding, confirm that you have an active GPU quota that has been approved     for your current NERC OpenShift Allocation through NERC ColdFront. Read     more about How to Access GPU Resources     on NERC OpenShift Allocation.</li> </ul>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#1-storing-data-with-connections","title":"1. Storing data with connections","text":"<p>For this tutorial, you will need two S3-compatible object storage buckets, such as NERC OpenStack Container (Ceph), MinIO, or AWS S3. You can either use your own storage buckets or run a provided script that automatically creates the following local S3 storage (MinIO) buckets:</p> <ul> <li> <p>my-storage \u2013 Use this bucket to store your models and data. You can reuse this bucket and its connection for notebooks and model servers.</p> </li> <li> <p>pipeline-artifacts \u2013 Use this bucket to store pipeline artifacts. A pipeline     artifacts bucket is required when setting up a pipeline server. For clarity,     this tutorial keeps it separate from the first storage bucket, as it is considered     best practice to use dedicated storage buckets for different purposes.</p> </li> </ul> <p>You must also create a connection to each storage bucket. For this tutorial, you have two options depending on whether you want to use your own storage buckets or a script to create local S3 storage (MinIO) buckets:</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#11-using-your-own-s3-compatible-storage-buckets","title":"1.1. Using your own S3-compatible storage buckets","text":"<p>Procedure:</p> <p>Manually create two connections: My Storage and Pipeline Artifacts by following How to create a connection.</p> <p>Verification:</p> <p>You should see two connections listed under your RHOAI Dashboard My Storage and Pipeline Artifacts as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#12-using-a-script-to-set-up-local-s3-storage-minio","title":"1.2. Using a script to set up local S3 storage (MinIO)","text":"<p>Alternatively, if you want to run a script that automates the setup by completing the following tasks:</p> <ul> <li> <p>Deploys a MinIO instance in your project namespace.</p> </li> <li> <p>Creates two storage buckets within the MinIO instance.</p> </li> <li> <p>Generates a random user ID and password for the MinIO Console.</p> </li> <li> <p>Establishes two connections in your project - one for each bucket -     using the same generated credentials.</p> </li> <li> <p>Installs all required network policies.</p> </li> </ul> <p>Procedure:</p> <p>i. From the OpenShift AI dashboard, you can return to OpenShift Web Console by using the application launcher icon (the black-and-white icon that looks like a grid), and choosing the \"OpenShift Console\" as shown below:</p> <p></p> <p>ii. From your NERC's OpenShift Web Console, navigate to your project corresponding to the NERC RHOAI Data Science Project and select the \"Import YAML\" button, represented by the \"+\" icon in the top navigation bar as shown below:</p> <p></p> <p>iii. Verify that you selected the correct project.</p> <p></p> <p>iv. Copy the following code and paste it into the Import YAML editor.</p> Local S3 storage (MinIO) Creation YAML Script <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: minio-setup\n  labels:\n    app: minio\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: minio-setup-edit\n  labels:\n    app: minio\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\nsubjects:\n- kind: ServiceAccount\n  name: minio-setup\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-pvc\n  labels:\n    app: minio\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi # Adjust the size according to your needs\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - minio server /data --console-address :9090\n        command:\n        - /bin/bash\n        - -c\n        envFrom:\n        - secretRef:\n            name: minio-root-user\n        image: quay.io/minio/minio:latest\n        name: minio\n        ports:\n        - containerPort: 9000\n          name: api\n          protocol: TCP\n        - containerPort: 9090\n          name: console\n          protocol: TCP\n        resources:\n          limits:\n            cpu: \"2\"\n            memory: 2Gi\n          requests:\n            cpu: 200m\n            memory: 1Gi\n        volumeMounts:\n        - mountPath: /data\n          name: minio-volume\n      volumes:\n      - name: minio-volume\n        persistentVolumeClaim:\n          claimName: minio-pvc\n      - emptyDir: {}\n        name: empty\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-ds-connections\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector: {}\n  template:\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          echo -n 'Waiting for minio route'\n          while ! oc get route minio-s3 2&gt;/dev/null | grep -qF minio-s3; do\n            echo -n .\n            sleep 5\n          done; echo\n\n          echo -n 'Waiting for minio root user secret'\n          while ! oc get secret minio-root-user 2&gt;/dev/null | grep -qF minio-root-user; do\n            echo -n .\n            sleep 5\n          done; echo\n\n          MINIO_ROOT_USER=$(oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_USER}}')\n          MINIO_ROOT_PASSWORD=$(oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_PASSWORD}}')\n          MINIO_HOST=https://$(oc get route minio-s3 -o template --template '{{.spec.host}}')\n\n          cat &lt;&lt; EOF | oc apply -f-\n          apiVersion: v1\n          kind: Secret\n          metadata:\n            annotations:\n              opendatahub.io/connection-type: s3\n              openshift.io/display-name: My Storage\n            labels:\n              opendatahub.io/dashboard: \"true\"\n              opendatahub.io/managed: \"true\"\n            name: aws-connection-my-storage\n          data:\n            AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}\n            AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}\n          stringData:\n            AWS_DEFAULT_REGION: us-east-1\n            AWS_S3_BUCKET: my-storage\n            AWS_S3_ENDPOINT: ${MINIO_HOST}\n          type: Opaque\n          EOF\n          cat &lt;&lt; EOF | oc apply -f-\n          apiVersion: v1\n          kind: Secret\n          metadata:\n            annotations:\n              opendatahub.io/connection-type: s3\n              openshift.io/display-name: Pipeline Artifacts\n            labels:\n              opendatahub.io/dashboard: \"true\"\n              opendatahub.io/managed: \"true\"\n            name: aws-connection-pipeline-artifacts\n          data:\n            AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}\n            AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}\n          stringData:\n            AWS_DEFAULT_REGION: us-east-1\n            AWS_S3_BUCKET: pipeline-artifacts\n            AWS_S3_ENDPOINT: ${MINIO_HOST}\n          type: Opaque\n          EOF\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: create-ds-connections\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-minio-buckets\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector: {}\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          oc get secret minio-root-user\n          env | grep MINIO\n          cat &lt;&lt; 'EOF' | python3\n          import boto3, os\n\n          s3 = boto3.client(\"s3\",\n                            endpoint_url=\"http://minio-service:9000\",\n                            aws_access_key_id=os.getenv(\"MINIO_ROOT_USER\"),\n                            aws_secret_access_key=os.getenv(\"MINIO_ROOT_PASSWORD\"))\n          bucket = 'pipeline-artifacts'\n          print('creating pipeline-artifacts bucket')\n          if bucket not in [bu[\"Name\"] for bu in s3.list_buckets()[\"Buckets\"]]:\n            s3.create_bucket(Bucket=bucket)\n          bucket = 'my-storage'\n          print('creating my-storage bucket')\n          if bucket not in [bu[\"Name\"] for bu in s3.list_buckets()[\"Buckets\"]]:\n            s3.create_bucket(Bucket=bucket)\n          EOF\n        command:\n        - /bin/bash\n        envFrom:\n        - secretRef:\n            name: minio-root-user\n        image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-generic-data-science-notebook:2023.2\n        imagePullPolicy: IfNotPresent\n        name: create-buckets\n      initContainers:\n      - args:\n        - -ec\n        - |-\n          echo -n 'Waiting for minio root user secret'\n          while ! oc get secret minio-root-user 2&gt;/dev/null | grep -qF minio-root-user; do\n          echo -n .\n          sleep 5\n          done; echo\n\n          echo -n 'Waiting for minio deployment'\n          while ! oc get deployment minio-deployment 2&gt;/dev/null | grep -qF minio-deployment; do\n            echo -n .\n            sleep 5\n          done; echo\n          oc wait --for=condition=available --timeout=60s deployment/minio-deployment\n          sleep 10\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: wait-for-minio\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-minio-root-user\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          if [ -n \"$(oc get secret minio-root-user -oname 2&gt;/dev/null)\" ]; then\n            echo \"Secret already exists. Skipping.\" &gt;&amp;2\n            exit 0\n          fi\n          genpass() {\n              &lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c\"${1:-32}\"\n          }\n          id=$(genpass 16)\n          secret=$(genpass)\n          cat &lt;&lt; EOF | oc apply -f-\n          apiVersion: v1\n          kind: Secret\n          metadata:\n            name: minio-root-user\n          type: Opaque\n          stringData:\n            MINIO_ROOT_USER: ${id}\n            MINIO_ROOT_PASSWORD: ${secret}\n          EOF\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: create-minio-root-user\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\n  labels:\n    app: minio\nspec:\n  ports:\n  - name: api\n    port: 9000\n    targetPort: api\n  - name: console\n    port: 9090\n    targetPort: 9090\n  selector:\n    app: minio\n  sessionAffinity: None\n  type: ClusterIP\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: minio-console\n  labels:\n    app: minio\nspec:\n  port:\n    targetPort: console\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  wildcardPolicy: None\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: minio-s3\n  labels:\n    app: minio\nspec:\n  port:\n    targetPort: api\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  wildcardPolicy: None\n</code></pre> <p>Very Important Note</p> <p>In this YAML file, the size of the storage is set as 10Gi. Change it if you need to.</p> <p>v. Click Create.</p> <p>Verification:</p> <p>i. Once Resource is successfully created, you will see a \"Resources successfully created\" message and the following resources listed:</p> <p></p> <p>ii. Once the deployment is successful, you will be able to see all resources are created and grouped under \"minio\" application grouping on the Workloads -&gt; Topology menu, as shown below:</p> <p></p> <p>Once successfully initiated, click on the minio deployment and select the \"Resources\" tab to review created Pods, Services, and Routes.</p> <p></p> <p>Please note the minio-console route URL by navigating to the \"Routes\" section under the Location path. When you click on the minio-console route URL, it will open the MinIO web console that looks like below:</p> <p></p> <p>MinIO Web Console Login Credential</p> <p>The Username and Password for the MinIO web console can be retrieved from the Connection's Access key and Secret key.</p> <p>iii. Navigate back to the OpenShift AI dashboard.</p> <p>a. Select Data Science Projects and then click the name of your project, i.e. Fraud detection.</p> <p>b. Click Connections. You should see two connections listed: My Storage and Pipeline Artifacts as shown below:</p> <p></p> <p>c. Verify the buckets are created on the MinIO Web Console:</p> <ul> <li> <p>Click on any connection from the list that was created and then click the action menu (\u22ee) at the end of the selected connection row. Choose \"Edit\" from the dropdown menu. This will open a pop-up  window as shown below:</p> <p></p> </li> <li> <p>Note both Access key (by clicking eye icon near the end of the textbox) and Secret key.</p> <p>Alternatively, Run <code>oc</code> commands to get Access key and Secret key</p> <p>Alternatively, you can run the following <code>oc</code> commands:</p> <p>i. To get Access key run:</p> <p><code>oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_USER}}' | base64 --decode</code></p> <p>ii. And to get Secret key run:</p> <p><code>oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_PASSWORD}}' | base64 --decode</code></p> </li> <li> <p>Return to the MinIO Web Console using the provided URL. Enter the Access Key as the Username and the Secret Key as the Password. This will open the Object Browser, where you should verify that both buckets: my-storage and pipeline-artifacts are visible as shown below:</p> <p></p> </li> </ul> <p>Alternatively, Running a script to install local MinIO object storage</p> <p>Alternatively, you can run a script to install local object storage buckets and create connections using the OpenShift CLI (<code>oc</code>). For that, you need to install and configure the OpenShift CLI by following the setup instructions. Once the OpenShift CLI is set up, execute the following command to install MinIO object storage along with local S3 storage (MinIO) buckets and necessary connections:</p> <p><code>oc apply -f https://raw.githubusercontent.com/nerc-project/fraud-detection/main/setup/setup-s3.yaml</code></p> <p>Clean Up</p> <p>To delete all resources if not necessary just run <code>oc delete -f https://raw.githubusercontent.com/nerc-project/fraud-detection/main/setup/setup-s3.yaml</code> or <code>oc delete all,sa,rolebindings,pvc,job -l app=minio</code>.</p> <p>Important Note</p> <p>The script is based on a guide for deploying MinIO. The MinIO-based Object Storage that the script creates is not meant for production usage.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#2-enabling-data-science-pipelines","title":"2. Enabling data science pipelines","text":"<p>In this section, you prepare your workbench environment so that you can use data science pipelines.</p> <p>Procedure:</p> <p>i. From the OpenShift AI dashboard, click the Pipelines tab.</p> <p>ii. Click Configure pipeline server.</p> <p></p> <p>iii. In the Configure pipeline server form, in the Access key field next to the key icon, click the dropdown menu and then click Pipeline Artifacts to populate the Configure pipeline server form with credentials for the connection.</p> <p></p> <p>iv. Leave the database configuration as the default.</p> <p>v. Click Configure pipeline server.</p> <p>vi. Wait until the loading spinner disappears and Start by importing a pipeline is displayed.</p> <p>Important Note</p> <p>You must wait until the pipeline configuration is complete before you continue and create your workbench. If you continue and create your workbench before the pipeline server is ready, your workbench will not be able to submit pipelines to it.</p> <p>If you have waited more than 5 minutes, and the pipeline server configuration does not complete, you can delete the pipeline server and create it again.</p> <p></p> <p>Verification:</p> <p>a. Navigate to the Pipelines tab for the project.</p> <p>b. Next to Import pipeline, click the action menu (\u22ee) and then select View pipeline server configuration.</p> <p></p> <p>An information box opens and displays the object storage connection information for the pipeline server. as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#3-creating-a-workbench-and-a-notebook","title":"3. Creating a workbench and a notebook","text":""},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#creating-a-workbench-and-selecting-a-notebook-image","title":"Creating a workbench and selecting a notebook image","text":"<p>Procedure:</p> <p>Prepare your Jupyter notebook server for using a GPU, you need to have:</p> <ul> <li>Select the correct data science project and create workbench, see     Populate the data science project     for more information.</li> </ul> <p>Please ensure that you start your Jupyter notebook server with options as depicted in the following configuration screen. This screen provides you with the opportunity to select a notebook image and configure its options, including the Accelerator and Number of accelerators (GPUs).</p> <p>Click Attach existing connections under the Connections section, and attach the \"My Storage\" connection that was set up previously to the workbench:</p> <p></p> <p>Search and add \"My Storage\":</p> <p></p> <p>Click on \"Attach\" button:</p> <p></p> <p>The final workbench setup, before clicking the Create workbench button, should look like this:</p> <p></p> <p>For our example project, let's name it \"Fraud detection\". We'll select the TensorFlow image with Recommended Version (selected by default), choose a Deployment size of Small, choose Accelerator of NVIDIA V100 GPU, Number of accelerators as 1, and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Running Workbench without GPU</p> <p>If you do not require GPU resources for your task, you can leave the Accelerator field set to its default None selection.</p> <p></p> <p>Verification:</p> <p>If this procedure is successful, you have started your Jupyter notebook server. When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment:</p> <p></p> <p>Note</p> <p>If you made a mistake, you can edit the workbench to make changes. Please make sure you set the Running status of your workbench to Stopped prior clicking the action menu (\u22ee) at the end of the selected workbench row as shown below:</p> <p></p> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when prompted, as shown below:</p> <p></p> <p>Next, you should see the NERC RHOAI JupyterLab Web Interface, as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#importing-the-tutorial-files-into-the-jupyter-environment","title":"Importing the tutorial files into the Jupyter environment","text":"<p>Bring the content of this tutorial inside your Jupyter environment:</p> <p>On the toolbar, click the Git Clone icon:</p> <p></p> <p>Enter the following Git Repo URL: https://github.com/nerc-project/fraud-detection</p> <p>Check the Include submodules option, and then click Clone.</p> <p></p> <p>In the file browser, double-click the newly-created fraud-detection folder.</p> <p></p> <p>Verification:</p> <p>In the file browser, you should see the notebooks that you cloned from Git.</p> <p></p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#training-a-model","title":"Training a model","text":"<p>In your Jupyter notebook environment, within the root folder path of <code>fraud-detection</code>, find the Jupyter notebook file <code>1_experiment_train.ipynb</code> that demonstrates how to train a model within the NERC RHOAI. Please follow the instructions directly in the notebook environment. The instructions guide you through some simple data exploration, experimentation, and model training tasks. To run it you need to double click it and execute the \"Run\" button to run all notebook cells at once.</p> <p>Very Important: Monitoring Your Model Training Process</p> <p>In the <code>1_experiment_train.ipynb</code> Jupyter notebook file, find the \"Monitor the Training\" section where you can use ClearML to monitor your model training process.</p> <p>Please register your account at https://app.clear.ml/login. After successfully registering your account, get Application API Credentials at: https://app.clear.ml/settings/workspace-configuration.</p> <p>Please update <code>CLEARML_API_ACCESS_KEY</code> and <code>CLEARML_API_SECRET_KEY</code> with your own ClearML Application API credentials, then uncomment the following code in the specified cell:</p> <p></p> <p>Verification:</p> <p>This process will take some time to complete. At the end, it will generate and save the model <code>model.onnx</code> within the <code>models/fraud/1/</code> folder path of <code>fraud-detection</code>.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#adding-mlflow-to-training-code","title":"Adding MLflow to Training Code","text":"<p>What is MLflow?</p> <p>MLflow is an open-source platform that helps manage the entire machine learning lifecycle, including experimentation, reproducibility, deployment, and model management. For more information about how to set up your own MLflow server, read this user guide.</p> <p>In your Jupyter notebook environment, within the root folder path of <code>fraud-detection</code>, find the <code>mlflow</code> directory. It contains the following files:</p> <ul> <li> <p><code>model.ipynb</code>: This Jupyter notebook contains the end-to-end machine learning     workflow, including data preprocessing, model training, and MLflow logging.</p> </li> <li> <p><code>requirements.txt</code>: This file lists all the Python package dependencies needed     to run the code in <code>model.ipynb</code>.</p> </li> </ul> <p>Go back to your \"Fraud detection\" workbench and add a new  Environment Variable to specify the Key/Value pairs related to the your MLflow server by clicking the action menu (\u22ee) at the end of your workbench.</p> <p>To add Environment variable please follow the following steps:</p> <p>i. Click on \"Add another key / value pair\" under existing \"Environment variables\" section:</p> <p>ii. Choose \"Key / Value\" and enter the following keys along with their corresponding values, which you have retrieved while \"Editing connection\":</p> <p>Environment Variables:</p> <pre><code>Key: MLFLOW_ROUTE\nValue: https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org\n</code></pre> <p></p> <p>Once you execute the provided <code>model.ipynb</code>, the training process logs model parameters, metrics, and artifacts to the MLflow tracking server. After training and tuning hyperparameters, the final model artifact is logged to the tracking server to record a link between the model, the input data it was trained on, and the code used to generate it.</p> <p></p> <p>You'll be able to track the entire training lifecycle by navigating to your MLflow tracking server at: <code>https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#4-preparing-a-model-for-deployment","title":"4. Preparing a model for deployment","text":"<p>Procedure:</p> <p>In your JupyterLab environment, open the notebook file <code>2_save_model.ipynb</code>. Follow the instructions in the notebook to store the model and save it in the portable ONNX format.</p> <p>Verification:</p> <p>After completing the notebook steps, verify that the <code>models/fraud/1/model.onnx</code> file is stored in the object storage corresponding to the My Storage data connection. Once saved, the model is now ready for use by your model server.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#5-deploying-and-testing-a-model","title":"5. Deploying and testing a model","text":""},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#deploying-a-model","title":"Deploying a model","text":"<p>Now that the model is stored and saved in the portable ONNX format, you can deploy it as an API using an OpenShift AI Model Server.</p> <p>OpenShift AI offers two options for model serving:</p> <ul> <li> <p>Single-model serving:</p> <p>Each model in the project is deployed on its own model server. This platform works well for large models or models that require dedicated resources.</p> </li> <li> <p>Multi-model serving:</p> <p>All models in the project are deployed on the same model server. This platform is suitable for sharing resources among deployed models.</p> </li> </ul> <p>Learn More about Model Serving</p> <p>To learn more about how Model Serving works in the NERC RHOAI environment, read this page.</p> <p>In this project, you are deploying only one model, so you can select either serving type. The steps for deploying the fraud detection model depend on the type of model serving platform you select:</p> <ul> <li> <p>Deploying a model on a single-model server</p> </li> <li> <p>Deploying a model on a multi-model server</p> </li> </ul>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#deploying-a-model-on-a-single-model-server","title":"Deploying a model on a single-model server","text":"<p>OpenShift AI single-model servers host only one model at a time. To deploy a model, you first create a new model server and then deploy your model to it.</p> <p>Procedure:</p> <p>In the OpenShift AI dashboard, navigate to the data science project details page and click the Models tab.</p> <p>In the \"Single-model serving platform\" tile, click Select single-model.</p> <p>You will be able to deploy the model by clicking the Deploy model button, as shown below:</p> <p></p> <p>In the pop-up window that appears, depicted as shown below, you can specify the following details:</p> <p>For Model deployment name, type fraud.</p> <p>For Serving runtime, select OpenVINO Model Server.</p> <p>For Model framework (name - version), select onnx-1.</p> <p>For Deployment mode, select Advanced (selected by default) - uses Knative Serverless.</p> <p>For Existing connection, select My Storage.</p> <p>Type the path that leads to the version folder that contains your model file: models/fraud</p> <p>Leave the other fields with the default settings.</p> <p></p> <p>Click Deploy.</p> <p>Verification:</p> <p>When you return to the Deployed models page, you will see your newly deployed model. Notice the loading symbol under the Status section. When the model has finished deploying, the status icon will be a green checkmark indicating the model deployment is complete as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#deploying-a-model-on-a-multi-model-server","title":"Deploying a model on a multi-model server","text":"<p>NERC RHOAI multi-model servers can host multiple models simultaneously. You can create a new model server and deploy your model to it.</p> <p>Procedure:</p> <p>In the OpenShift AI dashboard, navigate to the data science project details page and click the Models tab.</p> <p>In the Multi-model serving platform tile, click Select multi-model.</p> <p>You will be able to create a new model server by clicking the Add model server button, as shown below:</p> <p></p> <p>In the pop-up window that appears, depicted as shown below, you can specify the following details:</p> <p>For Model server name, type a name, for example Model Server.</p> <p>For Serving runtime, select OpenVINO Model Server.</p> <p>Leave the other fields with the default settings.</p> <p></p> <p>Click Add.</p> <p>In the Models list, next to the new model server, click Deploy model.</p> <p></p> <p>In the form, provide the following values:</p> <p>For Model deployment name, type fraud.</p> <p>For Model framework (name - version), select onnx-1.</p> <p>For Existing connection, select My Storage.</p> <p>Type the path that leads to the version folder that contains your model file: models/fraud</p> <p>Leave the other fields with the default settings.</p> <p></p> <p>Click Deploy.</p> <p>Verification:</p> <p>When you return to the Deployed models page, you will see your newly deployed model. You should click on the 1 on the Deployed models tab to see details. Notice the loading symbol under the Status section. When the model has finished deploying, the status icon will be a green checkmark indicating the model deployment is complete as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#testing-the-model-api","title":"Testing the model API","text":"<p>Now that you've deployed the model, you can test its API endpoints.</p> <p>Procedure:</p> <ul> <li> <p>In the OpenShift AI dashboard, navigate to the project details page and click the Models tab.</p> </li> <li> <p>Take note of the model's Inference endpoint URL. You need this information when you test the model API.</p> <p> </p> <p>The Inference endpoint field contains an Internal Service link, click the link to open a popup that shows the URL details, and then take note of the grpcUrl and restUrl values.</p> <p>Single-model Serving Inference Endpoints</p> <p>In the case of single-model serving, the Inference endpoint field contains an Internal Service link. Click the link to open a popup that displays the URL details. Take note of the url value if you want to access the inference endpoint from within the cluster. If you are accessing the model from outside the cluster, be sure to also note the External value.</p> <p></p> </li> <li> <p>Return to the Jupyter notebook environment and test your new inference endpoints.</p> <p>If you deployed your model with the multi-model serving platform, follow the instructions in <code>3_rest_requests_multi_model.ipynb</code> to make a REST API call. Be sure to update the rest_url with your own restUrl value (as noted above). To make a gRPC API call, follow the instructions in <code>4_grpc_requests_multi_model.ipynb</code>, updating the grpc_host with your own grpcUrl value (as noted above).</p> <p>If you deployed your model with the single-model serving platform, follow the directions in <code>5_rest_requests_single_model.ipynb</code> to try a REST API call. Be sure to update the rest_url with your own url or External value (as noted above).</p> </li> </ul>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#6-implementing-pipelines","title":"6. Implementing pipelines","text":"<p>Important Note</p> <p>If you create your workbench before the pipeline server is ready, it won't be able to submit pipelines. If the pipeline server was configured after your workbench was created, you'll need to stop and then restart your workbench. Wait until the workbench status shows as Running.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#automating-workflows-with-data-science-pipelines","title":"Automating workflows with data science pipelines","text":"<p>In previous sections of this tutorial, you used a notebook to train and save your model. Alternatively, you can automate these tasks using Red Hat OpenShift AI pipelines.</p> <p>Pipelines allow you to automate the execution of multiple notebooks and Python scripts. By using pipelines, you can run long training jobs or schedule model retraining without manually executing notebooks.</p> <p>In this section, you will create a simple pipeline using the GUI pipeline editor.</p> <p>This pipeline will:</p> <ul> <li> <p>Use the same notebook from the previous sections to train the model.</p> </li> <li> <p>Save the trained model to S3 storage bucket.</p> </li> </ul> <p>Your completed pipeline should resemble the one in the <code>6 Train Save.pipeline</code> file.</p> <p>Note</p> <p>To explore the pipeline editor, follow the steps in the next procedure to create your own pipeline. Alternatively, you can skip the procedure and instead feel free to run and use the provided <code>6 Train Save.pipeline</code> file.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#create-a-pipeline","title":"Create a pipeline","text":"<p>i. Open your workbench's JupyterLab environment. If the launcher is not visible, click + to open it.</p> <p></p> <p>ii. Click Pipeline Editor.</p> <p></p> <p>You've created a blank pipeline.</p> <p>iii. Set the default runtime image for when you run your notebook or Python code.</p> <p>a. In the pipeline editor, click Open Panel.</p> <p></p> <p>b. Select the Pipeline Properties tab.</p> <p></p> <p>c. In the Pipeline Properties panel, scroll down to Generic Node Defaults and Runtime Image. Set the value to <code>TensorFlow with CUDA and Python 3.11 (UBI9)</code>.</p> <p></p> <p>iv. Save the pipeline.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#add-nodes-to-your-pipeline","title":"Add nodes to your pipeline","text":"<p>Add some steps, or nodes in your pipeline. Your two nodes will use the <code>1_experiment_train.ipynb</code> and <code>2_save_model.ipynb</code> notebooks.</p> <p>i. From the file-browser panel, drag the <code>1_experiment_train.ipynb</code> and <code>2_save_model.ipynb</code> notebooks onto the pipeline canvas.</p> <p></p> <p>ii. Click the output port of <code>1_experiment_train.ipynb</code> and drag a connecting line to the input port of <code>2_save_model.ipynb</code>.</p> <p></p> <p>iii. Save the pipeline.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#specify-the-training-file-as-a-dependency","title":"Specify the training file as a dependency","text":"<p>Set node properties to specify the training file as a dependency.</p> <p>Very Important</p> <p>If you don't set this file dependency, the file won't be included in the node when it runs and the training job would fail.</p> <p>i. Click the <code>1_experiment_train.ipynb</code> node.</p> <p></p> <p>ii. In the Properties panel, click the Node Properties tab.</p> <p>iii. Scroll down to the File Dependencies section and then click Add.</p> <p></p> <p>iv. Set the value to <code>data/*.csv</code> which contains the data to train your model.</p> <p>v. Select the Include Subdirectories option.</p> <p></p> <p>vi. Save the pipeline.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#create-and-store-the-onnx-formatted-output-file","title":"Create and store the ONNX-formatted output file","text":"<p>In node 1, the notebook creates the <code>models/fraud/1/model.onnx</code> file. In node 2, the notebook uploads that file to the S3 storage bucket. You must set <code>models/fraud/1/model.onnx</code> file as the output file for both nodes.</p> <p>i. Select node 1.</p> <p>ii. Select the Node Properties tab.</p> <p>iii. Scroll down to the Output Files section, and then click Add.</p> <p>iv. Set the value to <code>models/fraud/1/model.onnx</code>.</p> <p></p> <p>v. Repeat steps ii-iv for node 2.</p> <p>vi. Save the pipeline.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#configure-the-connection-to-the-s3-storage-bucket","title":"Configure the connection to the S3 storage bucket","text":"<p>In node 2, the notebook uploads the model to the S3 storage bucket.</p> <p>You must set the S3 storage bucket keys by using the secret created by the My Storage connection that you set up in the Storing data with connections section of this tutorial.</p> <p>You can use this secret in your pipeline nodes without having to save the information in your pipeline code. This is important, for example, if you want to save your pipelines - without any secret keys - to source control.</p> <p>In this example, the secret is named <code>aws-connection-my-storage</code>.</p> <p>How to get the secret name?</p> <p>If you named your connection something other than \"My Storage\", you can obtain the secret name in the OpenShift AI dashboard by hovering over the help (?) icon in the Connections tab.</p> <p></p> <p>Very Important: Make sure to replace all instances of <code>aws-connection-my-storage</code> secret with your own.</p> <p>The <code>aws-connection-my-storage</code> secret includes the following fields:</p> <pre><code>AWS_ACCESS_KEY_ID\n\nAWS_DEFAULT_REGION\n\nAWS_S3_BUCKET\n\nAWS_S3_ENDPOINT\n\nAWS_SECRET_ACCESS_KEY\n</code></pre> <p>You must set the secret name and key for each of these fields.</p> <p>Procedure:</p> <p>i. Remove any pre-filled environment variables.</p> <p>a. Select node 2, and then select the Node Properties tab.</p> <p>Under Additional Properties, note that some environment variables have been pre-filled. The pipeline editor inferred that you need them from the notebook code.</p> <p>Since you don't want to save the value in your pipelines, remove all of these environment variables.</p> <p>b. Click Remove for each of the pre-filled environment variables.</p> <p></p> <p>ii. Add the S3 bucket and keys by using the Kubernetes secret.</p> <p>a. Under Kubernetes Secrets, click Add.</p> <p></p> <p>b. Enter the following values and then click Add.</p> <p>\ud83d\udd38  Environment Variable: AWS_ACCESS_KEY_ID</p> <ul> <li> <p>Secret Name: aws-connection-my-storage</p> </li> <li> <p>Secret Key: AWS_ACCESS_KEY_ID</p> </li> </ul> <p></p> <p>iii. Repeat Step ii for each of the following Kubernetes secrets:</p> <p>\ud83d\udd38  Environment Variable: AWS_SECRET_ACCESS_KEY</p> <ul> <li> <p>Secret Name: aws-connection-my-storage</p> </li> <li> <p>Secret Key: AWS_SECRET_ACCESS_KEY</p> </li> </ul> <p>\ud83d\udd38  Environment Variable: AWS_S3_ENDPOINT</p> <ul> <li> <p>Secret Name: aws-connection-my-storage</p> </li> <li> <p>Secret Key: AWS_S3_ENDPOINT</p> </li> </ul> <p>\ud83d\udd38  Environment Variable: AWS_DEFAULT_REGION</p> <ul> <li> <p>Secret Name: aws-connection-my-storage</p> </li> <li> <p>Secret Key: AWS_DEFAULT_REGION</p> </li> </ul> <p>\ud83d\udd38  Environment Variable: AWS_S3_BUCKET</p> <ul> <li> <p>Secret Name: aws-connection-my-storage</p> </li> <li> <p>Secret Key: AWS_S3_BUCKET</p> </li> </ul> <p>iv.Select File -&gt; Save Pipeline As to save and rename the .pipeline file. For example, rename it to My Train Save.pipeline.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#run-the-pipeline","title":"Run the Pipeline","text":"<p>You can use your own newly created pipeline or the pipeline in the provided <code>6 Train Save.pipeline</code> file.</p> <p>Procedure:</p> <p>i. Click the play button in the toolbar of the pipeline editor.</p> <p></p> <p>ii. In the next popup:</p> <p></p> <ul> <li> <p>Enter a name for your pipeline i.e. <code>My Train Save</code>.</p> </li> <li> <p>Verify that the Runtime Configuration: is set to Data Science Pipeline.</p> </li> <li> <p>Click OK.</p> </li> </ul> <p>Troubleshooting Help</p> <p>If you see an error message stating that \"no runtime configuration for Data Science Pipeline is defined\", you might have created your workbench before the pipeline server was available. If you configured the pipeline server after you created your workbench, you need to stop the workbench and then started your workbench.</p> <p>iii. In the OpenShift AI dashboard, open your data science project and expand the newly created pipeline.</p> <p></p> <p>iv. Click View runs.</p> <p></p> <p>v. Click your run and then view the pipeline run in progress.</p> <p></p> <p>The result should be a <code>models/fraud/1/model.onnx</code> file in your S3 bucket which you can serve, just like you did manually in the Preparing a model for deployment section.</p>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#running-a-data-science-pipeline-generated-from-python-code","title":"Running a data science pipeline generated from Python code","text":"<p>In the previous section, you created a simple pipeline using the graphical pipeline editor. However, it's often preferable to define pipelines in code, allowing for version control and easier collaboration.</p> <p>The Kubeflow pipelines (kfp) SDK offers a Python API for building pipelines programmatically. You can install the SDK using the following command:</p> <pre><code>pip install kfp\n</code></pre> <p>With this package, you can write pipeline definitions in Python, compile them into YAML format, and then import the resulting YAML into OpenShift AI.</p> <p>Note</p> <p>In OpenShift AI, the current version of kfp uses Argo Workflows as its execution backend.</p> <p>The GitHub repository provides the files for you to view and upload.</p> <ol> <li> <p>Optionally, view the provided Python code in your JupyterLab environment by navigating to the <code>fraud-detection</code> project's <code>pipeline</code> directory. It contains the following files:</p> <ul> <li> <p><code>7_get_data_train_upload-tekton.yaml</code> is unused older version of pipeline YAML file using the OpenShift Pipeline (Tekton).</p> </li> <li> <p><code>7_get_data_train_upload.py</code> is the main pipeline code.</p> </li> <li> <p><code>build.sh</code> is a script that builds the pipeline and creates the YAML file.</p> </li> </ul> <p>For your convenience, the output of the <code>build.sh</code> script is available in the top-level <code>fraud-detection</code> directory. The file is named <code>7_get_data_train_upload.yaml</code>, as shown below:</p> <p></p> <p>Note</p> <p>You can also run the <code>build.sh</code> script manually from your local terminal by executing the following command from the <code>pipeline</code> directory of the <code>fraud-detection</code> project:</p> <pre><code>sh ./build.sh\n</code></pre> </li> <li> <p>Right-click the <code>7_get_data_train_upload.yaml</code> file and then click Download.</p> </li> <li> <p>Upload the <code>7_get_data_train_upload.yaml</code> file to OpenShift AI.</p> <p>i. In the OpenShift AI dashboard, navigate to your data science project page. Click the Pipelines tab and then click Import pipeline.</p> <p>ii. Enter values for Pipeline name and Pipeline description.</p> <p>iii. Click Upload and then select <code>7_get_data_train_upload.yaml</code> from your local files to upload the pipeline.</p> <p></p> <p>iv. Click Import pipeline to import and save the pipeline.</p> <p>The pipeline shows in graphic view as shown below:</p> <p></p> </li> <li> <p>Select Actions &gt;&gt; Create run.</p> </li> <li> <p>On the Create run page, provide the following values:</p> <p>i. For Experiment, leave the value as <code>Default</code>.</p> <p>ii. For Name, type any name, for example <code>Run 1</code>.</p> <p>iii. For Pipeline, select the pipeline that you uploaded.</p> <p>You can leave the other fields with their default values.</p> <p></p> </li> <li> <p>Click Create run to create the run.</p> <p>A new run starts immediately.</p> <p></p> </li> </ol>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#schedule-execution","title":"Schedule execution","text":"<p>We can also schedule an execution so that the confidence check is executed at regular intervals.</p> <p>To do that:</p> <ul> <li> <p>Go back to the OpenShift AI dashboard, open your data science project</p> </li> <li> <p>Find the pipeline you just ran in the Pipelines tab</p> </li> <li> <p>Click the 3 dots at the very end of the pipeline row, and click \"Create schedule\".</p> <p></p> </li> <li> <p>On the next screen:</p> <p>i. keep the Experiment to <code>Default</code>,</p> <p>ii. Set a Name,</p> <p>iii. select a <code>Periodic</code> Trigger type,</p> <p>iv. run it every Day with Maximum concurrent runs of 3.</p> <p>v. keep the <code>My Train Save</code> Pipeline and Version (The Pipeline's name     we set while running the Pipeline for the first time)</p> <p>vi. and click Create schedule:</p> <p></p> <p>vii. This will shows the Graph view of the Scheduled Pipeline Run:</p> <p></p> <p>viii. In Data Science Pipelines -&gt; Runs, click the Schedules tab to verify that the Scheduled run is visible, as shown below:</p> <p></p> <p>It will run daily 3 runs, and will inform us if anything goes wrong with your training and saving the model process.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/fraud-detection-predictive-ai-app/#deploy-the-model-application-on-nerc-openshift","title":"Deploy the Model Application on NERC OpenShift","text":"<p>The model application includes a visual user interface (UI) powered by Gradio, allowing users to interact with the trained model. This interface enables users to input transaction data and receive predictions on potential fraud, providing an intuitive way to test the model's performance. Additionally, example inputs are provided within the UI to help users understand the expected data format and interact with the model effectively.</p> <p>The model application code is located in the <code>application</code> folder within the root directory of <code>fraud-detection</code>. You can find this folder in the GitHub repository you cloned during the step: Importing the tutorial files into the Jupyter environment.</p> <p></p> <p>If you look inside it <code>model_application.py</code>, you will find two crucial lines of code for retrieving environment variables:</p> <pre><code># Get a few environment variables. These are so we:\n# - Know what endpoint we should request\n# - Set server name and port for Gradio\nMODEL_NAME = os.getenv(\"MODEL_NAME\", \"fraud\")   &lt;----------\nREST_URL = os.getenv(\"INFERENCE_ENDPOINT\")   &lt;----------\nINFER_URL = f\"{REST_URL}/v2/models/{MODEL_NAME}/infer\"\n...\n\n    response = requests.post(INFER_URL, json=payload, headers=headers)    &lt;----------\n</code></pre> <p>This is how you send a request to the NERC RHOAI Model Server with the data that you want it to process for a prediction.</p> <p>We will deploy the application on OpenShift by linking it to the GitHub repository. OpenShift will fetch the repository, build a container image using the Dockerfile, and publish it automatically.</p> <p>To accomplish this, from the OpenShift AI dashboard, navigate to the OpenShift Web Console:</p> <p></p> <p>From your NERC's OpenShift Web Console, navigate to your project corresponding to the NERC RHOAI Data Science Project and select the \"Import from Git\" button, represented by the \"+\" icon in the top navigation bar as shown below:</p> <p></p> <p>Alternatively, navigate to the Topology page under Workloads. Right-click on the page and select the \"Import from Git\" option from the Add to Project menu as shown below:</p> <p></p> <p>In the \"Git Repo URL\" enter: <code>https://github.com/nerc-project/fraud-detection</code> (this is the same repository you pulled into RHOAI earlier). Then press \"Show advanced Git options\" and set \"Context dir\" to \"/application\" where the application Dockerfile is located as shown below:</p> <p></p> <p>At the General section, select \"Create application\" option under Application and then give unique Application name i.e. <code>fraud-detection-application</code> and also Name i.e. <code>fraud-detection-application</code> as shown below:</p> <p></p> <p>Finally, at the Deploy section, press \"Show advanced Deployment option\".</p> <p>Set these values in the Environment variables (runtime only) fields:</p> <p></p> <p>Name: MODEL_NAME</p> <p>Value: From the RHOAI projects interface (from the previous section), copy the Model name value. For example: <code>fraud</code>.</p> <p>Name: INFERENCE_ENDPOINT</p> <p>Value: From the RHOAI projects interface (from the previous section), copy the restUrl value. For example: <code>http://modelmesh-serving.&lt;your-namespace&gt;:8008</code>.</p> <p></p> <p>Your full settings page should look something like this:</p> <p></p> <p>Target Port</p> <p>Under \"Advanced options,\" make sure to set the Target Port to 8080, which corresponds to the exposed port in the application's Dockerfile.</p> <p>Press Create to start deploying the application.</p> <p>You should now see a new object is added in your topology map for the application that you just added. When the circle of your deployment turns dark blue it means that it has finished deploying.</p> <p>If you want more details on how the deployment is going, you can press the circle and look at Resources in the right menu that opens up. There you can see how the build is going and what's happening to the pod.</p> <p></p> <p>The application will be ready when the build is complete and the pod is \"Running\".</p> <p>When the application has been deployed successfully, you can either open the application URL using the Open URL icon as shown below or you can naviate to the route URL by navigating to the \"Routes\" section under the Location path as shown below:</p> <p></p> <p>This will open the application interface in a new tab.</p> <p>Congratulations, you now have an application running your AI model!</p> <p>Try entering a few values and click \"Submit\" to see if it predicts it as a credit fraud or not. You can select one of the example rows at the bottom of the application page, this will auto-populate the values to the form.</p> <p></p>"},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/","title":"How to access, download, and analyze data for S3 usage","text":""},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/#how-to-access-download-and-analyze-data-for-s3-usage","title":"How to access, download, and analyze data for S3 usage","text":"<p>Prerequisites:</p> <p>Prepare your Jupyter notebook server, you need to have:</p> <ul> <li>Select the correct data science project and create workbench, see     Populate the data science project     for more information.</li> </ul> <p>Please ensure that you start your Jupyter notebook server with options as depicted in the following configuration screen. This screen provides you with the opportunity to select a notebook image and configure its options, including the Accelerator and Number of accelerators (GPUs).</p> <p></p> <p>For our example project, let's name it \"Standard Data Science Workbench\". We'll select the Standard Data Science image with Recommended Version (selected by default), choose a Deployment size of Small, Accelerator as None (no GPU is needed for this setup), and allocate a Cluster storage space of 1GB.</p> <p>If this procedure is successful, you have started your Jupyter notebook server. When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment:</p> <p></p> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when prompted, as shown below:</p> <p></p> <p>Next, you should see the NERC RHOAI JupyterLab Web Interface, as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p>"},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/#clone-a-github-repository","title":"Clone a GitHub Repository","text":"<p>You can clone a Git repository in JupyterLab through the left-hand toolbar or the Git menu option in the main menu as shown below:</p> <p></p> <p>Let's clone a repository using the left-hand toolbar. Click on the Git icon, shown in below:</p> <p></p> <p>Then click on Clone a Repository as shown below:</p> <p></p> <p>Enter the git repository URL, which points to the end-to-end ML workflows demo project i.e. https://github.com/rh-aiservices-bu/access-s3-data.</p> <p>Then click Clone button, as shown below:</p> <p></p> <p>Cloning takes a few seconds, after which you can double-click and navigate to the newly-created folder i.e. <code>access-s3-data</code> that contains your cloned Git repository.</p> <p>You will be able to find the newly-created folder named <code>access-s3-data</code> based on the Git repository name, as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/#access-and-download-s3-data","title":"Access and download S3 data","text":"<p>In the Name menu, double-click the <code>downloadData.ipynb</code> notebook in the file explorer on the left side to launch it. This action will open another tab in the content section of the environment, on the right.</p> <p>Run each cell in the notebook, using the Shift-Enter key combination, and pay attention to the execution results. Using this notebook, we will:</p> <ul> <li> <p>Make a connection to an AWS S3 storage bucket</p> </li> <li> <p>Download a CSV file into the \"datasets\" folder</p> </li> <li> <p>Rename the downloaded CSV file to \"newtruckdata.csv\"</p> </li> </ul>"},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/#view-your-new-csv-file","title":"View your new CSV file","text":"<p>Inside the \"datasets\" directory, double-click the \"newtruckdata.csv\" file. File contents should appear as shown below:</p> <p></p> <p>The file contains the data you will analyze and perform some analytics.</p>"},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/#getting-ready-to-run-analysis-on-your-new-csv-file","title":"Getting ready to run analysis on your new CSV file","text":"<p>Since you now have data, you can open the next Jupyter notebook, <code>simpleCalc.ipynb</code>, and perform the following operations:</p> <ul> <li> <p>Create a dataframe.</p> </li> <li> <p>Perform simple total and average calculations.</p> </li> <li> <p>Print the calculation results.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/how-access-s3-data-then-download-and-analyze-it/#analyzing-your-s3-data-access-run-results","title":"Analyzing your S3 data access run results","text":"<p>Double-click the <code>simpleCalc.ipynb</code> Python file. When you execute the cells in the notebook, results appear like the ones shown below:</p> <p></p> <p>The cells in the above figure show the mileage of four vehicles. In the next cell, we calculate total mileage, total rows (number of vehicles) and the average mileage for all vehicles. Execute the \"Perform Calculations\" cell to see basic calculations performed on the data as shown below:</p> <p></p> <p>Calculations show the total mileage as 742, for four vehicles, and an average mileage of 185.5.</p> <p>Success! You have added analyzed your run results using the NERC RHOAI.</p> <p>Working with data in an S3-compatible object store</p> <p>For detailed instructions on how to connect to and use data from an S3-compatible object store in your workbench, see this Red Hat documentation: Working with Data in an S3-Compatible Object Store.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/","title":"Object Detection Application Using YOLOv5 Model","text":""},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#object-detection-application-using-yolov5-model","title":"Object Detection Application Using YOLOv5 Model","text":"<p>Here you will use an example Object Detection Application using YOLOv5 model to complete the following tasks:</p> <ul> <li> <p>Data Preparation by downloading dataset images and randomly split them in train/validate/test.</p> </li> <li> <p>Train model with YOLOv5 and test it.</p> </li> <li> <p>Export YoloV5 model to ONNX format.</p> </li> <li> <p>Save the Model to the S3 Object storage.</p> </li> <li> <p>Deploy the model by using OpenShift AI model serving.</p> </li> <li> <p>Use the gRPC and REST inference endpoints to query the model.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#about-the-yolov5-model","title":"About the YOLOv5 model","text":"<p>YOLO(You Only Look Once) is one of the leading object detection algorithms, leveraging deep learning techniques to achieve high speed and accuracy. Unlike the R-CNN(Region-based Convolutional Neural Network) model, YOLO takes a different approach by dividing the input image into a grid and predicting bounding boxes within each grid cell. This method simplifies the process of determining object class probabilities.</p> <p>By using a single-stage detection model, YOLO processes the entire image in one pass, making it significantly faster than traditional models. Due to its efficiency, YOLO is well-suited for real-time applications.</p> <p>YOLOv5, which further improved the model's performance and added new features such as support for panoptic segmentation and object tracking.</p> <p>YOLO has been widely applied across various fields, including autonomous vehicles, security and surveillance, and medical imaging.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#navigating-to-the-openshift-ai-dashboard","title":"Navigating to the OpenShift AI dashboard","text":"<p>Please follow these steps to access the NERC OpenShift AI dashboard.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#setting-up-your-data-science-project-in-the-nerc-rhoai","title":"Setting up your Data Science Project in the NERC RHOAI","text":"<p>Prerequisites:</p> <ul> <li>Before proceeding, confirm that you have an active GPU quota that has been approved     for your current NERC OpenShift Allocation through NERC ColdFront. Read     more about How to Access GPU Resources     on NERC OpenShift Allocation.</li> </ul>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#1-storing-data-with-connection","title":"1. Storing data with connection","text":"<p>For this tutorial, you will need two S3-compatible object storage buckets, such as NERC OpenStack Container (Ceph), MinIO, or AWS S3. You can either use your own storage buckets or run a provided script that automatically creates the following local S3 storage (MinIO) bucket:</p> <ul> <li>my-storage \u2013 Use this bucket to store your models and data. You can reuse this bucket and its connection for notebooks and model servers.</li> </ul>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#11-using-your-own-s3-compatible-storage-buckets","title":"1.1. Using your own S3-compatible storage buckets","text":"<p>Procedure:</p> <p>Manually create a connection: My Storage by following How to create a connection.</p> <p>Verification:</p> <p>You should see a connection listed under your RHOAI Dashboard My Storage as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#12-using-a-script-to-set-up-local-s3-storage-minio","title":"1.2. Using a script to set up local S3 storage (MinIO)","text":"<p>Alternatively, if you want to run a script that automates the setup by completing the following tasks:</p> <ul> <li> <p>Deploys a MinIO instance in your project namespace.</p> </li> <li> <p>Creates one storage buckets within the MinIO instance.</p> </li> <li> <p>Generates a random user ID and password for the MinIO Console.</p> </li> <li> <p>Establishes a connection in your project - for a bucket - using     the generated credentials.</p> </li> <li> <p>Installs all required network policies.</p> </li> </ul> <p>Procedure:</p> <p>i. From the OpenShift AI dashboard, you can return to OpenShift Web Console by using the application launcher icon (the black-and-white icon that looks like a grid), and choosing the \"OpenShift Console\" as shown below:</p> <p></p> <p>ii. From your NERC's OpenShift Web Console, navigate to your project corresponding to the NERC RHOAI Data Science Project and select the \"Import YAML\" button, represented by the \"+\" icon in the top navigation bar as shown below:</p> <p></p> <p>iii. Verify that you selected the correct project.</p> <p></p> <p>iv. Copy the following code and paste it into the Import YAML editor.</p> Local S3 storage (MinIO) Creation YAML Script <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: minio-setup\n  labels:\n    app: minio\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: minio-setup-edit\n  labels:\n    app: minio\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\nsubjects:\n- kind: ServiceAccount\n  name: minio-setup\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-pvc\n  labels:\n    app: minio\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi # Adjust the size according to your needs\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-deployment\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: minio\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - minio server /data --console-address :9090\n        command:\n        - /bin/bash\n        - -c\n        envFrom:\n        - secretRef:\n            name: minio-root-user\n        image: quay.io/minio/minio:latest\n        name: minio\n        ports:\n        - containerPort: 9000\n          name: api\n          protocol: TCP\n        - containerPort: 9090\n          name: console\n          protocol: TCP\n        resources:\n          limits:\n            cpu: \"2\"\n            memory: 2Gi\n          requests:\n            cpu: 200m\n            memory: 1Gi\n        volumeMounts:\n        - mountPath: /data\n          name: minio-volume\n      volumes:\n      - name: minio-volume\n        persistentVolumeClaim:\n          claimName: minio-pvc\n      - emptyDir: {}\n        name: empty\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-ds-connections\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector: {}\n  template:\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          echo -n 'Waiting for minio route'\n          while ! oc get route minio-s3 2&gt;/dev/null | grep -qF minio-s3; do\n            echo -n .\n            sleep 5\n          done; echo\n\n          echo -n 'Waiting for minio root user secret'\n          while ! oc get secret minio-root-user 2&gt;/dev/null | grep -qF minio-root-user; do\n            echo -n .\n            sleep 5\n          done; echo\n\n          MINIO_ROOT_USER=$(oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_USER}}')\n          MINIO_ROOT_PASSWORD=$(oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_PASSWORD}}')\n          MINIO_HOST=https://$(oc get route minio-s3 -o template --template '{{.spec.host}}')\n\n          cat &lt;&lt; EOF | oc apply -f-\n          apiVersion: v1\n          kind: Secret\n          metadata:\n            annotations:\n              opendatahub.io/connection-type: s3\n              openshift.io/display-name: My Storage\n            labels:\n              opendatahub.io/dashboard: \"true\"\n              opendatahub.io/managed: \"true\"\n            name: aws-connection-my-storage\n          data:\n            AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}\n            AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}\n          stringData:\n            AWS_DEFAULT_REGION: us-east-1\n            AWS_S3_BUCKET: my-storage\n            AWS_S3_ENDPOINT: ${MINIO_HOST}\n          type: Opaque\n          EOF\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: create-ds-connections\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-minio-buckets\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector: {}\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          oc get secret minio-root-user\n          env | grep MINIO\n          cat &lt;&lt; 'EOF' | python3\n          import boto3, os\n\n          s3 = boto3.client(\"s3\",\n                            endpoint_url=\"http://minio-service:9000\",\n                            aws_access_key_id=os.getenv(\"MINIO_ROOT_USER\"),\n                            aws_secret_access_key=os.getenv(\"MINIO_ROOT_PASSWORD\"))\n          bucket = 'my-storage'\n          print('creating my-storage bucket')\n          if bucket not in [bu[\"Name\"] for bu in s3.list_buckets()[\"Buckets\"]]:\n            s3.create_bucket(Bucket=bucket)\n          EOF\n        command:\n        - /bin/bash\n        envFrom:\n        - secretRef:\n            name: minio-root-user\n        image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-generic-data-science-notebook:2023.2\n        imagePullPolicy: IfNotPresent\n        name: create-buckets\n      initContainers:\n      - args:\n        - -ec\n        - |-\n          echo -n 'Waiting for minio root user secret'\n          while ! oc get secret minio-root-user 2&gt;/dev/null | grep -qF minio-root-user; do\n          echo -n .\n          sleep 5\n          done; echo\n\n          echo -n 'Waiting for minio deployment'\n          while ! oc get deployment minio-deployment 2&gt;/dev/null | grep -qF minio-deployment; do\n            echo -n .\n            sleep 5\n          done; echo\n          oc wait --for=condition=available --timeout=60s deployment/minio-deployment\n          sleep 10\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: wait-for-minio\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: create-minio-root-user\n  labels:\n    app: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - args:\n        - -ec\n        - |-\n          if [ -n \"$(oc get secret minio-root-user -oname 2&gt;/dev/null)\" ]; then\n            echo \"Secret already exists. Skipping.\" &gt;&amp;2\n            exit 0\n          fi\n          genpass() {\n              &lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c\"${1:-32}\"\n          }\n          id=$(genpass 16)\n          secret=$(genpass)\n          cat &lt;&lt; EOF | oc apply -f-\n          apiVersion: v1\n          kind: Secret\n          metadata:\n            name: minio-root-user\n          type: Opaque\n          stringData:\n            MINIO_ROOT_USER: ${id}\n            MINIO_ROOT_PASSWORD: ${secret}\n          EOF\n        command:\n        - /bin/bash\n        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest\n        imagePullPolicy: IfNotPresent\n        name: create-minio-root-user\n      restartPolicy: Never\n      serviceAccount: minio-setup\n      serviceAccountName: minio-setup\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\n  labels:\n    app: minio\nspec:\n  ports:\n  - name: api\n    port: 9000\n    targetPort: api\n  - name: console\n    port: 9090\n    targetPort: 9090\n  selector:\n    app: minio\n  sessionAffinity: None\n  type: ClusterIP\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: minio-console\n  labels:\n    app: minio\nspec:\n  port:\n    targetPort: console\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  wildcardPolicy: None\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: minio-s3\n  labels:\n    app: minio\nspec:\n  port:\n    targetPort: api\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: minio-service\n    weight: 100\n  wildcardPolicy: None\n</code></pre> <p>Very Important Note</p> <p>In this YAML file, the size of the storage is set as 10Gi. Change it if you need to.</p> <p>v. Click Create.</p> <p>Verification:</p> <p>i. Once Resource is successfully created, you will see a \"Resources successfully created\" message and the following resources listed:</p> <p></p> <p>ii. Once the deployment is successful, you will be able to see all resources are created and grouped under \"minio\" application grouping on the Workloads -&gt; Topology menu, as shown below:</p> <p></p> <p>Once successfully initiated, click on the minio deployment and select the \"Resources\" tab to review created Pods, Services, and Routes.</p> <p></p> <p>Please note the minio-console route URL by navigating to the \"Routes\" section under the Location path. When you click on the minio-console route URL, it will open the MinIO web console that looks like below:</p> <p></p> <p>MinIO Web Console Login Credential</p> <p>The Username and Password for the MinIO web console can be retrieved from the Connection's Access key and Secret key. Enter the Access Key as the Username and the Secret Key as the Password.</p> <p>iii. Navigate back to the OpenShift AI dashboard.</p> <p>a. Select Data Science Projects and then click the name of your project, i.e. Object Detection Workbench.</p> <p>b. Click Connections. You should see one connection listed: My Storage as shown below:</p> <p></p> <p>c. Verify the bucket is created on the MinIO Web Console:</p> <ul> <li> <p>Click on the newly created connection from the list and then click the action menu (\u22ee) at the end of the selected connection row. Choose \"Edit\" from the dropdown menu. This will open a pop-up window as shown below:</p> <p></p> </li> <li> <p>Note both Access key (by clicking eye icon near the end of the textbox) and Secret key.</p> <p>Alternatively, Run <code>oc</code> commands to get Access key and Secret key</p> <p>Alternatively, you can run the following <code>oc</code> commands:</p> <p>i. To get Access key run:</p> <p><code>oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_USER}}' | base64 --decode</code></p> <p>ii. And to get Secret key run:</p> <p><code>oc get secret minio-root-user -o template --template '{{.data.MINIO_ROOT_PASSWORD}}' | base64 --decode</code></p> </li> <li> <p>Return to the MinIO Web Console using the provided URL. Enter the Access Key as the Username and the Secret Key as the Password. This will open the Object Browser, where you should verify that the bucket: my-storage is visible as shown below:</p> <p></p> </li> </ul> <p>Alternatively, Running a script to install local MinIO object storage</p> <p>Alternatively, you can run a script to install local object storage buckets and create connections using the OpenShift CLI (<code>oc</code>). For that, you need to install and configure the OpenShift CLI by following the setup instructions. Once the OpenShift CLI is set up, execute the following command to install MinIO object storage along with local S3 storage (MinIO) buckets and necessary connections:</p> <p><code>oc apply -f https://raw.githubusercontent.com/nerc-project/object-detection/main/setup/setup-s3.yaml</code></p> <p>Clean Up</p> <p>To delete all resources if not necessary just run <code>oc delete -f https://raw.githubusercontent.com/nerc-project/object-detection/main/setup/setup-s3.yaml</code> or <code>oc delete all,sa,rolebindings,pvc,job -l app=minio</code>.</p> <p>Important Note</p> <p>The script is based on a guide for deploying MinIO. The MinIO-based Object Storage that the script creates is not meant for production usage.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#2-creating-a-workbench-and-a-notebook","title":"2. Creating a workbench and a notebook","text":""},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#creating-a-workbench-and-selecting-a-notebook-image","title":"Creating a workbench and selecting a notebook image","text":"<p>Procedure:</p> <p>Prepare your Jupyter notebook server for using a GPU, you need to have:</p> <ul> <li>Select the correct data science project and create workbench, see     Populate the data science project     for more information.</li> </ul> <p>Please ensure that you start your Jupyter notebook server with options as depicted in the following configuration screen. This screen provides you with the opportunity to select a notebook image and configure its options, including the Accelerator and Number of accelerators (GPUs).</p> <p>Click Attach existing connections under the Connections section, and attach the \"My Storage\" connection that was set up previously to the workbench:</p> <p></p> <p>Search and add \"My Storage\":</p> <p></p> <p>Click on \"Attach\" button:</p> <p></p> <p>The final workbench setup, before clicking the Create workbench button, should look like this:</p> <p></p> <p>For our example project, let's name it \"Fraud detection\". We'll select the PyTorch image with Recommended Version (selected by default), choose a Deployment size of Medium, choose Accelerator of NVIDIA V100 GPU, Number of accelerators as 1, and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Running Workbench without GPU</p> <p>If you do not require GPU resources for your task, you can leave the Accelerator field set to its default None selection.</p> <p></p> <p>Verification:</p> <p>If this procedure is successful, you have started your Jupyter notebook server. When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment:</p> <p></p> <p>Note</p> <p>If you made a mistake, you can edit the workbench to make changes. Please make sure you set the Running status of your workbench to Stopped prior clicking the action menu (\u22ee) at the end of the selected workbench row as shown below:</p> <p></p> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when prompted, as shown below:</p> <p></p> <p>Next, you should see the NERC RHOAI JupyterLab Web Interface, as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#importing-the-tutorial-files-into-the-jupyter-environment","title":"Importing the tutorial files into the Jupyter environment","text":"<p>Bring the content of this tutorial inside your Jupyter environment:</p> <p>On the toolbar, click the Git Clone icon:</p> <p></p> <p>Enter the following Git Repo URL: https://github.com/nerc-project/object-detection</p> <p>Check the Include submodules option, and then click Clone.</p> <p></p> <p>In the file browser, double-click the newly-created object-detection folder.</p> <p></p> <p>Verification:</p> <p>In the file browser, you should see the notebooks that you cloned from Git.</p> <p></p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#3-data-preparation","title":"3. Data Preparation","text":"<p>In your Jupyter notebook environment, within the root folder path of <code>object-detection</code>, find the Jupyter notebook file <code>01-data_preparation.ipynb</code> that demonstrates how to prepare the dataset within the NERC RHOAI. Please follow the instructions directly in the notebook environment. To run it you need to double click it and execute the \"Run\" button to run all notebook cells at once.</p> <p>To train the model, we need a dataset containing images of the target classes, along with their labels and bounding box definitions for object detection. In this example, we use images from Google's Open Images dataset, focusing on threeclasses: Bicycle, Car, and Traffic Sign.</p> <p>Using the Class and Image per Class </p> <p>In this example, we selected only a few classes to speed up the process and also we are only downloading 300 images per class to limit the processing time. However, to achieve a robust YOLOv5 model, it is recommended to train with over 1500 images per class, and more then 10,000 instances per class.</p> <p>We can now randomly split all our images in train/validate/test. We will use here a standard split scheme: 0.75, 0.125, 0.125.</p> <p>Once you have completed the entire notebook, the dataset will be ready for training!</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#4-model-training","title":"4. Model training","text":"<p>YOLOv5 is pre-trained to recognize various objects. In this guide, we will use Transfer Learning to fine-tune YOLOv5 for identifying a custom set of images.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#transfer-learning","title":"Transfer Learning","text":"<p>Transfer learning is a machine learning technique where a model trained for one task is adapted for another related task. Instead of training from scratch, it utilizes a pre-trained model as a foundation, significantly reducing the need for data and computing resources. This approach applies learned knowledge from one problem to another, making it highly effective in fields like natural language processing, computer vision, and speech recognition.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#training-with-yolov5","title":"Training with YOLOv5","text":"<p>For the training step, open the notebook file <code>02-model_training.ipynb</code> in the same Workbench environment and follow the instructions. The notebook will guide you through experimentation, model training, and testing the model.</p> <p>In this example, we will train using the smallest base model i.e. YOLOv5 to save time. However, you can modify the base model and adjust the training hyperparameters to achieve better results.</p> <p>Verification:</p> <p>This process will take some time to complete. At the end, it will generate and save the model <code>yolov5n.pt</code> within the root folder path of <code>object-detection</code>.</p> <p>Warning</p> <p>The memory allocated to your Workbench based on selected Container Size significantly affects the batch size you can use, regardless of your GPU's capacity.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#5-preparing-a-model-for-deployment","title":"5. Preparing a model for deployment","text":"<p>Procedure:</p> <p>In your JupyterLab environment, open the notebook file <code>03-yolov5_to_onnx.ipynb</code>. Follow the instructions in the notebook to store the model and save it in the portable ONNX format from YoloV5 model.</p> <p>Verification:</p> <p>After completing the notebook steps, verify that the <code>yolov5n.onnx</code> file is created within the root folder path of <code>object-detection</code>.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#6-save-the-model","title":"6. Save the Model","text":"<p>Procedure:</p> <p>In your JupyterLab environment, open the notebook file <code>04-save_model.ipynb</code> and follow the instructions. This notebook will guide you through saving the model to S3-compatible object storage, corresponding to the My Storage connection, which was set up in this step.</p> <p>Verification:</p> <p>After completing the notebook steps, verify that the <code>models/yolov5n.onnx</code> file is stored in the object storage. Once saved, the model is now ready for use by your model server.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#7-deploying-and-testing-a-model","title":"7. Deploying and testing a model","text":""},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#deploying-a-model","title":"Deploying a model","text":"<p>Now that the model is stored and saved in the portable ONNX format, you can deploy it as an API using an OpenShift AI Model Server.</p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#deploying-a-model-on-a-multi-model-server","title":"Deploying a model on a multi-model server","text":"<p>NERC RHOAI multi-model servers can host multiple models simultaneously. You can create a new model server and deploy your model to it.</p> <p>Procedure:</p> <p>In the OpenShift AI dashboard, navigate to the data science project details page and click the Models tab.</p> <p>Select Add model server as shown below:</p> <p></p> <p>In the pop-up window that appears, depicted as shown below, you can specify the following details:</p> <p>For Model server name, type a name, for example Model Server.</p> <p>For Serving runtime, select OpenVINO Model Server.</p> <p>Leave the other fields with the default settings.</p> <p></p> <p>Click Add.</p> <p>In the Models list, next to the new model server, click Deploy model.</p> <p></p> <p>In the form, provide the following values:</p> <p>For Model deployment name, type yolo.</p> <p>For Model framework (name - version), select onnx-1.</p> <p>For Existing connection, select My Storage.</p> <p>Type the path that leads to the version folder that contains your model file: models</p> <p>Leave the other fields with the default settings.</p> <p></p> <p>Click Deploy.</p> <p>Verification:</p> <p>When you return to the Deployed models page, you will see your newly deployed model. You should click on the 1 on the Deployed models tab to see details. Notice the loading symbol under the Status section. When the model has finished deploying, the status icon will be a green checkmark indicating the model deployment is complete as shown below:</p> <p></p>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#testing-the-model-api","title":"Testing the model API","text":"<p>Now that you've deployed the model, you can test its API endpoints.</p> <p>Procedure:</p> <ul> <li> <p>In the OpenShift AI dashboard, navigate to the project details page and click the Models tab.</p> </li> <li> <p>Take note of the model's Inference endpoint URL. You need this information when you test the model API.</p> <p></p> <p>The Inference endpoint field contains an Internal Service link, click the link to open a popup that shows the URL details, and then take note of the grpcUrl and restUrl values.</p> </li> <li> <p>Return to the Jupyter notebook environment and test your new inference endpoints.</p> <p>To make a gRPC API call, follow the instructions in <code>05-remote_inference_grpc.ipynb</code>, updating the grpc_host with your own grpcUrl value (as noted above).</p> <p>Follow the instructions in <code>06-remote_inference_rest.ipynb</code> to make a REST API call. Be sure to update the rest_url with your own restUrl value (as noted above).</p> <p></p> <p>Note</p> <p>The notebook provides an example for processing a single image and also demonstrates batch processing for multiple images from the images folder. This enables a basic benchmark to measure processing and inference time.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/object-detection-app-using-yolo5/#deploy-the-model-application-on-nerc-openshift","title":"Deploy the Model Application on NERC OpenShift","text":"<p>The model application includes a visual user interface (UI) powered by Streamlit, allowing users to interact with the trained model. This interface enables users to upload images and interact with machine learning models seamlessly. This app communicates with a remote model inference service via a REST API to process images and generate predictions about the objects in the uploaded image.</p> <p>The model application code is located in the <code>application</code> folder within the root directory of <code>object-detection</code>. You can find this folder in the GitHub repository you cloned during the step: Importing the tutorial files into the Jupyter environment.</p> <p></p> <p>If you look inside it <code>app.py</code>, you will find two crucial lines of code for retrieving environment variables:</p> <pre><code># Get a few environment variables. These are so we:\n# - Know what endpoint we should request\n# - Set server name and port for Streamlit\nMODEL_NAME = os.getenv(\"MODEL_NAME\", \"yolo\")    &lt;----------\nREST_URL = os.getenv(\"INFERENCE_ENDPOINT\")    &lt;----------\nINFER_URL = f\"{REST_URL}/v2/models/{MODEL_NAME}/infer\"\n...\n\n    infer = ort_v5(uploaded_image_path, INFER_URL, ...)    &lt;----------\n</code></pre> <p>This is how you send a request to the NERC RHOAI Model Server with the user uploaded image to process for a prediction.</p> <p>We will deploy the application on OpenShift by linking it to the GitHub repository. OpenShift will fetch the repository, build a container image using the Dockerfile, and publish it automatically.</p> <p>To accomplish this, from the OpenShift AI dashboard, navigate to the OpenShift Web Console:</p> <p></p> <p>From your NERC's OpenShift Web Console, navigate to your project corresponding to the NERC RHOAI Data Science Project and select the \"Import from Git\" button, represented by the \"+\" icon in the top navigation bar as shown below:</p> <p></p> <p>Alternatively, navigate to the Topology page under Workloads. Right-click on the page and select the \"Import from Git\" option from the Add to Project in-context menu as shown below:</p> <p></p> <p>In the \"Git Repo URL\" enter: <code>https://github.com/nerc-project/object-detection</code> (this is the same repository you pulled into RHOAI earlier). Then press \"Show advanced Git options\" and set \"Context dir\" to \"/application\" where the application Dockerfile is located as shown below:</p> <p></p> <p>At the General section, select \"Create application\" option under Application and then give unique Application name i.e. <code>object-detection-application</code> and also Name i.e. <code>object-detection-application</code> as shown below:</p> <p></p> <p>Finally, at the Deploy section, press \"Show advanced Deployment option\".</p> <p>Set these values in the Environment variables (runtime only) fields:</p> <p></p> <p>Name: MODEL_NAME</p> <p>Value: From the RHOAI projects interface (from the previous section), copy the Model name value. For example: <code>yolo</code>.</p> <p>Name: INFERENCE_ENDPOINT</p> <p>Value: From the RHOAI projects interface (from the previous section), copy the restUrl value. For example: <code>http://modelmesh-serving.&lt;your-namespace&gt;:8008</code>.</p> <p></p> <p>Your full settings page should look something like this:</p> <p></p> <p>Target Port</p> <p>Under \"Advanced options,\" make sure to set the Target Port to 8501, which corresponds to the exposed port in the application's Dockerfile.</p> <p>Press Create to start deploying the application.</p> <p>You should now see a new object is added in your topology map for the application that you just added. When the circle of your deployment turns dark blue it means that it has finished deploying.</p> <p>If you want more details on how the deployment is going, you can press the circle and look at Resources in the right menu that opens up. There you can see how the build is going and what's happening to the pod.</p> <p></p> <p>The application will be ready when the build is complete and the pod is \"Running\".</p> <p>When the application has been deployed successfully, you can either open the application URL using the Open URL icon as shown below or you can naviate to the route URL by navigating to the \"Routes\" section under the Location path as shown below:</p> <p></p> <p>This will open the application interface in a new tab.</p> <p>Congratulations, you now have an application running your AI model!</p> <p>Try uploading your own image by browsing or dragging and dropping a file (.jpg,.png, or .jpeg only) to see if the app can accurately predict and label objects in it.</p> <p></p> <p></p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/","title":"Predictive & Generative AI (LLM & RAG) in Action","text":""},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#predictive-generative-ai-llm-rag-in-action","title":"Predictive &amp; Generative AI (LLM &amp; RAG) in Action","text":""},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#context-and-definitions","title":"Context and definitions","text":""},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#model-building-vs-model-tuning","title":"Model Building vs. Model Tuning","text":"<p>Building an AI model from scratch requires gathering large datasets (corpus) and training a model, which is resource-intensive and time-consuming. In contrast, model tuning optimizes a pretrained model's hyperparameters to improve its performance, making it more efficient for specific tasks.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#model-tuning-vs-fine-tuning","title":"Model Tuning vs. Fine Tuning","text":"<p>Model tuning optimizes a model's hyperparameters, while fine-tuning adapts a pretrained model for a specific task by adjusting its weights and sometimes architecture. Fine-tuning enhances the model's performance for specialized tasks, like sentiment analysis or question answering, without needing to train a new model from scratch.</p> Aspect Model Tuning Fine-Tuning Purpose Optimizes hyperparameters to improve model performance Adapts a pre-trained model to a specific task Changes Adjusts training settings (learning rate, batch size, etc.) Modifies model weights and sometimes architecture Scope Can be applied to both pre-trained and newly trained models Focuses on pre-trained models for domain-specific tasks Example Finding the best learning rate for training Training a pre-trained BERT model on medical texts <p>Note</p> <p>Model tuning focuses on optimizing hyperparameters, while fine-tuning adapts a model by modifying its weights for a specific application.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#pre-trained-models-vs-fine-tuned-llms","title":"Pre-trained models vs. Fine-tuned LLMs","text":"<p>Pre-trained models like Granite, Mistral, GPT-4, or LLaMA are trained on vast, diverse datasets, enabling them to perform general-purpose language tasks. However, they may lack domain-specific knowledge or precision in specialized contexts.</p> <p>Fine-tuning involves further training a model on a smaller, task-specific dataset to improve its performance for a particular use case. It enhances accuracy, efficiency, and customization while reducing prompt complexity and incorporating proprietary data.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#foundation-model","title":"Foundation Model","text":"<p>Foundation models are a subset of pre-trained models designed to serve as a strong base for multiple downstream tasks without requiring extensive modifications. They are typically large-scale models trained on diverse datasets, capturing broad knowledge that can be fine-tuned for specialized applications.</p> <p>By leveraging foundation models, developers can significantly reduce the time and resources needed for task-specific training. These models enable efficient adaptation to various domains, making them a powerful tool for AI-driven applications.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#llm-serving","title":"LLM Serving","text":"<p>On NERC, we offer various solutions to handle LLM serving.</p> <ol> <li> <p>An easier option is to use the NERC RHOAI Serving runtime, which can be utilized     when setting up the RHOAI Model Server, as explained here.</p> </li> <li> <p>Alternatively, you can set up compatible LLM servers as standalone deployments.</p> </li> </ol>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#as-a-standalone-deployment","title":"As a Standalone Deployment","text":"<p>In this repository, you will find:</p> <ul> <li> <p>Deployment Recipes for various LLM servers as standalone deployments.</p> </li> <li> <p>Notebook Examples demonstrating how to query different LLM servers.</p> </li> </ul> <p>Within the <code>standalone</code> folder of each LLM server's directory, you will find all the necessary YAML files defining the required PVC, Deployment, Service, Route, etc. objects.</p> <p>If the deployment's container image relies on a custom-built Docker image, the LLM server directory will also include a Containerfile. You can use this file to build the inference runtime container from scratch.</p> <p>Very Important Note</p> <p>This is only an example! Adapt the files to your specific requirements, especially the Deployment configuration, including CPU, RAM, GPU, tolerations, and other relevant settings.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#llm-clients","title":"LLM clients","text":"<p>LLM clients provide user-friendly interfaces for interacting with LLMs. These clients help streamline the deployment and usage of LLMs for various applications, such as chatbots, document processing, and AI-powered automation.  </p> <ul> <li> <p>AnythingLLM:</p> <p>AnythingLLM is an open-source framework that allows users to connect and interact with LLMs efficiently. It supports integrations with multiple AI models and provides a web-based UI for seamless communication.</p> <p>You can deploy and run the AnythingLLM directly on the NERC RHOAI by following these intructions.</p> </li> <li> <p>Open WebUI:</p> <p>Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for RAG, making it a powerful AI deployment solution.</p> <p>You can deploy and run the Open WebUI directly on the NERC OpenShift using Helm by following these intructions.</p> </li> </ul>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#language-model-evaluation","title":"Language Model Evaluation","text":"<p>\u29bf Overview</p> <ul> <li> <p>LLM Evaluation capabilities provides secure, standardized framework for AI model assessment</p> </li> <li> <p>Integrates LM-evaluation-harness with Unitxt task cards for benchmarking without custom code</p> </li> <li> <p>Enables data scientists and AI engineers to easily deploy jobs testing metrics like toxicity and accuracy</p> </li> <li> <p>Advances AI quality assurance and model governance capabilities</p> </li> </ul> <p>\u29bf Why does it matter?</p> <ul> <li> <p>Enhancing Quality Assurance: Test models against both industry standard and proprietary benchmarks</p> </li> <li> <p>Simplifying Compliance: Standardized evaluation metrics help meet regulatory and governance requirements</p> </li> <li> <p>Boosting Security: Secure evaluation environment with configurable data isolation protects sensitive information</p> </li> <li> <p>Accelerating Development: Quick feedback cycles on model performance speed up iteration and improvement</p> </li> </ul> <p>\u29bf References</p> <p>To learn more about evaluating large language models, read this documentation.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#quantization","title":"Quantization","text":"<p>Model quantization reduces computational and memory demands by converting model parameters into lower-precision formats. This improves efficiency, making models more suitable for resource-constrained devices, albeit with a slight loss in accuracy.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#vector-database","title":"Vector Database","text":"<p>A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving multi-dimensional vector data. These databases enable advanced data analysis and similarity search operations, going beyond the traditional structured query approach used in conventional databases. These databases are widely used in machine learning, image processing, and NLP to enable fast retrieval and complex computations on vector data.</p> <p>What Is a Vector?</p> <p>A vector is a numerical representation of data that can capture the context and semantics of data.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#qdrant","title":"Qdrant","text":"<p>Qdrant is an AI-native vector database and semantic search engine designed to extract meaningful insights from unstructured data.</p> <p>You can deploy and run the Qdrant vector database directly on the NERC OpenShift environment by following these intructions.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#milvus","title":"Milvus","text":"<p>Milvus is an open-source vector database built for AI applications, particularly those involving machine learning models that generate high-dimensional vectors for similarity search and clustering. It is optimized to handle large-scale vector data with low latency and high throughput, making it perfect for use cases like recommendation systems, image and video search, and natural language processing.</p> <p>Milvus is an excellent choice for those looking for a scalable, flexible, and high-performance vector database to support their AI applications.</p> <p>Deployment instructions for the NERC OpenShift are available here.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#pgvector","title":"pgvector","text":"<p>pgvector is an open-source extension for PostgreSQL that allows efficient storage, indexing, and querying of high-dimensional vector embeddings for similarity search. It seamlessly integrates with PostgreSQL, making it ideal for AI applications such as recommendation systems, image retrieval, and natural language processing (NLP).</p> <p>You can deploy and run pgvector directly on the NERC OpenShift environment by following these intructions.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#embeddings","title":"Embeddings","text":"<p>Embeddings represent words or phrases as multi-dimensional vectors that capture semantic relationships, allowing machines to better understand language. They are created through unsupervised learning and are crucial for tasks like sentiment analysis, machine translation, and recommendation systems.</p> <p>Embeddings are another type of model often associated with LLMs as they are used to convert documents into vectors. A database of those vectors can then be queried to find the documents related to a query you make.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#predictive-ai","title":"Predictive AI","text":"<p>Using historical data, predictive AI enables organizations to identify patterns and make informed decisions about the future. By training a model with your own data, you're essentially creating a tool that can forecast outcomes based on patterns and insights derived from the past data it has learned from. It powers applications such as demand forecasting, predictive maintenance, and operational planning. Predictive AI relies on well-established data science and Machine Learning (ML) techniques, continuously improving as it processes more data.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#predictive-ai-tutorial-credit-card-fraud-detection-application","title":"Predictive AI tutorial - Credit Card Fraud Detection Application","text":"<p>In this tutorial, we will demonstrate how to use NERC Red Hat OpenShift AI (RHOAI) to train a fraud detection model in JupyterLab, deploy it, refine it using automated pipelines, and deploy the \"Credit Card Fraud Detection\" application that use Gradio as the UI engine, in NERC OpenShift, which connects to the deployed model.</p> <p></p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#predictive-ai-tutorial-object-detection-application-using-yolov5-model","title":"Predictive AI tutorial - Object Detection Application Using YOLOv5 Model","text":"<p>In this tutorial, we will explore how to use YOLOv5), an object detection model, to recognize specific objects in images, as well as how to deploy and utilize the model.</p> <p></p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#generative-ai-genai","title":"Generative AI (GenAI)","text":"<p>Generative AI, powered by deep learning models like transformers, can generate new content, including text, images, and code. It is especially valuable for applications such as chatbots, automated content creation, and creative tools. Models like generative pretrained transformers (GPTs) have transformed natural language processing and creative industries by producing human-like text, images, music, and more by generating outputs that closely resemble the data on which they were trained.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#comparision-between-predictive-ai-and-generative-ai","title":"Comparision Between Predictive AI and Generative AI","text":"Type of AI Purpose Techniques Examples Predictive AI Uses historical data to forecast future events or trends. Regression analysis, classification, time series forecasting, machine learning models. Predicting stock prices, customer behavior, equipment failure. Generative AI Creates new content like text, images, and music based on input data. Generative Adversarial Networks (GANs), Transformers. Text generation, image creation, music composition."},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#large-language-model-llm","title":"Large Language Model (LLM)","text":"<p>A subset of Generative AI focused specifically on understanding and generating human language. LLMs are key drivers behind the rapid growth of GenAI. These models are pre-trained or fine-tuned on large corpuses of text data to produce coherent and contextually relevant text. LLMs, such as GPT, are pretrained on massive datasets and excel at understanding and generating natural language, making them invaluable for applications like customer support automation and marketing copy generation.</p> <p></p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#llm-application","title":"LLM Application","text":"<p>In this tutorial, we will demonstrate how to deploy an LLM Model server that includes command-line tools and a server with a simple web interface, i.e., \"Chat\", which can connect to a pre-trained foundation model.</p> <p></p> <p>However, LLMs have limitations, such as relying solely on text and lacking the ability to understand the broader context beyond their limited window of training data.</p> <p></p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#retrieval-augumented-generation-rag","title":"Retrieval Augumented Generation (RAG)","text":"<p>Retrieval-augmented generation (RAG) is a natural language processing (NLP) technique that enhances large language models (LLMs) by incorporating contextual information to generate more accurate responses. In recent years, RAG has become the de facto method for integrating enterprise or user-specific data into models. An AI technique that enhances generative models by integrating a retrieval system to fetch relevant documents, providing more accurate and contextually enriched responses.</p> <p>Note</p> <p>This IDC predicts that by 2028, 80% of RAG implementations will be used to enhance the accuracy and relevance of GenAI applications, and by 2026, two-thirds of A1000 businesses will leverage RAG to power domain-specific knowledge discovery. For more you can read here.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#rag-workflow","title":"RAG Workflow","text":"<p>1. Retrieve: The user query is used to retrieve relevant context from an external knowledge source. For this, the user query is embedded with an embedding model into the same vector space as the additional context in the vector database. This allows to perform a similarity search, and the top k closest data objects from the vector database are returned.</p> <p>2. Augment: The user query and the retrieved additional context are stuffed into a prompt template.</p> <p>3. Generate: Finally, the retrieval-augmented prompt is fed to the LLM.</p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#rag-best-practices","title":"RAG Best Practices","text":""},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#rag-application-talk-with-your-pdf","title":"RAG Application - Talk with your PDF","text":"<p>In this tutorial, we will demonstrate how to set up a Workbench using a Data Science Project (DSP) on the NERC RHOAI, and explore an LLM application, i.e., \"Talk with your PDF,\" which connects to a pre-trained foundation model. Finally, we will deploy this application on NERC OpenShift.</p> <p></p>"},{"location":"openshift-ai/other-projects/predictive-and-generative-AI/#agentic-ai-the-future","title":"Agentic AI: The Future","text":"<p>RAG utilizes one or more external vector databases to provide additional context to a generative AI model when answering queries. Another emerging approach is Agentic AI systems, which integrate multiple generative AI agents to query external sources-such as internal databases, corporate intranets, or the internet-ensuring models have access to the most accurate and up-to-date information.</p> <p>AI Agents are autonomous systems designed to perform tasks by dynamically interacting with data, tools, or APIs, often improving RAG workflows by executing complex, multi-step reasoning processes. They address RAG limitations by automating task orchestration and adapting responses based on real-time feedback and context.</p> <p></p>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/","title":"Serving Text Generation Inference Service (TGIS) and FLAN-T5 Small Model","text":""},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#serving-text-generation-inference-service-tgis-and-flan-t5-small-model","title":"Serving Text Generation Inference Service (TGIS) and FLAN-T5 Small Model","text":"<p>Prerequisites:</p> <ul> <li> <p>You have enabled the Single-model Serving platform. For more information     about enabling the single-model serving platform, see     Setting up the Single-model Server platform.</p> </li> <li> <p><code>FLAN-T5 small</code> model: Google's FLAN-T5 Small     is a lightweight model known for its compatibility and low resource requirements     (no GPU needed). Although it's not a high-performing model, the same     process applies to any compatible model. FLAN-T5 Small is based on the     pretrained T5 model and has been fine-tuned using instruction data to     improve zero-shot and few-shot NLP tasks such as reasoning and question     answering.</p> </li> </ul> <p>Procedure:</p> <p>At a high level, we will:</p> <ul> <li> <p>Download a model from Huggingface</p> </li> <li> <p>Deploy the model by using single-model serving with a serving runtime</p> </li> <li> <p>Test the model API</p> </li> </ul>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#set-up-local-s3-storage-minio-and-connection","title":"Set up local S3 storage (MinIO) and Connection","text":"<ol> <li> <p>Navigating to the OpenShift AI dashboard.</p> <p>Please follow these steps to access the NERC OpenShift AI dashboard.</p> </li> <li> <p>Using a script to set up local S3 storage (MinIO) on your Data Science Project     in the NERC RHOAI as described here.</p> </li> <li> <p>Once your local S3 Object storage using MinIO is completed, you can browse to     the MinIO Web Console using the provided URL. Enter the Access Key as     the Username and the Secret Key as the Password. This will open     the Object Browser, where you should verify that the bucket: my-storage     is visible as shown below:</p> <p></p> </li> <li> <p>Click Connections. You should see one connection listed: My Storage as shown below:</p> </li> </ol> <p></p>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#creating-a-workbench-and-a-notebook","title":"Creating a workbench and a notebook","text":"<p>Procedure:</p> <p>Prepare your Jupyter notebook server for using No GPU, you need to have:</p> <ul> <li>Select the correct data science project and create workbench, see     Populate the data science project     for more information.</li> </ul> <p>Please ensure that you start your Jupyter notebook server with options as depicted in the following configuration screen. This screen provides you with the opportunity to select a notebook image and configure its options. As we do not require any GPU resources to run <code>FLAN-T5 small</code> model, we can leave the Accelerator field set to its default None selection.</p> <p>Click Attach existing connections under the Connections section, and attach the \"My Storage\" connection that was set up previously to the workbench:</p> <p></p> <p>Search and add \"My Storage\":</p> <p></p> <p>Click on \"Attach\" button:</p> <p></p> <p>The final workbench setup, before clicking the Create workbench button, should look like this:</p> <p></p> <p>For our example project, let's name it \"TGIS Workbench\". We'll select the Standard Data Science image with Recommended Version (selected by default), choose a Deployment size of Small, Accelerator as None (no GPU is needed for this setup), and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Verification:</p> <p>If this procedure is successful, you have started your Jupyter notebook server. When your workbench is ready and the status changes to Running, click the open icon () next to your workbench's name, or click the workbench name directly to access your environment:</p> <p></p> <p>Note</p> <p>If you made a mistake, you can edit the workbench to make changes. Please make sure you set the Running status of your workbench to Stopped prior clicking the action menu (\u22ee) at the end of the selected workbench row as shown below:</p> <p></p> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when prompted, as shown below:</p> <p></p> <p>Next, you should see the NERC RHOAI JupyterLab Web Interface, as shown below:</p> <p></p> <p>The Jupyter environment is currently empty. To begin, populate it with content using Git. On the left side of the navigation pane, locate the Name explorer panel, where you can create and manage your project directories.</p> <p>Learn More About Working with Notebooks</p> <p>For detailed guidance on using notebooks on NERC RHOAI JupyterLab, please refer to this documentation.</p>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#importing-the-tutorial-files-into-the-jupyter-environment","title":"Importing the tutorial files into the Jupyter environment","text":"<p>Bring the content of this tutorial inside your Jupyter environment:</p> <p>On the toolbar, click the Git Clone icon:</p> <p></p> <p>Enter the following Git Repo URL: https://github.com/nerc-project/llm-on-nerc</p> <p>Check the Include submodules option, and then click Clone.</p> <p></p> <p>In the file browser, double-click the newly-created llm-on-nerc folder.</p> <p></p> <p>Verification:</p> <p>In the file browser, you should see the notebooks that you cloned from Git.</p> <p></p>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#downloading-model","title":"Downloading Model","text":"<p>Prerequisites:</p> <p>First, let's navigate to the relevant notebooks.</p> <ol> <li> <p>Navigate to <code>llm-on-nerc/llm/tgis</code></p> </li> <li> <p>In your notebook environment, open the file <code>1_download_save.ipynb</code></p> </li> <li> <p>Follow the instructions directly in the notebook.</p> </li> <li> <p>The instructions will guide you through downloading the model from Hugging Face Hub model hub and uploading it to your <code>models</code> bucket, which is located within your main bucket mapped through the Connection.</p> </li> </ol> <p></p> <p>Verification:</p> <p>When you have completed the notebook instructions, you should see files listed in the directory/prefix: <code>models/flan-t5-small</code></p> <pre><code>models/flan-t5-small/README.md\nmodels/flan-t5-small/config.json\nmodels/flan-t5-small/flax_model.msgpack\nmodels/flan-t5-small/generation_config.json\nmodels/flan-t5-small/model.safetensors\nmodels/flan-t5-small/pytorch_model.bin\nmodels/flan-t5-small/special_tokens_map.json\nmodels/flan-t5-small/spiece.model\nmodels/flan-t5-small/tf_model.h5\nmodels/flan-t5-small/tokenizer.json\nmodels/flan-t5-small/tokenizer_config.json\n</code></pre>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#setting-up-single-model-server-and-deploy-the-model","title":"Setting up Single-model Server and Deploy the model","text":"<ol> <li> <p>In the left menu, click Data science projects.</p> <p>The Data science projects page opens.</p> </li> <li> <p>Click the name of the project that you want to deploy a model in.</p> <p>A project details page opens.</p> </li> <li> <p>Click the Models tab.</p> </li> <li> <p>Click the Deploy model button.</p> <p></p> </li> <li> <p>The Deploy model dialog opens.</p> <p>Enter the following information for your new model:</p> <ul> <li> <p>Model deployment name: Enter a unique name for the model that you are     deploying (e.g., \"flan-t5-small\").</p> </li> <li> <p>Serving runtime: Select TGIS Standalone ServingRuntime for KServe     runtime.</p> </li> <li> <p>Model framework (name - version): This is pre-selected as <code>pytorch</code>.</p> </li> <li> <p>Deployment mode: From the Deployment mode list, select Advanced     option - uses Knative Serverless.</p> </li> <li> <p>Number of model server replicas to deploy has Minimum replicas:     <code>1</code> and Maximum replicas:<code>1</code>.</p> </li> <li> <p>Model server size: This is the amount of resources, CPU, and RAM that     will be allocated to your server. Here, you can select <code>Small</code> size.</p> </li> <li> <p>Accelerator: Select <code>None</code>.</p> </li> <li> <p>Model route: Select the checkbox for \"Make deployed models available     through an external route\" this will enable us to send requests to the model     endpoint from outside the cluster.</p> </li> <li> <p>Token authentication: Select the checkbox for \"Require token authentication\"     if you want to secure or restrict access to the model by forcing requests     to provide an authorization token, which is important for security. While     selecting it, you can keep the populated Service account name i.e. <code>default-name</code>.</p> </li> <li> <p>Source model location:</p> <p>i.  Select the Connection option from the dropdown list that you created     as described here to     store the model by using the Existing connection option Connection     dropdown list i.e. <code>My Storage</code>.</p> <p>Alternatively, you can create a new connection directly from this menu by selecting Create connection option.</p> <p>ii. Path: If your model is not located at the root of the bucket of     your connection, you must enter the path to the folder it is in i.e.     <code>models/flan-t5-small</code>.</p> </li> <li> <p>Configuration parameters: You can customize the runtime parameters     in the Additional serving runtime arguments field. You don't need     to add any arguments here.</p> </li> </ul> </li> </ol> <p>For our example, set the Model deployment name to <code>flan-t5-small</code>, and select Serving runtime as <code>TGIS Standalone ServingRuntime for KServe</code>. Also, ensure that the Deployment mode is set to <code>Advanced</code> - uses Knative Serverless.</p> <p>Please leave the other fields at their default settings. For example, the Number of model server replicas to deploy has Minimum replicas set to <code>1</code> and Maximum replicas set to <code>1</code>, Model server size set to <code>Small</code>, and Accelerator set to <code>None</code>.</p> <p></p> <p>At this point, ensure that both Make deployed models available through an external route and Require token authentication are unchecked. Select <code>My Storage</code> as the Connection from the Existing connection, and for the model Path location, enter <code>models/flan-t5-small</code> as the folder path, as shown below:</p> <p></p> <p>When you are ready to deploy your model, select the Deploy button.</p> <p>Confirm that the deployed model appears on the Models tab for your project. After some time, once the model has finished deploying, the model deployments page of the dashboard will display a green checkmark in the Status column, indicating that the deployment is complete.</p> <p>To view details for the deployed model, click the dropdown arrow icon to the left of your deployed model name (e.g., <code>flan-t5-small</code>), as shown below:</p> <p></p> <p>You can also modify the configure properties for your deployed model configuration by clicking on the three dots on the right side, and selecting Edit. This will bring back the same configuration pop-up window we used earlier. This menu also has the option for you to Delete the deployed model.</p>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#check-the-model-api","title":"Check the Model API","text":"<p>The deployed model is now accessible through the API endpoint of the model server. The information about the endpoint is different, depending on how you configured the model server.</p> <p>As in this example, you have NOT exposed the model externally through a route, click on the \"Internal and external endpoint details\" link in the Inference endpoint section. A popup will display the internal address for the url for the inference endpoint as shown below:</p> <p></p> <p>Notes:</p> <ul> <li>The internal URL displayed is only the base address of the endpoint of the     following format: <code>https://name-of-your-model.name-of-your-project-namespace.svc.cluster.local</code>     that is accessible only within your cluster locally.</li> </ul>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#testing-the-model-api","title":"Testing the model API","text":"<p>Now that you've deployed the model, you can test its API endpoints.</p> <ul> <li> <p>Return to the Jupyter environment.</p> </li> <li> <p>Open the file called <code>2_grpc_request.ipynb</code>.</p> </li> <li> <p>Read the code and follow the instructions.</p> </li> </ul> <p></p>"},{"location":"openshift-ai/other-projects/serving-tgis-and-FLAN-T5-Model/#summary","title":"Summary","text":"<p>Deploying validated models from Red Hat AI's Hugging Face Validated Models repository in disconnected OpenShift AI environments involves the following steps:</p> <ul> <li> <p>Set up local S3 storage (MinIO) and create a connection to point to the bucket.</p> </li> <li> <p>Select the desired model.</p> </li> <li> <p>Download the model and upload it to the S3 storage bucket.</p> </li> <li> <p>Identify the required serving runtime.</p> </li> <li> <p>Configure a single-model server and deploy the model using the connection.</p> </li> <li> <p>Verify and test the model's API inference endpoints.</p> </li> </ul> <p>This process ensures that AI workloads run seamlessly in restricted or disconnected environments, allowing you to securely leverage validated and optimized AI models.</p>"},{"location":"openstack/","title":"OpenStack","text":""},{"location":"openstack/#openstack-tutorial-index","title":"OpenStack Tutorial Index","text":"<p>If you're just starting out, we recommend starting from</p> <p>Access the OpenStack Dashboard and going through the tutorial in order.</p> <p>If you just need to review a specific step, you can find the page you need in the list below.</p>"},{"location":"openstack/#logging-in","title":"Logging In","text":"<ul> <li>Access the OpenStack Dashboard &lt;&lt;-- Start Here</li> <li>Dashboard Overview</li> </ul>"},{"location":"openstack/#access-and-security","title":"Access and Security","text":"<ul> <li>Security Groups</li> <li>Create a Key Pair</li> </ul>"},{"location":"openstack/#create-connect-to-the-vm","title":"Create &amp; Connect to the VM","text":"<ul> <li>Launch a VM</li> <li>Create a Windows VM</li> <li>Available Images</li> <li>Available NOVA Flavors</li> <li>Assign a Floating IP</li> <li>SSH to the VM</li> </ul>"},{"location":"openstack/#openstack-cli","title":"OpenStack CLI","text":"<ul> <li>OpenStack CLI</li> <li>Launch a VM using OpenStack CLI</li> </ul>"},{"location":"openstack/#persistent-storage","title":"Persistent Storage","text":""},{"location":"openstack/#block-storage-volumes-cinder","title":"Block Storage/ Volumes/ Cinder","text":"<ul> <li>Block Storage/ Volumes/ Cinder</li> <li>Create an empty volume</li> <li>Attach the volume to an instance</li> <li>Format and Mount the Volume</li> <li>Detach a Volume</li> <li>Delete Volumes</li> <li>Extending Volume</li> <li>Transfer a Volume</li> </ul>"},{"location":"openstack/#object-storage-swift","title":"Object Storage/ Swift","text":"<ul> <li>Object Storage/ Swift</li> <li>Mount The Object Storage</li> </ul>"},{"location":"openstack/#data-transfer","title":"Data Transfer","text":"<ul> <li>Data Transfer To/ From NERC VM</li> </ul>"},{"location":"openstack/#backup-your-instance-and-data","title":"Backup your instance and data","text":"<ul> <li>Backup with snapshots</li> </ul>"},{"location":"openstack/#vm-management","title":"VM Management","text":"<ul> <li>VM Management</li> </ul>"},{"location":"openstack/#decommission-openstack-resources","title":"Decommission OpenStack Resources","text":"<ul> <li>Decommission OpenStack Resources</li> </ul>"},{"location":"openstack/#advanced-openstack-topics","title":"Advanced OpenStack Topics","text":""},{"location":"openstack/#setting-up-your-own-network","title":"Setting Up Your Own Network","text":"<ul> <li>Set up your own Private Network</li> <li>Create a Router</li> </ul>"},{"location":"openstack/#domain-or-host-name-for-your-vm","title":"Domain or Host Name for your VM","text":"<ul> <li>Set up Domain names for your VMs</li> </ul>"},{"location":"openstack/#using-terraform-to-provision-nerc-resources","title":"Using Terraform to provision NERC resources","text":"<ul> <li>Terraform on NERC</li> </ul>"},{"location":"openstack/#python-sdk","title":"Python SDK","text":"<ul> <li>Python SDK</li> </ul>"},{"location":"openstack/#setting-up-your-own-custom-images","title":"Setting Up Your Own Custom Images","text":"<ul> <li>Microsoft Windows image</li> </ul>"},{"location":"openstack/access-and-security/create-a-key-pair/","title":"Create a Key Pair","text":""},{"location":"openstack/access-and-security/create-a-key-pair/#create-a-key-pair","title":"Create a Key-pair","text":"<p>NOTE</p> <p>If you will be using PuTTY on Windows, please read this first.</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#add-a-key-pair","title":"Add a Key Pair","text":"<p>For security, the VM images have password authentication disabled by default, so you will need to use an SSH key pair to log in.</p> <p>You can view key pairs by clicking Project, then click Compute panel and choose Key Pairs from the tabs that appears. This shows the key pairs that are available for this project.</p> <p></p>"},{"location":"openstack/access-and-security/create-a-key-pair/#generate-a-key-pair","title":"Generate a Key Pair","text":"<p>Prerequisite</p> <p>You need ssh installed in your system.</p> <p>You can create a key pair on your local machine, then upload the public key to the cloud. This is the recommended method.</p> <p>Open a terminal and type the following commands (in this example, we have named the key cloud.key, but you can name it anything you want):</p> <pre><code>cd ~/.ssh\nssh-keygen -t rsa -f ~/.ssh/cloud.key -C \"label_your_key\"\n</code></pre> <p>Example:</p> <p></p> <p>You will be prompted to create a passphrase for the key. IMPORTANT: Do not forget the passphrase! If you do, you will be unable to use your key.</p> <p>This process creates two files in your <code>.ssh</code> folder:</p> <pre><code>cloud.key      # private key - don't share this with anyone, and never upload\n# it anywhere ever\ncloud.key.pub  # this is your public key\n</code></pre> <p>Pro Tip</p> <p>The <code>-C \"label\"</code> field is not required, but it is useful to quickly identify different public keys later.</p> <p>You could use your email address as the label, or a user@host tag that identifies the computer the key is for.</p> <p>For example, if Bob has both a laptop and a desktop computer that he will, he might use <code>-C \"Bob@laptop\"</code> to label the key he generates on the laptop, and <code>-C \"Bob@desktop\"</code> for the desktop.</p> <p>On your terminal:</p> <pre><code>pbcopy &lt; ~/.ssh/cloud.key.pub  #copies the contents of public key to your clipboard\n</code></pre> <p>Pro Tip</p> <p>If <code>pbcopy</code> isn't working, you can locate the hidden <code>.ssh</code> folder, open the file in your favorite text editor, and copy it to your clipboard.</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#import-the-generated-key-pair","title":"Import the generated Key Pair","text":"<p>Now that you have created your keypair in <code>~/.ssh/cloud.key.pub</code>, you can upload it to OpenStack by either using Horizon dashboard or OpenStack CLI as described below:</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#1-using-nercs-horizon-dashboard","title":"1. Using NERC's Horizon dashboard","text":"<p>Go back to the Openstack Dashboard, where you should still be on the Key Pairs tab</p> <p>(If not, find it under Project -&gt; Compute -&gt; Key Pairs)</p> <p>Choose \"Import Public Key\". Give the key a name in the \"Key Pair Name\" Box, choose \"SSH Key\" as the Key Type dropdown option and paste the public key that you just copied in the \"Public Key\" text box.</p> <p></p> <p>Click \"Import Public Key\". You will see your key pair appear in the list.</p> <p></p> <p>You can now skip ahead to Adding the key to an ssh-agent.</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#2-using-the-openstack-cli","title":"2. Using the OpenStack CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see     OpenStack Command Line setup     for more information.</li> </ul> <p>To create OpenStack keypair using the CLI, do this:</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#using-the-openstack-client-commands","title":"Using the openstack client commands","text":"<p>Now that you have created your keypair in <code>~/.ssh/cloud.key.pub</code>, you can upload it to OpenStack with name \"my-key\" as follows:</p> <pre><code>openstack keypair create --public-key ~/.ssh/cloud.key.pub my-key\n+-------------+-------------------------------------------------+\n| Field       | Value                                           |\n+-------------+-------------------------------------------------+\n| created_at  | None                                            |\n| fingerprint | 1c:40:db:ea:82:c2:c3:05:58:81:84:4b:e3:4f:c2:a1 |\n| id          | my-key                                          |\n| is_deleted  | None                                            |\n| name        | my-key                                          |\n| type        | ssh                                             |\n| user_id     | 938eb8bfc72e4ca3ad2c94e2eb4059f7                |\n+-------------+-------------------------------------------------+\n</code></pre>"},{"location":"openstack/access-and-security/create-a-key-pair/#create-a-key-pair-using-horizon-dashboard","title":"Create a Key Pair using Horizon dashboard","text":"<p>Alternatively, if you are having trouble creating and importing a key pair with the instructions above, the Openstack Horizon dashboard can make one for you.</p> <p>Click \"Create a Key Pair\", and enter a name for the key pair.</p> <p></p> <p>Click on \"Create a Key Pair\" button. You will be prompted to download a <code>.pem</code> file containing your private key.</p> <p>In the example, we have named the key 'cloud_key.pem', but you can name it anything.</p> <p>Save this file to your hard drive, for example in your Downloads folder.</p> <p>Copy this key inside the <code>.ssh</code> folder on your local machine/laptop, using the following steps:</p> <pre><code>cd ~/Downloads          # Navigate to the folder where you saved the .pem file\nmv cloud.pem ~/.ssh/    # This command will copy the key you downloaded to\n# your .ssh folder.\ncd ~/.ssh               # Navigate to your .ssh folder\nchmod 400 cloud.pem     # Change the permissions of the file\n</code></pre> <p>To see your public key, navigate to Project -&gt; Compute -&gt; Key Pairs</p> <p>You should see your key in the list.</p> <p></p> <p>If you click on the name of the newly added key, you will see a screen of information that includes details about your public key:</p> <p></p> <p>The public key is the part of the key you distribute to VMs and remote servers.</p> <p>You may find it convenient to paste it into a file inside your <code>.ssh</code> folder, so you don't always need to log into the website to see it.</p> <p>Call the file something like <code>cloud_key.pub</code> to distinguish it from your private key.</p> <p>Very Important: Security Best Practice</p> <p>Never share your private key with anyone, or upload it to a server!</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#adding-your-ssh-key-to-the-ssh-agent","title":"Adding your SSH key to the ssh-agent","text":"<p>If you have many VMs, you will most likely be using one or two VMs with public IPs as a gateway to others which are not reachable from the internet.</p> <p>In order to be able to use your key for multiple SSH hops, do NOT copy your private key to the gateway VM!</p> <p>The correct method to use Agent Forwarding, which adds the key to an ssh-agent on your local machine and 'forwards' it over the SSH connection.</p> <p>If ssh-agent is not already running in background, you need to start the ssh-agent in the background.</p> <pre><code>eval \"$(ssh-agent -s)\"\n&gt; Agent pid 59566\n</code></pre> <p>Then, add the key to your ssh agent:</p> <pre><code>cd ~/.ssh\nssh-add cloud.key\nIdentity added: cloud.key (test_user@laptop)\n</code></pre> <p>Check that it is added with the command</p> <pre><code>ssh-add -l\n2048 SHA256:D0DLuODzs15j2OaZnA8I52aEeY3exRT2PCsUyAXgI24 test_user@laptop (RSA)\n</code></pre> <p>Depending on your system, you might have to repeat these steps after you reboot or log out of your computer.</p> <p>You can always check if your ssh key is added by running the <code>ssh-add -l</code> command.</p> <p>A key with the default name id_rsa will be added by default at login, although you will still need to unlock it with your passphrase the first time you use it.</p> <p>Once the key is added, you will be able to forward it over an SSH connection, like this:</p> <pre><code>ssh -A -i cloud.key &lt;username&gt;@&lt;remote-host-IP&gt;\n</code></pre> <p>Connecting via SSH is discussed in more detail later in the tutorial (SSH to Cloud VM); for now, just proceed to the next step below.</p>"},{"location":"openstack/access-and-security/create-a-key-pair/#ssh-keys-with-putty-on-windows","title":"SSH keys with PuTTY on Windows","text":"<p>PuTTY requires SSH keys to be in its own <code>ppk</code> format. To convert between OpenSSH keys used by OpenStack and PuTTY's format, you need a utility called PuTTYgen.</p> <p>If it was not installed when you originally installed PuTTY, you can get it here: Download PuTTY.</p> <p>You have 2 options for generating keys that will work with PuTTY:</p> <ol> <li> <p>Generate an OpenSSH key with ssh-keygen or from the Horizon GUI using the    instructions above, then use PuTTYgen to convert the private key to .ppk</p> </li> <li> <p>Generate a .ppk key with PuTTYgen, and import the provided OpenSSH public    key to OpenStack using the 'Import the generated Key Pair' instructions    above.</p> </li> </ol> <p>There is a detailed walkthrough of how to use PuTTYgen here: Use SSH Keys with PuTTY on Windows.</p>"},{"location":"openstack/access-and-security/security-groups/","title":"Security Groups","text":""},{"location":"openstack/access-and-security/security-groups/#security-groups","title":"Security Groups","text":"<p>Security groups can be thought of like firewalls. They ultimately control inbound and outbound traffic to your virtual machines.</p> <p>Before you launch an instance, you should add security group rules to enable users to ping and use SSH to connect to the instance. Security groups are sets of IP filter rules that define networking access and are applied to all instances within a project. To do so, you either add rules to the default security group Add a rule to the default security group or add a new security group with rules.</p> <p>You can view security groups by clicking Project, then click Network panel and choose Security Groups from the tabs that appears.</p> <p>Navigate to Project -&gt; Network -&gt; Security Groups.</p> <p>You should see a 'default' security group. The default security group allows traffic only between members of the security group, so by default you can always connect between VMs in this group. However, it blocks all traffic from outside, including incoming SSH connections. In order to access instances via a public IP, an additional security group is needed. on the other hand, for a VM that hosts a web server, you need a security group which allows access to ports 80 (for http) and 443 (for https).</p> <p></p> <p>Important Note</p> <p>We strongly advise against altering the default security group and suggest refraining from adding extra security rules to it. This is because the default security group is automatically assigned to any newly created VMs. It is considered a best practice to create separate security groups for related services, as these groups can be reused multiple times.Security groups are very highly configurable, for insance, you might create a basic/ generic group for ssh (port 22) and icmp (which is what we will show as an example here) and then a separate security group for http (port 80) and https (port 443) access if you're running a web service on your instance.</p> <p>You can also limit access based on where the traffic originates, using either IP addresses or security groups to define the allowed sources.</p>"},{"location":"openstack/access-and-security/security-groups/#create-a-new-security-group","title":"Create a new Security Group","text":""},{"location":"openstack/access-and-security/security-groups/#allowing-ssh","title":"Allowing SSH","text":"<p>To allow access to your VM for things like SSH, you will need to create a security group and add rules to it.</p> <p>Click on \"Create Security Group\". Give your new group a name, and a brief description.</p> <p></p> <p>You will see some existing rules:</p> <p></p> <p>Let's create the new rule to allow SSH. Click on \"Add Rule\".</p> <p>You will see there are a lot of options you can configure on the Add Rule dialog box.</p> <p>To check all available Rule</p> <p>You can choose the desired rule template as shown under Rule dropdown options. This will automatically select the Port required for the selected custom rule.</p> <p></p> <p></p> <p>Enter the following values:</p> <ul> <li> <p>Rule: SSH</p> </li> <li> <p>Remote: CIDR</p> </li> <li> <p>CIDR: 0.0.0.0/0</p> <p>Note</p> <p>To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box.</p> </li> </ul> <p>The new rule now appears in the list. This signifies that any instances using this newly added Security Group will now have SSH port 22 open for requests from any IP address.</p> <p></p>"},{"location":"openstack/access-and-security/security-groups/#allowing-ping","title":"Allowing Ping","text":"<p>The default configuration blocks ping responses, so you will need to add an additional group and/or rule if you want your public IPs to respond to ping requests.</p> <p>Ping is ICMP traffic, so the easiest way to allow it is to add a new rule and choose \"ALL ICMP\" from the dropdown.</p> <p>In the Add Rule dialog box, enter the following values:</p> <ul> <li> <p>Rule: All ICMP</p> </li> <li> <p>Direction: Ingress</p> </li> <li> <p>Remote: CIDR</p> </li> <li> <p>CIDR: 0.0.0.0/0</p> </li> </ul> <p></p> <p>Instances will now accept all incoming ICMP packets.</p>"},{"location":"openstack/access-and-security/security-groups/#allowing-rdp","title":"Allowing RDP","text":"<p>To allow access to your VM for things like Remote Desktop Protocol (RDP), you will need to create a security group and add rules to it.</p> <p>Click on \"Create Security Group\". Give your new group a name, and a brief description.</p> <p></p> <p>You will see some existing rules:</p> <p></p> <p>Let's create the new rule to allow SSH. Click on \"Add Rule\".</p> <p>You will see there are a lot of options you can configure on the Add Rule dialog box.</p> <p>Choose \"RDP\" from the Rule dropdown option as shown below:</p> <p></p> <p>Enter the following values:</p> <ul> <li> <p>Rule: RDP</p> </li> <li> <p>Remote: CIDR</p> </li> <li> <p>CIDR: 0.0.0.0/0</p> <p>Note</p> <p>To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box.</p> </li> </ul> <p>The new rule now appears in the list. This signifies that any instances using this newly added Security Group will now have RDP port 3389 open for requests from any IP address.</p> <p></p>"},{"location":"openstack/access-and-security/security-groups/#editing-existing-security-group-and-adding-new-security-rules","title":"Editing Existing Security Group and Adding New Security Rules","text":"<ul> <li> <p>Navigate to Security Groups:</p> <p>Navigate to Project -&gt; Network -&gt; Security Groups.</p> </li> <li> <p>Select the Security Group:</p> <p>Choose the security group to which you want to add new rules.</p> </li> <li> <p>Add New Rule:</p> <p>Look for an option to add a new rule within the selected security group.</p> <p></p> <p>Specify the protocol, port range, and source/destination details for the new rule.</p> <p></p> </li> <li> <p>Save Changes:</p> <p>Save the changes to apply the new security rules to the selected security group.</p> </li> </ul> <p>Important Note</p> <p>Security group changes may take some time to propagate to the instances associated with the modified group. Ensure that new rules align with your network security requirements.</p>"},{"location":"openstack/access-and-security/security-groups/#update-security-groups-to-a-running-vm","title":"Update Security Group(s) to a running VM","text":"<p>If you want to attach/deattach any new Security Group(s) to a running VM after it was launched. First create all new Security Group(s) with all rules required as described here. Note that same Security Groups can be used by multiple VMs so don't create same or redundant Security Rules based Security Groups as there are Quota per project. Once have created all Security Groups, you can easily attach them with any existing VM(s). You can select the VM from Compute -&gt; Instances tab and then select \"Edit Security Groups\" as shown below:</p> <p></p> <p>Then select all Security Group(s) that you want to attach to this VM by clicking on \"+\" icon and then click \"Save\" as shown here:</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/","title":"Set up Domain names for your VMs","text":""},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#set-up-domain-names-for-your-vms","title":"Set up Domain names for your VMs","text":""},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#what-is-dns","title":"What is DNS?","text":"<p>The Domain Name System (DNS) is a ranked and distributed system for naming resources connected to a network, and works by storing various types of record, such as an IP address associated with a domain name.</p> <p>DNS simplifies the communication between computers and servers through a network and provides a user-friendly method for users to interact with and get the desired information.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#what-is-a-a-record","title":"What is a \"A record\"?","text":"<p>A record: The primary DNS record used to connect your domain to an IP address that directs visitors to your website.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#how-to-get-user-friendly-domain-names-hostnames-for-your-nerc-vms","title":"How to get user-friendly domain names (hostnames) for your NERC VMs?","text":"<p>NERC does not currently offer integrated domain name service management.</p> <p>You can use one of the following methods to configure name resolution (DNS) for your NERC's virtual instances.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#1-using-freely-available-free-dynamic-dns-services","title":"1. Using freely available free Dynamic DNS services","text":"<p>Get a free domain or host name from no-ip.com or other</p> <p>free Dynamic DNS services.</p> <p>Here we will describe how to use No-IP to configure dynamic DNS.</p> <p>Step 1: Create your No-IP Account.</p> <p></p> <p>During this process you can add your desired unique hostname with pre-existing domain name or you can choose to create your hostname later on.</p> <p></p> <p>Step 2: Confirm Your Account by verifing your email address.</p> <p></p> <p>Step 3: Log In to Your Account to view your dashboard.</p> <p></p> <p>Step 4: Add Floating IP of your instance to the Hostname.</p> <p>Click on \"Modify\" to add your own Floating IP attached to your NERC virtual instance.</p> <p></p> <p>Then, browse your host or domain name as you setup during registration or later i.e. http://nerc.hopto.org on above example.</p> <p>Easy video tutorial can be found here.</p> <p>Having a free option is great for quick demonstrate your project but this has the following restrictions:</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#2-using-nginx-proxy-manager","title":"2. Using Nginx Proxy Manager","text":"<p>You can setup Nginx Proxy Manager on one of your NERC VMs and then use this Nginx Proxy Manager as your gateway to forward to your other web based services.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#quick-setup","title":"Quick Setup","text":"<p>i. Launch a VM with a security group that has opened rule for port 80, 443 and 22 to enable SSH Port Forwarding, aka SSH Tunneling i.e. Local Port Forwarding into the VM.</p> <p>ii. SSH into your VM using your private key after attaching a Floating IP.</p> <p>iii. Install Docker and Docker-Compose based on your OS choice for your VM.</p> <p>iv. Create a <code>docker-compose.yml</code> file similar to this:</p> <pre><code>version: \"3\"\nservices:\n    app:\n        image: \"jc21/nginx-proxy-manager:latest\"\n        restart: unless-stopped\n        ports:\n            - \"80:80\"\n            - \"81:81\"\n            - \"443:443\"\n        volumes:\n            - ./data:/data\n            - ./letsencrypt:/etc/letsencrypt\n</code></pre> <p>v. Bring up your stack by running:</p> <pre><code>docker-compose up -d\n\n# If using docker-compose-plugin\ndocker compose up -d\n</code></pre> <p>vi. Once the docker container runs successfully, connect to it on Admin Web Port i.e. 81 opened for the admin interface via SSH Tunneling i.e. Local Port Forwarding from your local machine's terminal by running:</p> <p><code>ssh -N -L &lt;Your_Preferred_Port&gt;:localhost:81 &lt;User&gt;@&lt;Floating-IP&gt; -i &lt;Path_To_Your_Private_Key&gt;</code></p> <p>Here, you can choose any port that is available on your machine as <code>&lt;Your_Preferred_Port&gt;</code> and then VM's assigned Floating IP as <code>&lt;Floating-IP&gt;</code> and associated Private Key pair attached to the VM as <code>&lt;Path_To_Your_Private_Key&gt;</code>.</p> <p>For e.g. <code>ssh -N -L 8081:localhost:81 ubuntu@199.94.60.24 -i ~/.ssh/cloud.key</code></p> <p>vii. Once the SSH Tunneling is successful, log in to the Nginx Proxy Manager Admin UI on your web browser: <code>http://localhost:&lt;Your_Preferred_Port&gt;</code> i.e. <code>http://localhost:8081</code></p> <p>Information</p> <p>It may take some time to spin up the Admin UI. Your terminal running the SSH Tunneling i.e. Local Port Forwarding will not show any logs or output when successfully done. Also your should not close or terminate the terminal while runnng the tunneling sessions and using the Admin UI.</p> <p>Default Admin User:</p> <pre><code>Email:    admin@example.com\nPassword: changeme\n</code></pre> <p>Immediately after logging in with this default user you will be asked to modify your admin details and change your password.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#how-to-create-a-proxy-host-with-lets-encrypt-ssl-certificate-attached-to-it","title":"How to create a Proxy Host with Let's Encrypt SSL Certificate attached to it","text":"<p>i. Click on Hosts &gt;&gt; Proxy Hosts, then click on \"Add Proxy Host\" button as shown below:</p> <p></p> <p>ii. On the popup box, enter your Domain Names (This need to be registed from your research institution or purchased on other third party vendor services and your have its administrative access)</p> <p>Important Note</p> <p>The Domain Name need to have an A Record pointing to the public floating IP of your NERC VM where you are hosting the Nginx Proxy Manager!</p> <p>Please fill out the following information on this popup box:</p> <ul> <li> <p>Scheme: http</p> </li> <li> <p>Forward Hostname/IP:     <code>&lt;The Private-IP of your NERC VM where you are hosting the web services&gt;</code></p> </li> <li> <p>Forward Port: <code>&lt;Port exposed on your VM to the public&gt;</code></p> </li> <li> <p>Enable all toggles i.e. Cache Assets, Block Common Exploits, Websockets Support</p> </li> <li> <p>Access List: Publicly Accessible</p> </li> </ul> <p>For your reference, you can review your selection should looks like below with your own Domain Name and other settings:</p> <p></p> <p>Also, select the SSL tab and then \"Request a new SSL Certificate\" with settings as shown below:</p> <p></p> <p>iii. Once Saved clicking the \"Save\" button. It should show you Status \"Online\" and when you click on the created Proxy Host link it will load the web services with https and domain name you defined i.e. <code>https://&lt;Your-Domain-Name&gt;</code>.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#3-using-your-local-research-computing-rc-department-or-academic-institutions-central-it-services","title":"3. Using your local Research Computing (RC) department or academic institution's Central IT services","text":"<p>You need to contact and work with your Research Computing department or academic institution's Central IT services to create A record for your hostname that maps to the address of a Floating IP of your NERC virtual instance.</p>"},{"location":"openstack/advanced-openstack-topics/domain-name-system/domain-names-for-your-vms/#4-using-commercial-dns-providers","title":"4. Using commercial DNS providers","text":"<p>Alternatively, you can purchase a fully registered domain name or host name from commercial hosting providers and then register DNS records for your virtual instance from commercial cloud servies i.e. AWS Route53, Azure DNS, CloudFlare, Google Cloud Platform, GoDaddy, etc.</p>"},{"location":"openstack/advanced-openstack-topics/python-sdk/python-SDK/","title":"Python SDK","text":""},{"location":"openstack/advanced-openstack-topics/python-sdk/python-SDK/#references","title":"References","text":"<p>Python SDK page at PyPi</p> <p>OpenStack Python SDK User Guide</p> <p>From the Python SDK page at Pypi:</p> <p>Definition</p> <p>Python SDK is a client library for building applications to work with OpenStack clouds. The project aims to provide a consistent and complete set of interactions with OpenStack's many services, along with complete documentation, examples, and tools.</p> <p>If you need to plug OpenStack into existing scripts using another language, there are a variety of other SDKs at various levels of active development.</p> <p>A list of known SDKs is maintained on the official OpenStack wiki. Known SDKs</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/create-a-router/","title":"Create a Router","text":""},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/create-a-router/#create-a-router","title":"Create a Router","text":"<p>A router acts as a gateway for external connectivity.</p> <p>By connecting your private network to the public network via a router, you can connect your instance to the Internet, install packages, etc. without needing to associate it with a public IP address.</p> <p>You can view routers by clicking Project, then click Network panel and choose Routers from the tabs that appears.</p> <p>Click \"Create Network\" button on the right side of the screen.</p> <p>In the Create Router dialog box, specify a name for the router.</p> <p>From the External Network dropdown, select the 'provider' network, and click \"Create Router\" button. This will set the Gateway for the new router to public network.</p> <p></p> <p>The new router is now displayed in the Routers tab. You should now see the router in the Network Topology view. (It also appears under Project -&gt; Network -&gt; Routers).</p> <p>Notice that it is now connected to the public network, but not your private network.</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/create-a-router/#set-internal-interface-on-the-router","title":"Set Internal Interface on the Router","text":"<p>In order to route between your private network and the outside world, you must give the router an interface on your private network.</p> <p>Perform the following steps in order to To connect a private network to the newly created router:</p> <p>a. On the Routers tab, click the name of the router.</p> <p></p> <p>b. On the Router Details page, click the Interfaces tab, then click Add Interface.</p> <p>c. In the Add Interface dialog box, select a Subnet.</p> <p></p> <p>Optionally, in the Add Interface dialog box, set an IP Address for the router interface for the selected subnet.</p> <p>If you choose not to set the IP Address value, then by default OpenStack Networking uses the first host IP address in the subnet.</p> <p>The Router Name and Router ID fields are automatically updated.</p> <p>d. Click \"Add Interface\".</p> <p>The Router will now appear connected to the private network in Network Topology tab.</p> <p></p> <p>OR,</p> <p>You can set Internal Interface on the Router From the Network Topology view, click on the router you just created, and click 'Add Interface' on the popup that appears.</p> <p></p> <p>Then, this will show Add Interface dialog box. So, you just complete steps b to c as mentioned above.</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/set-up-a-private-network/","title":"Set up a Private Network","text":""},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/set-up-a-private-network/#set-up-a-private-network","title":"Set up a Private Network","text":"<p>Default Network for your Project</p> <p>During your project setup, NERC will setup a default network, router and interface for your project that is ready-to-use.</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-a-network/set-up-a-private-network/#create-your-own-private-network","title":"Create Your Own Private Network","text":"<p>You can view/ create your/ existing network topology by clicking Project, then click Network panel and choose Network Topology from the tabs that appears. This shows public network which is accessible to all projects.</p> <p></p> <p>Click on \"Networks\" tab and then click \"Create Network\" button on the right side of the screen.</p> <p>In the Create Network dialog box, specify the following values.</p> <ul> <li> <p>Network tab:</p> <p>Network Name: Specify a name to identify the network.</p> <p>Admin State: The state to start the network in.</p> <p>Create Subnet: Select this checkbox to create a subnet</p> <p>Give your network a name, and leave the two checkboxes for \"Admin State\" and \"Create Subnet\" with the default settings.</p> <p></p> </li> <li> <p>Subnet tab:</p> <p>You do not have to specify a subnet when you create a network, but if you do not specify a subnet, the network can not be attached to an instance.</p> <p>Subnet Name: Specify a name for the subnet.</p> <p>Network Address: Specify the IP address for the subnet. For your private networks, you should use IP addresses which fall within the ranges that are specifically reserved for private networks:</p> <pre><code>10.0.0.0/8\n172.16.0.0/12\n192.168.0.0/16\n</code></pre> <p>In the example below, we configure a network containing addresses 192.168.0.1 to 192.168.0.255 using CIDR 192.168.0.0/24 Technically, your private network will still work if you choose any IP outside these ranges, but this causes problems with connecting to IPs in the outside world - so don't do it!</p> <p></p> <p>IP Version: Select IPv4 or IPv6.</p> <p>Gateway IP: Specify an IP address for a specific gateway. This parameter is optional.</p> <p>Disable Gateway: Select this checkbox to disable a gateway IP address.</p> </li> <li> <p>Subnet Details tab</p> <p>Enable DHCP: Select this checkbox to enable DHCP so that your VM instances will automatically be assigned an IP on the subnet.</p> <p>Allocation Pools: Specify IP address pools.</p> <p>DNS Name Servers: Specify a name for the DNS server. If you use '8.8.8.8' (you may recognize this as one of Google's public name servers).</p> <p>Host Routes: Specify the IP address of host routes.</p> <p>For now, you can leave the Allocation Pools and Host Routes boxes empty and click on \"Create\" button. But here we specify Allocation Pools of <code>192.168.0.2,192.168.0.254</code>.</p> <p></p> <p>The Network Topology should now show your virtual private network next to the public network.</p> <p></p> </li> </ul>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/","title":"Setting up custom images","text":""},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#virtual-machine-image-guide","title":"Virtual Machine Image Guide","text":"<p>An OpenStack Compute cloud needs to have virtual machine images in order to launch an instance. A virtual machine image is a single file which contains a virtual disk that has a bootable operating system installed on it.</p> <p>Very Important</p> <p>The provided Windows Server 2022 R2 image is for evaluation only. This evaluation edition expires in 180 days. This is intended to evaluate if the product is right for you. This is on user discretion to update, extend, and handle licensing issues for future usages.</p> <p>How to extend activation grace period for another 180 days?</p> <p>Remote desktop to your running Windows VM. Using the search function in your taskbar, look up Command Prompt. When you see it in the results, right-click on it and choose Run as Administrator. Your VM's current activation grace period can be reset by running: <code>slmgr -rearm</code>. Once this command is run successfully, restart your instance for the changes to take effect. This command typically resets the activation timer to 180 days and can be performed only for a limited number of times. For more about this read here.</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#existing-microsoft-windows-image","title":"Existing Microsoft Windows Image","text":"<p>Cloudbase Solutions provides Microsoft Windows Server 2022 R2 Standard Evaluation for OpenStack. This includes the required support for hypervisor-specific drivers (Hyper-V / KVM). Also integrated are the guest initialization tools (Cloudbase-Init), security updates, proper performance, and security configurations as well as the final Sysprep.</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#how-to-build-and-upload-your-custom-microsoft-windows-image","title":"How to Build and Upload your custom Microsoft Windows Image","text":"<p>Overall Process</p> <p>To create a new image, you will need the installation CD or DVD ISO file for the guest operating system. You will also need access to a virtualization tool. You can use KVM hypervisor for this. Or, if you have a GUI desktop virtualization tool (such as, virt-manager, VMware Fusion or VirtualBox), you can use that instead. Convert the file to QCOW2 (KVM, Xen) once you are done.</p> <p>You can customize and build the new image manually on your own system and then upload the image to the NERC's OpenStack Compute cloud. Please follow the following steps which describes how to obtain, create, and modify virtual machine images that are compatible with the NERC's OpenStack.</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#1-prerequisite","title":"1. Prerequisite","text":"<p>Follow these steps to prepare the installation</p> <p>a. Download a Windows Server 2022 installation ISO file. Evaluation images are available on the Microsoft website (registration required).</p> <p>b. Download the signed VirtIO drivers ISO file from the Fedora website.</p> <p>c. Install Virtual Machine Manager on your local Windows 10 machine using WSL:</p> <ul> <li> <p>Enable WSL on your local Windows 10 subsystem for Linux:</p> <p>The steps given here are straightforward, however, before following them make sure on Windows 10, you have WSL enabled and have at least Ubuntu 20.04 or above LTS version running over it. If you don't know how to do that then see our tutorial on how to enable WSL and install Ubuntu over it.</p> </li> <li> <p>Download and install MobaXterm:</p> <p>MobaXterm is a free application that can be downloaded using this link. After downloading, install it like any other normal Windows software.</p> </li> <li> <p>Open MobaXterm and run WSL Linux:</p> <p>As you open this advanced terminal for Windows 10, WSL installed Ubuntu app will show on the left side panel of it. Double click on that to start the WSL session.</p> <p></p> </li> <li> <p>Install Virt-Manager:</p> <pre><code>sudo apt update\nsudo apt install virt-manager\n</code></pre> </li> <li> <p>Run Virtual Machine Manager:</p> <p>Start the Virtual Machine Manager running this command on the opened terminal: <code>virt-manager</code> as shown below:</p> <p></p> <p>This will open Virt-Manager as following:</p> <p></p> </li> <li> <p>Connect QEMU/KVM user session on Virt-Manager:</p> <p></p> <p></p> <p></p> </li> </ul>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#2-create-a-virtual-machine","title":"2. Create a virtual machine","text":"<p>Create a virtual machine with the storage set to a 15 GB qcow2 disk image using Virtual Machine Manager</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Please set 15 GB disk image size as shown below:</p> <p></p> <p>Set the virtual machine name and also make sure \"Customize configuration before install\" is selected as shown below:</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#3-customize-the-virtual-machine","title":"3. Customize the Virtual machine","text":"<p>Enable the VirtIO driver. By default, the Windows installer does not detect the disk.</p> <p></p> <p></p> <p>Click Add Hardware &gt; select CDROM device and attach to downloaded virtio-win-* ISO file:</p> <p></p> <p></p> <p></p> <p>Make sure the NIC is using the virtio Device model as shown below:</p> <p></p> <p></p> <p>Make sure to set proper order of Boot Options as shown below, so that CDROM with Windows ISO is set on the first and Apply the order change. After this please begin windows installation by clicking on \"Begin Installation\" button.</p> <p></p> <p>Click \"Apply\" button.</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#4-continue-with-the-windows-installation","title":"4. Continue with the Windows installation","text":"<p>You need to continue with the Windows installation process.</p> <p>When prompted you can choose \"Windows Server 2022 Standard Evaluation (Desktop Experinece)\" option as shown below:</p> <p></p> <p></p> <p>Load VirtIO SCSI drivers and network drivers by choosing an installation target when prompted. Click Load driver and browse the file system.</p> <p></p> <p></p> <p></p> <p>Select the <code>E:\\virtio-win-*\\viostor\\2k22\\amd64</code> folder. When converting an image file with Windows, ensure the virtio driver is installed. Otherwise, you will get a blue screen when launching the image due to lack of the virtio driver.</p> <p></p> <p>The Windows installer displays a list of drivers to install. Select the VirtIO SCSI drivers.</p> <p></p> <p>Click Load driver again and browse the file system, and select the <code>E:\\NetKVM\\2k22\\amd64</code> folder.</p> <p></p> <p>Select the network drivers, and continue the installation.</p> <p></p> <p></p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#5-restart-the-installed-virtual-machine-vm","title":"5. Restart the installed virtual machine (VM)","text":"<p>Once the installation is completed, the VM restarts</p> <p>Define a password for the Adminstrator when prompted and click on \"Finish\" button:</p> <p></p> <p>Send the \"Ctrl+Alt+Delete\" key using Send Key Menu, this will unlock the windows and then prompt login for the Administrator - please login using the password you set on previous step:</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#6-go-to-device-manager-and-install-all-unrecognized-devices","title":"6. Go to device manager and install all unrecognized devices","text":"<p>Similarly as shown above repeat and install all missing drivers.</p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#7-enable-remote-desktop-protocol-rdp-login","title":"7. Enable Remote Desktop Protocol (RDP) login","text":"<p>Explicitly enable RDP login and uncheck \"Require computers to use Network Level Authentication to connect\" option</p> <p></p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#8-delete-the-recovery-parition","title":"8. Delete the recovery parition","text":"<p>Delete the recovery parition which will allow expanding the Image as required running the following commands on Command Prompt (Run as Adminstrator)</p> <pre><code>diskpart\nselect disk 0\nlist partition\nselect partition 3\ndelete partition override\nlist partition\n</code></pre> <p></p> <p>and then extend <code>C:</code> drive to take up the remaining space using \"Disk Management\".</p> <p></p> <p></p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#9-install-any-new-windows-updates-optional","title":"9. Install any new Windows updates. (Optional)","text":""},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#10-setup-cloudbase-init-to-generate-qcow2-image","title":"10. Setup cloudbase-init to generate QCOW2 image","text":"<p>Download and install stable version of cloudbase-init (A Windows project providing guest initialization features, similar to cloud-init) by browsing the Download Page on the web browser on virtual machine running Windows, you can escape registering and just click on \"No. just show me the downloads\" to navigate to the download page as shown below:</p> <p></p> <p>During Installation, set Serial port for logging to COM1 as shown below:</p> <p></p> <p>When the installation is done, in the Complete the Cloudbase-Init Setup Wizard window, select the Run Sysprep and Shutdown checkboxes and click \"Finish\" as shown below:</p> <p></p> <p>Wait for the machine to shutdown.</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#11-where-is-the-newly-generated-qcow2-image","title":"11. Where is the newly generated QCOW2 image?","text":"<p>The Sysprep will generate QCOW2 image i.e. <code>win2k22.qcow2</code> on <code>/home/&lt;YourUserName&gt;/.local/share/libvirt/images/</code></p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#12-create-openstack-image-and-push-to-nercs-image-list","title":"12. Create OpenStack image and push to NERC's image list","text":"<p>You can copy/download this windows image to the folder where you configured your OpenStack CLI as described Here and upload to the NERC's OpenStack running the following OpenStack Image API command:</p> <pre><code>openstack image create --disk-format qcow2 --file win2k22.qcow2 MS-Windows-2022\n</code></pre> <p>You can verify the uploaded image is available by running:</p> <pre><code>openstack image list\n\n+--------------------------------------+---------------------+--------+\n| ID                                   | Name                | Status |\n+--------------------------------------+---------------------+--------+\n| a9b48e65-0cf9-413a-8215-81439cd63966 | MS-Windows-2022     | active |\n| ...                                  | ...                 | ...    |\n+--------------------------------------+---------------------+--------+\n</code></pre>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#13-launch-an-instance-using-newly-uploaded-ms-windows-2022-image","title":"13. Launch an instance using newly uploaded MS-Windows-2022 image","text":"<p>Login to the NERC's OpenStack and verify the uploaded MS-Windows-2022 is there also available on the NERC's OpenStack Images List for your project as shown below:</p> <p></p> <p>Create a Volume using that Windows Image:</p> <p></p> <p></p> <p>Once successfully Volume is created, we can use the Volume to launch an instance as shown below:</p> <p></p> <p>Add other information and setup a Security Group that allows RDP (port: 3389) as shown below:</p> <p></p> <p>After some time the instance will be Active in Running state as shown below:</p> <p></p> <p>Attach a Floating IP to your instance:</p> <p></p> <p>More About Floating IP</p> <p>If you don't have any available floating IPs, please refer to this documentation on how to allocate a new Floating IP to your project.</p> <p>Click on detail view of the Instance and then click on Console tab menu and click on \"Send CtrlAltDel\" button located on the top right side of the console as shown below:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"openstack/advanced-openstack-topics/setting-up-your-own-images/how-to-build-windows-image/#14-how-to-have-remote-desktop-login-to-your-windows-instance","title":"14. How to have Remote Desktop login to your Windows instance","text":"<p>Remote Desktop login should work with the Floating IP associated with the instance:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>For more detailed information about OpenStack's image management, the OpenStack image creation guide provides further references and details.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/","title":"Terraform on NERC","text":""},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#provisioning-the-nerc-resources-using-terraform","title":"Provisioning the NERC resources using Terraform","text":"<p>Terraform is an open-source Infrastructure as Code (IaC) software tool that works with NERC and allows you to orchestrate, provision, and manage infrastructure resources quickly and easily. Terraform codifies cloud application programming interfaces (APIs) into human-readable, declarative configuration (*.tf) files. These files are used to manage underlying infrastructure rather than through NERC's web-based graphical interface - Horizon. Terraform allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. Terraform's main job is to create, modify, and destroy compute instances, private networks and other NERC resources.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#benefits-of-terraform","title":"Benefits of Terraform","text":"<p>If you have multiple instances/ VMs you are managing for your work or research, it can be simpler and more reproducible if you are doing it with automation tool like Terraform.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#installing-terraform","title":"Installing Terraform","text":"<p>To use Terraform you will need to install it from here.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#basic-template-to-use-terraform-on-your-nerc-project","title":"Basic Template to use Terraform on your NERC Project","text":"<p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/terraform-nerc.git\n</code></pre> <p>Then run this base template for terraform to provision some basic NERC's OpenStack resources within this repo.</p> <p>Note</p> <p>The <code>main</code> branch of this git repo should be a good starting point in developing your own terraform code.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#template-to-setup-r-shiny-server-using-terraform-on-your-nerc-project","title":"Template to setup R Shiny server using Terraform on your NERC Project","text":"<p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/terraform-nerc-r-shiny.git\n</code></pre> <p>Then you can run this template locally using terraform to provision R Shiny server on NERC's OpenStack resources within this repo.</p> <p>Important Note</p> <p>Please make sure to review bash script file i.e. <code>install-R-Shiny-&lt;OS&gt;.sh</code> located in this repo that is pointing as <code>user-data-path</code> variable in <code>example.tfvars</code>. This repo includes the script required to setup the R Shiny Server. You can use similar concept to any other project that needs custom user defined scripts while launching an instance. If you want to change and update this script you can just change this file and then run <code>terraform plan</code> and <code>terraform apply</code> command pointing this <code>example.tfvars</code> file.</p> <p>Which R Shiny Script i.e. <code>install-R-Shiny-&lt;OS&gt;.sh</code> to Choose?</p> <p>Please use the appropriate bash script file i.e. <code>install-R-Shiny-&lt;OS&gt;.sh</code> based on your operating system (OS):</p> <ul> <li> <p>AlmaLinux -&gt; <code>install-R-Shiny-AlmaLinux.sh</code></p> </li> <li> <p>CentOS -&gt; <code>install-R-Shiny-Centos.sh</code></p> </li> <li> <p>Ubuntu -&gt; <code>install-R-Shiny-Ubuntu.sh</code></p> </li> </ul>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#how-terraform-works","title":"How Terraform Works","text":"<p>Terraform reads configuration files and provides an execution plan of changes, which can be reviewed for safety and then applied and provisioned. Terraform reads all files with the extension .tf in your current directory. Resources can be in a single file, or organised across several different files.</p> <p>The basic Terraform deployment workflow is:</p> <p>i. Scope - Identify the infrastructure for your project.</p> <p>ii. Author - Write the configuration for your infrastructure in which you declare the elements of your infrastructure that you want to create.</p> <p>The format of the resource definition is straightforward and looks like this:</p> <pre><code>resource type_of_resource \"resource name\" {\n    attribute = \"attribue value\"\n    ...\n}\n</code></pre> <p>iii. Initialize - Install the plugins Terraform needs to manage the infrastructure.</p> <p>iv. Plan - Preview the changes Terraform will make to match your configuration.</p> <p>v. Apply - Make the planned changes.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#running-terraform","title":"Running Terraform","text":"<p>The Terraform deployment workflow on the NERC looks like this:</p> <p></p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>You can download the \"NERC's OpenStack RC File\" with the credentials for    your NERC project from the NERC's OpenStack dashboard.    Then you need to source that RC file using: <code>source *-openrc.sh</code>. You can    read here    on how to do this.</p> </li> <li> <p>Setup SSH key pairs running <code>ssh-keygen -t rsa -f username-keypair</code> and then    make sure the newly generated SSH key pairs exist on your <code>~/.ssh</code> folder.</p> </li> </ol>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#terraform-init","title":"Terraform Init","text":"<p>The first command that should be run after writing a new Terraform configuration or cloning an existing one is <code>terraform init</code>. This command is used to initialize a working directory containing Terraform configuration files and install the plugins.</p> <p>Information</p> <p>You will need to run <code>terraform init</code> if you make any changes to providers.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#terraform-plan","title":"Terraform Plan","text":"<p><code>terraform plan</code> command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure based on your configuration files.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#terraform-apply","title":"Terraform Apply","text":"<p>When you use <code>terraform apply</code> without passing it a saved plan file, it incorporates the <code>terraform plan</code> command functionality and so the planning options are also available while running this command.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#input-variables-on-the-command-line","title":"Input Variables on the Command Line","text":"<p>You can use the <code>-var 'NAME=VALUE'</code> command line option to specify values for input variables declared in your root module for e.g. <code>terraform plan -var 'name=value'</code></p> <p>In most cases, it will be more convenient to set values for potentially many input variables declared in the root module of the configuration, using definitions from a \"tfvars\" file and use it using <code>-var-file=FILENAME</code> command for e.g. <code>terraform plan -var-file=FILENAME</code></p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#track-your-infrastructure-and-collaborate","title":"Track your infrastructure and Collaborate","text":"<p>Terraform keeps track of your real infrastructure in a state file, which acts as a source of truth for your environment. Terraform uses the state file to determine the changes to make to your infrastructure so that it will match your configuration. Terraform's state allows you to track resource changes throughout your deployments. You can securely share your state with your teammates, provide a stable environment for Terraform to run in, and prevent race conditions when multiple people make configuration changes at once.</p>"},{"location":"openstack/advanced-openstack-topics/terraform/terraform-on-NERC/#some-useful-terraform-commands","title":"Some useful Terraform commands","text":"<pre><code>terraform init\n\nterraform fmt\n\nterraform validate\n\nterraform plan\n\nterraform apply\n\nterraform show\n\nterraform destroy\n\nterraform output\n</code></pre>"},{"location":"openstack/backup/backup-with-snapshots/","title":"Backup your instance & data","text":""},{"location":"openstack/backup/backup-with-snapshots/#backup-with-snapshots","title":"Backup with snapshots","text":"<p>When you start a new instance, you can choose the Instance Boot Source from the following list:</p> <ul> <li> <p>boot from image</p> </li> <li> <p>boot from instance snapshot</p> </li> <li> <p>boot from volume</p> </li> <li> <p>boot from volume snapshot</p> </li> </ul> <p>In its default configuration, when the instance is launched from an Image or an Instance Snapshot, the choice for utilizing persistent storage is configured by selecting the Yes option for \"Create New Volume\". Additionally, the \"Delete Volume on Instance Delete\" setting is pre-set to No, as indicated here:</p> <p></p> <p>Very Important: How do you make your VM setup and data persistent?</p> <p>For more in-depth information on making your VM setup and data persistent, you can explore the details here.</p>"},{"location":"openstack/backup/backup-with-snapshots/#create-and-use-instance-snapshots","title":"Create and use Instance snapshots","text":"<p>The OpenStack snapshot mechanism allows you to create new images from your instances while they are either running or stopped. An instance snapshot captures the current state of a running VM along with its storage, configuration, and memory. It includes the VM's disk image, memory state, and any configuration settings. Useful for preserving the entire state of a VM, including its running processes and in-memory data.</p> <p>This mainly serves two purposes:</p> <ul> <li> <p>As a backup mechanism: save the main disk of your instance to an image in     Horizon dashboard under Project -&gt; Compute -&gt; Images and later boot a new instance     from this image with the saved data.</p> </li> <li> <p>As a templating mechanism: customise and upgrade a base image and save it to     use as a template for new instances.</p> </li> </ul> <p>Considerations: using Instance snapshots</p> <p>It consumes more storage space due to including memory state. So, make sure your resource allocations for Storage is sufficient to hold all. They are suitable for scenarios where maintaining the exact VM state is crucial. The creation time of instance snapshot will be proportional to the size of the VM state.</p>"},{"location":"openstack/backup/backup-with-snapshots/#how-to-create-an-instance-snapshot","title":"How to create an instance snapshot","text":""},{"location":"openstack/backup/backup-with-snapshots/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul> <p>To snapshot an instance to an image using the CLI, do this:</p>"},{"location":"openstack/backup/backup-with-snapshots/#using-the-openstack-client","title":"Using the openstack client","text":"<pre><code>openstack server image create --name &lt;name of my snapshot&gt; --wait &lt;instance name or uuid&gt;\n</code></pre>"},{"location":"openstack/backup/backup-with-snapshots/#to-view-newly-created-snapshot-image","title":"To view newly created snapshot image","text":"<pre><code>openstack image show --fit-width &lt;name of my snapshot&gt;\n</code></pre> <p>Using this snapshot, the VM can be rolled back to the previous state with a server rebuild.</p> <pre><code>openstack server rebuild --image &lt;name of my snapshot&gt; &lt;existing instance name or uuid&gt;\n</code></pre> <p>For e.g.</p> <pre><code>openstack server image create --name my-snapshot --wait test-nerc-0\n\nopenstack image show --fit-width my-snapshot\n\nopenstack server rebuild --image my-snapshot test-nerc-0\n</code></pre> <p>Important Information</p> <p>During the time it takes to do the snapshot, the machine can become unresponsive.</p>"},{"location":"openstack/backup/backup-with-snapshots/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard, you can create a snapshot via the \"Compute -&gt; Instances\" page by clicking on the \"Create snapshot\" action button on desired instance as shown below:</p> <p></p> <p></p> <p>Live snapshots and data consistency</p> <p>We call a snapshot taken against a running instance with no downtime a \"live snapshot\". These snapshots are simply disk-only snapshots, and may be inconsistent if the instance's OS is not aware of the snapshot being taken. This is why we highly recommend, if possible, to Shut Off the instance before creating snapshots.</p>"},{"location":"openstack/backup/backup-with-snapshots/#how-to-restore-from-instance-snapshot","title":"How to restore from Instance snapshot","text":"<p>Once created, you can find the image listed under Images in the Horizon dashboard.</p> <p>Navigate to Project -&gt; Compute -&gt; Images.</p> <p></p> <p>You have the option to launch this image as a new instance, or by clicking on the arrow next to Launch, create a volume from the image, edit details about the image, update the image metadata, or delete it:</p> <p></p> <p>You can then select the snapshot when creating a new instance or directly click \"Launch\" button to use the snapshot image to launch a new instance.</p>"},{"location":"openstack/backup/backup-with-snapshots/#take-and-use-volume-snapshots","title":"Take and use Volume Snapshots","text":""},{"location":"openstack/backup/backup-with-snapshots/#volume-snapshots","title":"Volume snapshots","text":"<p>You can also create snapshots of a volume, that then later can be used to create other volumes or to rollback to a precedent point in time. You can take a snapshot of volume that may or may not be attached to an instance. Snapshot of available volumes or volumes that are not attached to an instance does not affect the data on the volume. Snapshot of a volume serves as a backup for the persistent data on the volume at a given point in time. Snapshots are of the size of the actual data existing on the volume at the time at which the snapshot is taken. Volume snapshots are pointers in the RW history of a volume. The creation of a snapshot takes a few seconds and it can be done while the volume is in-use.</p> <p>Warning</p> <p>Taking snapshots of volumes that are in use or attached to active instances can result in data inconsistency on the volume. This is why we highly recommend, if possible, to Shut Off the instance before creating snapshots.</p> <p>Once you have the snapshot, you can use it to create other volumes based on this snapshot. Creation time for these volumes may depend on the type of the volume you are creating as it may entitle some data transfer. But this is efficient for backup and recovery of specific data without the need for the complete VM state. Also, it consumes less storage space compared to instance snapshots.</p>"},{"location":"openstack/backup/backup-with-snapshots/#how-to-create-a-volume-snapshot","title":"How to create a volume snapshot","text":""},{"location":"openstack/backup/backup-with-snapshots/#using-the-openstack-cli","title":"Using the OpenStack CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul> <p>To snapshot an instance to an image using the CLI, do this:</p>"},{"location":"openstack/backup/backup-with-snapshots/#using-the-openstack-client-commands","title":"Using the openstack client commands","text":"<pre><code>openstack volume snapshot create --volume &lt;volume name or uuid&gt; &lt;name of my snapshot&gt;\n</code></pre> <p>For e.g.</p> <pre><code>openstack volume snapshot create --volume test_volume my-volume-snapshot\n+-------------+--------------------------------------+\n| Field       | Value                                |\n+-------------+--------------------------------------+\n| created_at  | 2022-04-12T19:48:42.707250           |\n| description | None                                 |\n| id          | f1cf6846-4aba-4eb8-b3e4-2ff309f8f599 |\n| name        | my-volume-snapshot                   |\n| properties  |                                      |\n| size        | 25                                   |\n| status      | creating                             |\n| updated_at  | None                                 |\n| volume_id   | f2630d21-f8f5-4f02-adc7-14a3aa72cc9d |\n+-------------+--------------------------------------+\n</code></pre> <p>Important Information</p> <p>if the volume is in-use, you may need to specify <code>--force</code></p> <p>You can list the volume snapshots with the following command.</p> <pre><code>openstack volume snapshot list\n</code></pre> <p>For e.g.</p> <pre><code>openstack volume snapshot list\n+--------------------------------------+--------------------+-------------+-----------+------+\n| ID                                   | Name               | Description | Status    | Size |\n+--------------------------------------+--------------------+-------------+-----------+------+\n| f1cf6846-4aba-4eb8-b3e4-2ff309f8f599 | my-volume-snapshot | None        | available |   25 |\n+--------------------------------------+--------------------+-------------+-----------+------+\n</code></pre> <p>Once the volume snapshot is in available state, then you can create other volumes based on that snapshot. You don't need to specify the size of the volume, it will use the size of the snapshot.</p> <pre><code>openstack volume create --description --source &lt;name of my snapshot&gt; \"Volume from an snapshot\" &lt;volume name or uuid&gt;\n</code></pre> <p>You can delete the snapshots just by issuing the following command</p> <pre><code>openstack volume snapshot delete &lt;name of my snapshot&gt;\n</code></pre> <p>For e.g.</p> <pre><code>openstack volume snapshot delete my-volume-snapshot\n</code></pre>"},{"location":"openstack/backup/backup-with-snapshots/#using-nercs-horizon-dashboard","title":"Using NERC's Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard, you can create a snapshot via the \"Volumes\" menu by clicking on the \"Create Snapshot\" action button on desired volume as shown below:</p> <p></p> <p>In the dialog box that opens, enter a snapshot name and a brief description.</p> <p></p>"},{"location":"openstack/backup/backup-with-snapshots/#how-to-restore-from-volume-snapshot","title":"How to restore from Volume snapshot","text":"<p>Once a snapshot is created and is in \"Available\" status, you can view and manage it under the Volumes menu in the Horizon dashboard under Volume Snapshots.</p> <p>Navigate to Project -&gt; Volumes -&gt; Snapshots.</p> <p></p> <p>You have the option to directly launch this volume as an instance by clicking on the arrow next to \"Create Volume\" and selecting \"Launch as Instance\".</p> <p></p> <p>Also it has other options i.e. to create a volume from the snapshot, edit details about the snapshot, delete it, or Update the snapshot metadata.</p> <p>Here, we will first Create Volume from Snapshot by clicking \"Create Volume\" button as shown below:</p> <p></p> <p>In the dialog box that opens, enter a volume name and a brief description.</p> <p></p> <p>Any snapshots made into volumes can be found under Volumes:</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p></p> <p>Then using this newly created volume, you can launch it as an instance by clicking on the arrow next to \"Edit Volume\" and selecting \"Launch as Instance\" as shown below:</p> <p></p> <p>Very Important: Requested/Approved Allocated Storage Quota and Cost</p> <p>Please remember that any volumes and snapshots stored will consume your Storage quotas, which represent the storage space allocated to your project. For NERC (OpenStack) Resource Allocations, storage quotas are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes. You can delete any volumes and snapshots that are no longer needed to free up space. However, even if you delete volumes and snapshots, you will still be billed based on your approved and reserved storage allocation, which reserves storage from the total NESE storage pool.</p> <p>If you request additional storage by specifying a changed quota value for the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes through NERC's ColdFront interface, invoicing for the extra storage will take place upon fulfillment or approval of your request, as explained in our Billing FAQs.</p> <p>Conversely, if you request a reduction in the Storage quotas, specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\", through a change request using ColdFront, your invoicing will be adjusted accordingly when the request is submitted.</p> <p>In both scenarios, 'invoicing' refers to the accumulation of hours corresponding to the added or removed storage quantity.</p> <p>Help Regarding Billing</p> <p>Please send your questions or concerns regarding Storage and Cost by emailing us at help@nerc.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System.</p>"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/","title":"Assign a Floating IP","text":""},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#assign-a-floating-ip","title":"Assign a Floating IP","text":"<p>When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated.</p> <p>However, in addition to the fixed IP address, a Floating IP address can also be attached to an instance. Unlike fixed IP addresses, Floating IP addresses can have their associations modified at any time, regardless of the state of the instances involved. Floating IPs are a limited resource, so your project will have a quota based on its needs. You should only assign public IPs to VMs that need them. This procedure details the reservation of a Floating IP address from an existing pool of addresses and the association of that address with a specific instance.</p> <p>By attaching a Floating IP to your instance, you can ssh into your vm from your local machine.</p> <p>Make sure you are using key forwarding as described in Create a Key Pair.</p>"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#allocate-a-floating-ip","title":"Allocate a Floating IP","text":"<p>Navigate to Project -&gt; Compute -&gt; Instances.</p> <p>Next to Instance Name -&gt; Click Actions dropdown arrow (far right) -&gt; Choose Associate Floating IP</p> <p></p> <p>If you have some floating IPs already allocated to your project which are not yet associated with a VM, they will be available in the dropdown list on this screen.</p> <p></p> <p>If you have no floating IPs allocated, or all your allocated IPs are in use already, the dropdown list will be empty.</p> <p></p> <p>Click the \"+\" icon to allocate an IP. You will see the following screen.</p> <p></p> <p>Make sure 'provider' appears in the dropdown menu, and that you have not already met your quota of allocated IPs.</p> <p>In this example, the project has a quota of 50 floating IPs, but we have allocated 5 so far, so we can still allocate up to next 45 Floating IPs.</p> <p>Click \"Allocate IP\".</p> <p>You will get a green \"success\" popup in the top right corner that shows your public IP address and that is listed as option to choose from \"IP Address\" dropdown list.</p> <p></p> <p>You will be able to select between multiple Floating IPs under \"IP Address\" dropdown and any unassociated VMs from \"Port to be associated\" dropdown options:</p> <p></p> <p>Now click on \"Associate\" button.</p> <p>Then, a green \"success\" popup in the top left and you can see the Floating IP is attached to your VM on the Instances page:</p> <p></p> <p>Floating IP Quota Exceed</p> <p>If you have already exceed your quota, you will get a red error message saying \"You are already using all of your available floating IPs\" as shown below:</p> <p></p> <p>NOTE: By default, each approved project is provided with only 2 OpenStack Floating IPs, regardless of the units requested in the quota, as described here. Your PI or Project Manager(s) can adjust the quota and request additional Floating IPs as needed, following this documentation. This is controlled by the \"OpenStack Floating IP Quota\" attribute.</p>"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#disassociate-a-floating-ip","title":"Disassociate a Floating IP","text":"<p>You may need to disassociate a Floating IP from an instance which no longer needs it, so you can assign it to one that does.</p> <p>Navigate to Project -&gt; Compute -&gt; Instances.</p> <p>Find the instance you want to remove the IP from in the list. Click the red \"Disassociate Floating IP\" to the right.</p> <p>This IP will be disassociated from the instance, but it will still remain allocated to your project.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#release-a-floating-ip","title":"Release a Floating IP","text":"<p>You may discover that your project does not need all the floating IPs that are allocated to it.</p> <p>We can release a Floating IP while disassociating it just we need to check the \"Release Floating IP\" option as shown here:</p> <p></p> <p>OR,</p> <p>Navigate to Project -&gt; Network -&gt; Floating IPs.</p> <p>To release the Floating IP address back into the Floating IP pool, click the Release Floating IP option in the Actions column.</p> <p></p> <p>Pro Tip</p> <p>You can also choose multiple Floating IPs and release them all at once.</p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/","title":"Create a Windows VM","text":""},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#create-a-windows-virtual-machine","title":"Create a Windows virtual machine","text":""},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#launch-an-instance-using-a-boot-volume","title":"Launch an Instance using a boot volume","text":"<p>In this example, we will illustrate how to utilize a boot volume to launch a Windows virtual machine, similar steps can be used on other types of virtual machines. The following steps show how to create a virtual machine which boots from an external volume:</p> <ul> <li> <p>Create a volume with source data from the image</p> </li> <li> <p>Launch a VM with that volume as the system disk</p> </li> </ul> <p>Recommendations</p> <ul> <li> <p>The recommended method for creating a Windows desktop virtual machine is to boot from volume. However, you can also launch a Windows-based instance using the standard boot from image method, as described here. When launching a Windows VM using boot from image, please ensure that you have configured a Security Group that allows RDP (port 3389), as shown below:</p> <p></p> </li> </ul> <p>To access the Windows VM, you must log in via Remote Desktop, as described here.</p> <p>To configure a password for the Administrator user account:</p> <ol> <li> <p>Go to the Configuration section during instance launch.</p> </li> <li> <p>Enter the following PowerShell-based custom script:</p> <pre><code>#ps1\n\nnet user Administrator '&lt;Your_Own_Admin_Password&gt;'\n</code></pre> </li> <li> <p>Replace <code>&lt;Your_Own_Admin_Password&gt;</code> with your desired password.</p> </li> </ol> <p>Note: Ensure that your script in the \"Configuration\" section resembles the format shown below:</p> <p></p> <p>This enables Remote Desktop login for the Administrator user account.</p> <ul> <li> <p>To ensure smooth upgrade and maintenance of the system, select at least 100 GiB for the size of the volume.</p> </li> <li> <p>Make sure your project has sufficient storage quotas.</p> </li> </ul>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#create-a-bootable-volume-from-the-existing-windows-image","title":"Create a Bootable Volume from the Existing Windows Image","text":""},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#1-using-nercs-horizon-dashboard","title":"1. Using NERC's Horizon dashboard","text":"<p>Navigate: Project -&gt; Compute -&gt; Images.</p> <p>Make sure you are able to see MS-Windows-2022 is available on Images List for your project as shown below:</p> <p></p> <p>Create a Volume using that Windows Image:</p> <p></p> <p>To ensure smooth upgrade and maintenance of the system, select at least 100 GiB for the size of the volume as shown below:</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#2-using-the-openstack-cli","title":"2. Using the OpenStack CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see     OpenStack Command Line setup     for more information.</li> </ul> <p>To create a volume from image using the CLI, do this:</p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#using-the-openstack-client-commands","title":"Using the openstack client commands","text":"<p>Identify the image for the initial volume contents from <code>openstack image list</code>.</p> <pre><code>openstack image list\n+--------------------------------------+---------------------+--------+\n| ID                                   | Name                | Status |\n+--------------------------------------+---------------------+--------+\n| a9b48e65-0cf9-413a-8215-81439cd63966 | MS-Windows-2022     | active |\n...\n+--------------------------------------+---------------------+--------+\n</code></pre> <p>In the example above, this is image id <code>a9b48e65-0cf9-413a-8215-81439cd63966</code> for <code>MS-Windows-2022</code>.</p> <p>Creating a disk from this image with a size of 100 GiB named \"my-volume\" as follows.</p> <pre><code>openstack volume create --image a9b48e65-0cf9-413a-8215-81439cd63966 --size 100 --description \"Using MS Windows Image\" my-volume\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | false                                |\n| consistencygroup_id | None                                 |\n| created_at          | 2024-02-03T23:38:50.000000           |\n| description         | Using MS Windows Image               |\n| encrypted           | False                                |\n| id                  | d8a5da4c-41c8-4c2d-b57a-8b6678ce4936 |\n| multiattach         | False                                |\n| name                | my-volume                            |\n| properties          |                                      |\n| replication_status  | None                                 |\n| size                | 100                                  |\n| snapshot_id         | None                                 |\n| source_volid        | None                                 |\n| status              | creating                             |\n| type                | tripleo                              |\n| updated_at          | None                                 |\n| user_id             | 938eb8bfc72e4cb3ad2b94e2eb4059f7     |\n+---------------------+--------------------------------------+\n</code></pre> <p>Checking the status again using <code>openstack volume show my-volume</code> will allow the volume creation to be followed.</p> <p>\"downloading\" means that the volume contents is being transferred from the image service to the volume service</p> <p>\"available\" means the volume can now be used for booting. A set of volume_image meta data is also copied from the image service.</p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#launch-instance-from-existing-bootable-volume","title":"Launch instance from existing Bootable Volume","text":""},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#1-using-horizon-dashboard","title":"1. Using Horizon dashboard","text":"<p>Navigate: Project -&gt; Volumes -&gt; Volumes.</p> <p>Once successfully Volume is created by following this, we can use the Bootable Volume to launch an Windows instance as shown below:</p> <p></p> <p>How do you make your VM setup and data persistent?</p> <p>Only one instance at a time can be booted from a given volume. Make sure \"Delete Volume on Instance Delete\" is selected as No if you want the volume to persist even after the instance is terminated, which is the default setting, as shown below:</p> <p></p> <p>NOTE: For more in-depth information on making your VM setup and data persistent, you can explore the details here.</p> <p>Add other information and setup a Security Group that allows RDP (port: 3389) as shown below:</p> <p></p> <p>Very Important: Setting Administrator Credentials to Log into Your VM.</p> <p>To access this Windows VM, you must log in using Remote Desktop, as described here. To configure a password for the \"Administrator\" user account, proceed to the \"Configuration\" section and enter the supplied PowerShell-based Customized Script. Make sure to substitute <code>&lt;Your_Own_Admin_Password&gt;</code> with your preferred password, which will enable Remote Desktop login to the Windows VM.</p> <pre><code>#ps1\n\nnet user Administrator '&lt;Your_Own_Admin_Password&gt;'\n</code></pre> <p>Please ensure that your script in the \"Configuration\" section resembles the following syntax:</p> <p></p> <p>After some time the instance will be Active in Running state as shown below:</p> <p></p> <p>Attach a Floating IP to your instance:</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#2-using-the-openstack-cli-from-the-terminal","title":"2. Using the OpenStack CLI from the terminal","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see     OpenStack Command Line setup     for more information.</li> </ul> <p>To launch an instance from existing bootable volume using the CLI, do this:</p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#using-the-openstack-client-commands-from-terminal","title":"Using the openstack client commands from terminal","text":"<p>Get the flavor name using <code>openstack flavor list</code>:</p> <pre><code>openstack flavor list | grep cpu-su.4\n| b3f5dded-efe3-4630-a988-2959b73eba70 | cpu-su.4      |  16384 |   20 |         0 |     4 | True      |\n</code></pre> <p>To access this Windows VM, you must log in using Remote Desktop, as described here. Before launching the VM using the OpenStack CLI, we'll prepare a PowerShell-based Customized Script as \"user-data\".</p> <p>What is a user data file?</p> <p>A user data file is a text file that you can include when running the <code>openstack server create</code> command. This file is used to customize your instance during boot.</p> <p>You can place user data in a local file and pass it through the <code>--user-data &lt;user-data-file&gt;</code> parameter at instance creation. You'll create a local file named <code>admin_password.ps1</code> with the following content. Please remember to replace <code>&lt;Your_Own_Admin_Password&gt;</code> with your chosen password, which will be used to log in to the Windows VM via Remote Desktop.</p> <pre><code>#ps1\n\nnet user Administrator '&lt;Your_Own_Admin_Password&gt;'\n</code></pre> <p>Setup a Security Group named \"rdp_test\" that allows RDP (port: 3389) using the CLI, use the command <code>openstack security group create &lt;group-name&gt;</code>:</p> <pre><code>openstack security group create --description 'Allows RDP' rdp_test\n\nopenstack security group rule create --protocol tcp --dst-port 3389 rdp_test\n</code></pre> <p>To create a Windows VM named \"my-vm\" using the specified parameters, including the flavor name \"cpu-su.4\", existing key pair \"my-key\", security group \"rdp_test\", user data from the file \"admin_password.ps1\" created above, and the volume with name \"my-volume\" created above, you can run the following command:</p> <pre><code>openstack server create --flavor cpu-su.4 \\\n    --key-name my-key \\\n    --security-group rdp_test \\\n    --user-data admin_password.ps1 \\\n    --volume my-volume \\\n    my-vm\n</code></pre> <p>To list all Floating IP addresses that are allocated to the current project, run:</p> <pre><code>openstack floating ip list\n\n+--------------------------------------+---------------------+------------------+------+\n| ID                                   | Floating IP Address | Fixed IP Address | Port |\n+--------------------------------------+---------------------+------------------+------+\n| 760963b2-779c-4a49-a50d-f073c1ca5b9e | 199.94.60.220       | 192.168.0.195    | None |\n+--------------------------------------+---------------------+------------------+------+\n</code></pre> <p>More About Floating IP</p> <p>If the above command returns an empty list, meaning you don't have any available floating IPs, please refer to this documentation on how to allocate a new Floating IP to your project.</p> <p>Attach a Floating IP to your instance:</p> <pre><code>openstack server add floating ip INSTANCE_NAME_OR_ID FLOATING_IP_ADDRESS\n</code></pre> <p>For example:</p> <pre><code>openstack server add floating ip my-vm 199.94.60.220\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#accessing-the-graphical-console-in-the-horizon-dashboard","title":"Accessing the graphical console in the Horizon dashboard","text":"<p>You can access the graphical console using the browser once the VM is in status ACTIVE. It can take up to 15 minutes to reach this state.</p> <p>The console is accessed by selecting the Instance Details for the machine and the 'Console' tab as shown below:</p> <p></p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#how-to-add-remote-desktop-login-to-your-windows-instance","title":"How to add Remote Desktop login to your Windows instance","text":"<p>When the build and the Windows installation steps have completed, you can access the console using the Windows Remote Desktop application. Remote Desktop login should work with the Floating IP associated with the instance:</p> <p></p> <p></p> <p></p> <p>What is the user login for Windows Server 2022?</p> <p>The default username is \"Administrator,\" and the password is the one you set using the user data PowerShell script during the launch.</p> <p></p> <p></p> <p>Storage and Volume</p> <ul> <li> <p>System disks are the first disk based on the flavor disk space and are generally used to store the operating system created from an image when the virtual machine is booted.</p> </li> <li> <p>Volumes are persistent virtualized block devices independent of any particular instance. Volumes may be attached to a single instance at a time, but may be detached or reattached to a different instance while retaining all data, much like a USB drive. The size of the volume can be selected when it is created within the storage quota limits for the particular resource allocation.</p> </li> </ul>"},{"location":"openstack/create-and-connect-to-the-VM/create-a-Windows-VM/#connect-additional-disk-using-volume","title":"Connect additional disk using volume","text":"<p>To attach additional disk to a running Windows machine you can follow this documentation. This guide provides instructions on formatting and mounting a volume as an attached disk within a Windows virtual machine.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/","title":"Available NOVA Flavors","text":""},{"location":"openstack/create-and-connect-to-the-VM/flavors/#nova-flavors","title":"Nova flavors","text":"<p>on NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server.</p> <p>Note</p> <p>Flavors are visible only while you are launching an instance and under \"Flavor\" tab as explained here.</p> <p>The important fields are</p> Field Description RAM Memory size in MiB Disk Size of disk in GiB Ephemeral Size of a second disk. 0 means no second disk is defined and mounted. VCPUs Number of virtual cores"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#comparison-between-cpu-and-gpu","title":"Comparison Between CPU and GPU","text":"<p>Here are the key differences between CPUs and GPUs:</p> CPUs GPUs Work mostly in sequence. While several cores and excellent task switching give the impression of parallelism, a CPU is fundamentally designed to run one task at a time. Are designed to work in parallel. A vast number of cores and threading managed in hardware enable GPUs to perform many simple calculations simultaneously. Are designed for task parallelism. Are designed for data parallelism. Have a small number of cores that can complete single complex tasks at very high speeds. Have a large number of cores that work in tandem to compute many simple tasks. Have access to a large amount of relatively slow RAM with low latency, optimizing them for latency (operation). Have access to a relatively small amount of very fast RAM with higher latency, optimizing them for throughput. Have a very versatile instruction set, allowing the execution of complex tasks in fewer cycles but creating overhead in others. Have a limited (but highly optimized) instruction set, allowing them to execute their designed tasks very efficiently. Task switching (as a result of running the OS) creates overhead. Task switching is not used; instead, numerous serial data streams are processed in parallel from point A to point B. Will always work for any given use case but may not provide adequate performance for some tasks. Would only be a valid choice for some use cases but would provide excellent performance in those cases. <p>In summary, for applications such as Machine Learning (ML), Artificial Intelligence (AI), or image processing, a GPU can provide a performance increase of 50x to 200x compared to a typical CPU performing the same tasks.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#currently-our-setup-supports-and-offers-the-following-flavors","title":"Currently, our setup supports and offers the following flavors","text":"<p>NERC offers the following flavors based on our Infrastructure-as-a-Service (IaaS) - OpenStack offerings (Tiers of Service).</p> <p>Pro Tip</p> <p>Choose a flavor for your instance from the available Tier that suits your requirements, use-cases, and budget when launching a VM as shown here.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#1-standard-compute-tier","title":"1. Standard Compute Tier","text":"<p>The standard compute flavor \"cpu-su\" is provided from Lenovo SD530 (2x Intel 8268 2.9 GHz, 48 cores, 384 GB memory) server. The base unit is 1 vCPU, 4 GB memory with default of 20 GB root disk at a rate of $0.013 / hr of wall time.</p> Flavor SUs GPU vCPU RAM(GiB) Storage(GiB) Cost / hr cpu-su.1 1 0 1 4 20 $0.013 cpu-su.2 2 0 2 8 20 $0.026 cpu-su.4 4 0 4 16 20 $0.052 cpu-su.8 8 0 8 32 20 $0.104 cpu-su.16 16 0 16 64 20 $0.208"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#2-memory-optimized-tier","title":"2. Memory Optimized Tier","text":"<p>The memory optimized flavor \"mem-su\" is provided from the same servers at \"cpu-su\" but with 8 GB of memory per core. The base unit is 1 vCPU, 8 GB memory with default of 20 GB root disk at a rate of $0.026 / hr of wall time.</p> Flavor SUs GPU vCPU RAM(GiB) Storage(GiB) Cost / hr mem-su.1 1 0 1 8 20 $0.026 mem-su.2 2 0 2 16 20 $0.052 mem-su.4 4 0 4 32 20 $0.104 mem-su.8 8 0 8 64 20 $0.208 mem-su.16 16 0 16 128 20 $0.416"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#3-gpu-tier","title":"3. GPU Tier","text":"<p>NERC also supports the most demanding workloads including Artificial Intelligence (AI), Machine Learning (ML) training and Deep Learning modeling, simulation, data analytics, data visualization, distributed databases, and more. For such demanding workloads, the NERC's GPU-based distributed computing flavor is recommended, which is integrated into a specialized hardware such as GPUs that produce unprecedented performance boosts for technical computing workloads.</p> <p>Guidelines for Utilizing GPU-Based Flavors in Active Resource Allocation</p> <p>To effectively utilize GPU-based flavors on any NERC (OpenStack) resource allocation, the Principal Investigator (PI) or project manager(s) must submit a change request for their currently active NERC (OpenStack) resource allocation. This request should specify the number of GPUs they intend to use by setting the \"OpenStack GPU Quota\" attribute. We recommend ensuring that this count accurately reflects the current GPU usage. Additionally, they need to adjust the quota values for \"OpenStack Compute RAM Quota (MiB)\" and \"OpenStack Compute vCPU Quota\" to sufficiently accommodate the GPU flavor they wish to use when launching a VM in their OpenStack Project.</p> <p>Once the change request is reviewed and approved by the NERC's admin, users will be able to select the appropriate GPU-based flavor during the flavor selection tab when launching a new VM.</p> <p>There are four different options within the GPU tier, featuring the newer NVIDIA A100 SXM4, NVIDIA A100s, NVIDIA V100s, and NVIDIA K80s.</p> <p>How can I get customized A100 SXM4 GPUs not listed in the current flavors?</p> <p>We also provide customized A100 SXM4 GPU-based flavors, which are not publicly listed on our NVIDIA A100 SXM4 40GB GPU Tiers list. These options are exclusively available for demanding projects and are subject to availability.</p> <p>To request access, please fill out this form. Our team will review your request and reach out to you to discuss further.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#i-nvidia-a100-sxm4-40gb","title":"i. NVIDIA A100 SXM4 40GB","text":"<p>The \"gpu-su-a100sxm4\" flavor is provided from Lenovo SD650-N V2 (2x Intel Xeon Platinum 8358 32C 250W 2.6GHz, 128 cores, 1024 GB RAM 4x NVIDIA HGX A100 40GB) servers. The higher number of tensor cores available can significantly enhance the speed of machine learning applications. The base unit is 32 vCPU, 240 GB memory with default of 20 GB root disk at a rate of $2.078 / hr of wall time.</p> Flavor SUs GPU vCPU RAM(GiB) Storage(GiB) Cost / hr gpu-su-a100sxm4.1 1 1 32 240 20 $2.078 gpu-su-a100sxm4.2 2 2 64 480 20 $4.156 <p>How to setup NVIDIA driver for \"gpu-su-a100sxm4\" flavor based VM?</p> <p>After launching a VM with an NVIDIA A100 SXM4 GPU flavor, you will need to setup the NVIDIA driver in order to use GPU-based codes and libraries. Please run the following commands to setup the NVIDIA driver and CUDA version required for these flavors in order to execute GPU-based codes. NOTE: These commands are ONLY applicable for the VM based on \"ubuntu-22.04-x86_64\" image. You might need to find corresponding packages for your own OS of choice.</p> <pre><code>sudo apt update\nsudo apt -y install nvidia-driver-495\n# Just click *Enter* if any popups appear!\n# Confirm and verify that you can see the NVIDIA device attached to your VM\nlspci | grep -i nvidia\n# 00:05.0 3D controller: NVIDIA Corporation GA100 [A100 SXM4 40GB] (rev a1)\nsudo reboot\n# SSH back to your VM and then you will be able to use nvidia-smi command\nnvidia-smi\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#ii-nvidia-a100-40gb","title":"ii. NVIDIA A100 40GB","text":"<p>The \"gpu-su-a100\" flavor is provided from Lenovo SR670 (2x Intel 8268 2.9 GHz, 48 cores, 384 GB memory, 4x NVIDIA A100 40GB) servers. These latest GPUs deliver industry-leading high throughput and low latency networking. The base unit is 24 vCPU, 74 GB memory with default of 20 GB root disk at a rate of $1.803 / hr of wall time.</p> Flavor SUs GPU vCPU RAM(GiB) Storage(GiB) Cost / hr gpu-su-a100.1 1 1 24 74 20 $1.803 gpu-su-a100.2 2 2 48 148 20 $3.606 <p>How to setup NVIDIA driver for \"gpu-su-a100\" flavor based VM?</p> <p>After launching a VM with an NVIDIA A100 GPU flavor, you will need to setup the NVIDIA driver in order to use GPU-based codes and libraries. Please run the following commands to setup the NVIDIA driver and CUDA version required for these flavors in order to execute GPU-based codes. NOTE: These commands are ONLY applicable for the VM based on \"ubuntu-22.04-x86_64\" image. You might need to find corresponding packages for your own OS of choice.</p> <pre><code>sudo apt update\nsudo apt -y install nvidia-driver-495\n# Just click *Enter* if any popups appear!\n# Confirm and verify that you can see the NVIDIA device attached to your VM\nlspci | grep -i nvidia\n# 0:05.0 3D controller: NVIDIA Corporation GA100 [A100 PCIe 40GB] (rev a1)\nsudo reboot\n# SSH back to your VM and then you will be able to use nvidia-smi command\nnvidia-smi\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#iii-nvidia-v100-32gb","title":"iii. NVIDIA V100 32GB","text":"<p>The \"gpu-su-v100\" flavor is provided from Dell R740xd (2x Intel Xeon Gold 6148, 40 cores, 768GB memory, 1x NVIDIA V100 32GB) servers. The base unit is 48 vCPU, 192 GB memory with default of 20 GB root disk at a rate of $1.214 / hr of wall time.</p> Flavor SUs GPU vCPU RAM(GiB) Storage(GiB) Cost / hr gpu-su-v100.1 1 1 48 192 20 $1.214 <p>How to setup NVIDIA driver for \"gpu-su-v100\" flavor based VM?</p> <p>After launching a VM with an NVIDIA V100 GPU flavor, you will need to setup the NVIDIA driver in order to use GPU-based codes and libraries. Please run the following commands to setup the NVIDIA driver and CUDA version required for these flavors in order to execute GPU-based codes. NOTE: These commands are ONLY applicable for the VM based on \"ubuntu-22.04-x86_64\" image. You might need to find corresponding packages for your own OS of choice.</p> <pre><code>sudo apt update\nsudo apt -y install nvidia-driver-470\n# Just click *Enter* if any popups appear!\n# Confirm and verify that you can see the NVIDIA device attached to your VM\nlspci | grep -i nvidia\n# 00:05.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] (rev a1)\nsudo reboot\n# SSH back to your VM and then you will be able to use nvidia-smi command\nnvidia-smi\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#iv-nvidia-k80-12gb","title":"iv. NVIDIA K80 12GB","text":"<p>The \"gpu-su-k80\" flavor is provided from Supermicro X10DRG-H (2x Intel E5-2620 2.40GHz, 24 cores, 128GB memory, 4x NVIDIA K80 12GB) servers. The base unit is 6 vCPU, 28.5 GB memory with default of 20 GB root disk at a rate of $0.463 / hr of wall time.</p> Flavor SUs GPU vCPU RAM(GiB) Storage(GiB) Cost / hr gpu-su-k80.1 1 1 6 28.5 20 $0.463 gpu-su-k80.2 2 2 12 57 20 $0.926 gpu-su-k80.4 4 4 24 114 20 $1.852 <p>How to setup NVIDIA driver for \"gpu-su-k80\" flavor based VM?</p> <p>After launching a VM with an NVIDIA K80 GPU flavor, you will need to setup the NVIDIA driver in order to use GPU-based codes and libraries. Please run the following commands to setup the NVIDIA driver and CUDA version required for these flavors in order to execute GPU-based codes. NOTE: These commands are ONLY applicable for the VM based on \"ubuntu-22.04-x86_64\" image. You might need to find corresponding packages for your own OS of choice.</p> <pre><code>sudo apt update\nsudo apt -y install nvidia-driver-470\n# Just click *Enter* if any popups appear!\n# Confirm and verify that you can see the NVIDIA device attached to your VM\nlspci | grep -i nvidia\n# 00:05.0 3D controller: NVIDIA Corporation GK210GL [Tesla K80] (rev a1)\nsudo reboot\n# SSH back to your VM and then you will be able to use nvidia-smi command\nnvidia-smi\n</code></pre> <p>NERC IaaS Storage Tiers Cost</p> <p>Storage both OpenStack Swift (object storage) and Cinder (block storage/ volumes) are charged separately at a rate of $0.009 TiB/hr or $9.00E-6 GiB/hr. More about cost can be found here and some of the common billing related FAQs are listed here.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#how-can-i-get-customized-a100-sxm4-gpus-not-listed-in-the-current-flavors","title":"How can I get customized A100 SXM4 GPUs not listed in the current flavors?","text":"<p>We also provide customized A100 SXM4 GPU-based flavors, which are not publicly listed on our NVIDIA A100 SXM4 40GB GPU Tiers list. These options are exclusively available for demanding projects and are subject to availability.</p> <p>To request access, please fill out this form. Our team will review your request and reach out to you to discuss further.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#how-to-change-flavor-of-an-instance","title":"How to Change Flavor of an instance","text":""},{"location":"openstack/create-and-connect-to-the-VM/flavors/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard, you can navigate to Project -&gt; Compute -&gt; Instances.</p> <p>You can select the instance you wish to extend or change the flavor. Here, you will see several options available under the Actions menu located on the right-hand side of your instance, as shown here:</p> <p></p> <p>Click \"Resize Instance\".</p> <p>In the Resize Instance dialog box, select the new flavor of your choice under the \"New Flavor\" dropdown options. In this example, we are changing the current flavor \"cpu-su.1\" to the new flavor \"cpu-su.2\" for our VM, as shown below:</p> <p></p> <p>Once reviwing the new flavor details and verified all details, press \"Resize\" button.</p> <p>Very Important Information</p> <p>You will only be able to choose flavors that are within your current available resource quotas, i.e., vCPUs and RAM.</p> <p>You will see the status of the resize in the following page.</p> <p>When it says \"Confirm or Revert Resize/Migrate\", login to the instance and verify that it worked as intended (meaning the instance is working as before but with the new flavor).</p> <p>If you are happy with the result, press \"Confirm Resize/Rigrate\" in drop-down to the far right (it should be pre-selected) as shown below:</p> <p></p> <p>This will finalise the process and make it permanent.</p> <p>If you are unhappy (for some reason the process failed), you are able to instead press \"Revert resize/Migrate\" (available in the drop-down). This will revert the process.</p>"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see     OpenStack Command Line setup     for more information.</li> </ul> <p>If you want to change the flavor that is bound to a VM, then you can run the following openstack client commands, here we are changing flavor of an existing VM i.e. named \"test-vm\" from <code>mem-su.2</code> to <code>mem-su.4</code>:</p> <p>First, stop the running VM using:</p> <pre><code>openstack server stop test-vm\n</code></pre> <p>Then, verify the status is \"SHUTOFF\" and also the used flavor is <code>mem-su.2</code> as shown below:</p> <pre><code>openstack server list\n+--------------------------------------+------+---------+--------------------------------------------+--------------------------+---------+\n| ID | Name | Status | Networks | Image | Flavor |\n+--------------------------------------+------+---------+--------------------------------------------+--------------------------+---------+\n| cd51dbba-fe95-413c-9afc-71370be4d4fd | test-vm | SHUTOFF | default_network=192.168.0.58, 199.94.60.10 | N/A (booted from volume) | mem-su.2 |\n+--------------------------------------+------+---------+--------------------------------------------+--------------------------+---------+\n</code></pre> <p>Then, resize the flavor from <code>mem-su.2</code> to <code>mem-su.4</code> by running:</p> <pre><code>openstack server resize --flavor mem-su.4 cd51dbba-fe95-413c-9afc-71370be4d4fd\n</code></pre> <p>Confirm the resize:</p> <pre><code>openstack server resize confirm cd51dbba-fe95-413c-9afc-71370be4d4fd\n</code></pre> <p>Then, start the VM:</p> <pre><code>openstack server start cd51dbba-fe95-413c-9afc-71370be4d4fd\n</code></pre> <p>Verify the VM is using the new flavor of <code>mem-su.4</code> as shown below:</p> <pre><code>openstack server list\n+--------------------------------------+------+--------+--------------------------------------------+--------------------------+---------+\n| ID | Name | Status | Networks | Image | Flavor |\n+--------------------------------------+------+--------+--------------------------------------------+--------------------------+---------+\n| cd51dbba-fe95-413c-9afc-71370be4d4fd | test-vm | ACTIVE | default_network=192.168.0.58, 199.94.60.10 | N/A (booted from volume) | mem-su.4 |\n+--------------------------------------+------+--------+--------------------------------------------+--------------------------+---------+\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/images/","title":"Available Images","text":""},{"location":"openstack/create-and-connect-to-the-VM/images/#images","title":"Images","text":"<p>Image composed of a virtual collection of a kernel, operating system, and configuration.</p>"},{"location":"openstack/create-and-connect-to-the-VM/images/#glance","title":"Glance","text":"<p>Glance is the API-driven OpenStack image service that provides services and associated libraries to store, browse, register, distribute, and retrieve bootable disk images. It acts as a registry for virtual machine images, allowing users to copy server images for immediate storage. These images can be used as templates when setting up new instances.</p>"},{"location":"openstack/create-and-connect-to-the-VM/images/#nerc-images-list","title":"NERC Images List","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate to Project -&gt; Compute -&gt; Images.</p> <p>NERC provides a set of default images that can be used as source while launching an instance:</p> ID Name a9b48e65-0cf9-413a-8215-81439cd63966 MS-Windows-2022 cfecb5d4-599c-4ffd-9baf-9cbe35424f97 almalinux-8-x86_64 263f045e-86c6-4344-b2de-aa475dbfa910 almalinux-9-x86_64 41fa5991-89d5-45ae-8268-b22224c772b2 debian-10-x86_64 99194159-fcd1-4281-b3e1-15956c275692 fedora-36-x86_64 74a33f77-fc42-4dd1-a5a2-55fb18fc50cc rocky-8-x86_64 d7d41e5f-58f4-4ba6-9280-7fef9ac49060 rocky-9-x86_64 75a40234-702b-4ab7-9d83-f436b05827c9 ubuntu-18.04-x86_64 8c87cf6f-32f9-4a4b-91a5-0d734b7c9770 ubuntu-20.04-x86_64 da314c41-19bf-486a-b8da-39ca51fd17de ubuntu-22.04-x86_64 17912292-8861-489a-b37e-bb78e15b934a ubuntu-24.04-x86_64"},{"location":"openstack/create-and-connect-to-the-VM/images/#how-to-create-and-upload-own-custom-images","title":"How to create and upload own custom images?","text":"<p>Beside the above mentioned system provided images users can customize and upload their own images to the NERC, as documented in this documentation.</p> <p>Please refer to this guide to learn more about how to obtain other publicly available virtual machine images for the NERC OpenStack platform within your project space.</p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/","title":"Launch a VM","text":""},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#how-to-launch-an-instance","title":"How to launch an Instance","text":"<p>Prerequisites:</p> <ul> <li> <p>You followed the instruction in Create a Key Pair     to set up a public ssh key.</p> </li> <li> <p>Make sure you have added rules in the     Security Groups to     allow ssh using Port 22 access to the instance.</p> </li> </ul>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate: Project -&gt; Compute -&gt; Instances.</p> <p>Click on \"Launch Instance\" button:</p> <p></p> <p>In the Launch Instance dialog box, specify the following values:</p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#details-tab","title":"Details Tab","text":"<p>Instance Name: Give your instance a name that assign a name to the virtual machine.</p> <p>Important Note</p> <p>The instance name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly.</p> <p>Availability Zone: By default, this value is set to the availability zone given by the cloud provider i.e. <code>nova</code>.</p> <p>Count: To launch multiple instances, enter a value greater than 1. The default is 1.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#source-tab","title":"Source Tab","text":"<p>Double check that in the dropdown \"Select Boot Source\".</p> <p>When you start a new instance, you can choose the Instance Boot Source from the following list:</p> <ul> <li> <p>boot from image</p> </li> <li> <p>boot from instance snapshot</p> </li> <li> <p>boot from volume</p> </li> <li> <p>boot from volume snapshot</p> </li> </ul> <p>In its default configuration, when the instance is launched from an Image or an Instance Snapshot, the choice for utilizing persistent storage is configured by selecting the Yes option for \"Create New Volume\". Additionally, the \"Delete Volume on Instance Delete\" setting is pre-set to No, as indicated here:</p> <p></p> <p>If you set the \"Create New Volume\" option to No, the instance will boot from either an image or a snapshot, with the instance only being attached to an ephemeral disk as described here. To mitigate potential data loss, we strongly recommend regularly taking a snapshot of such a running ephemeral instance, referred to as an \"instance snapshot\", especially if you want to safeguard or recover important states of your instance.</p> <p>When deploying a non-ephemeral instance, which involves creating a new volume and selecting Yes for \"Delete Volume on Instance Delete\", deleting the instance will also remove the associated volume. Consequently, all data on that disk is permanently lost, which is undesirable when the data on attached volumes needs to persist even after the instance is deleted. Ideally, selecting \"Yes\" for this setting should be reserved for instances where persistent data storage is not required.</p> <p>Very Important: How do you make your VM setup and data persistent?</p> <p>For more in-depth information on making your VM setup and data persistent, you can explore the details here.</p> <p>To start a VM, for the first time we will need a base image so, please make sure \"Image\" dropdown option is selected. In the example, we chose ubuntu-22.04-x86_64, you may choose any available images.</p> <p>Bootable Images</p> <p>NERC has made several Public bootable images available to the users as listed here. Customers can also upload their own custom images, as documented in this guide.</p> <p>To view them, Navigate: Project -&gt; Compute -&gt; Images.</p> <p></p> <p></p> <p>How to override the flavor's Default root disk volume size</p> <p>If you don't specify custom value for the \"Volume Size (GB)\", that will be set to the root disk size of your selected Flavor. For more about the default root disk size you can refer to this documentation. We can override this value by entering our own custom value (in GiB) and that is available as a Volume that is attach to the instance to enable persistent storage.</p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#flavor-tab","title":"Flavor Tab","text":"<p>Specify the size of the instance to launch. Choose <code>cpu-su.4</code> from the 'Flavor' tab by clicking on the \"+\" icon.</p> <p>Important Note</p> <p>on NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server.</p> <p>Some of the flavors will not be available for your use as per your resource Quota limits and will be shown as below:</p> <p></p> <p>NOTE: More details about available flavors can be found here and how to change request the current allocation quota attributes can be found here.</p> <p>After choosing <code>cpu-su.4</code>, you should see it moved up to \"Allocated\".</p> <p></p> <p>Storage and Volume</p> <ul> <li> <p>System disks are the first disk based on the flavor disk space and are generally used to store the operating system created from an image when the virtual machine is booted.</p> </li> <li> <p>Volumes are persistent virtualized block devices independent of any particular instance. Volumes may be attached to a single instance at a time, but may be detached or reattached to a different instance while retaining all data, much like a USB drive. The size of the volume can be selected when it is created within the storage quota limits for the particular resource allocation.</p> </li> </ul>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#networks-tab","title":"Networks Tab","text":"<p>Make sure the Default Network that is created by default is moved up to \"Allocated\". If not, you can click on the \"+\" icon in \"Available\".</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#security-groups-tab","title":"Security Groups Tab","text":"<p>Make sure to add the security group where you enabled SSH. To add an SSH security group first, see here.</p> <p></p> <p>How to update New Security Group(s) on any running VM?</p> <p>If you want to attach/deattach any new Security Group(s) to/from a running VM after it has launched. First create all new Security Group(s) with all the rules required. Following this guide, you'll be able to attach created security group(s) with all the required rules to a running VM. You can modify the Rules setup for any Security Group(s) but that will affect all VMs using that security groups.</p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#key-pair-tab","title":"Key Pair Tab","text":"<p>Add the key pair you created for your local machine/laptop to use with this VM. To add a Key Pair first create and add them to your Project as described here.</p> <p></p> <p>Important Note</p> <p>If you did not provide a key pair, security groups, or rules, users can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured. We recommend limiting access as much as possible for best security practices.</p>"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#ignore-other-tabs","title":"Ignore other Tabs","text":"<p>Network Ports, Configuration, Server Groups, Schedular Hints, and Metadata: tab: Please ignore these tabs as these are not important and only for advance setup.</p> <p>How to use 'Configuration' tab</p> <p>If you want to specify a customization script that runs after your instance launches then you can write those custom script inside the \"Customization Script\" text area or load the script by uploading the script file.</p> <p>For example:</p> <p></p> <p>You are now ready to launch your VM - go ahead and click \"Launch Instance\". This will initiate an instance.</p> <p>On a successful launch you would be redirected to Compute -&gt; Instances tab and can see the VM spawning.</p> <p>Once your VM is successfully running you will see the Power State changes from \"No State\" to \"running\".</p> <p></p> <p>Note</p> <p>Here we explained about launching an instance using Image but you can also launch an instance from the \"instance snapshot\" or \"volume\" or \"volume snapshot\" option similar to the steps above. If you want to use OpenStack CLI to launch a VM you can read this or if you want to provision the NERC resources using Terraform you can read this.</p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/","title":"SSH to the VM","text":""},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#ssh-to-the-vm","title":"SSH to the VM","text":"<p>Shell, or SSH, is used to administering and managing Linux workloads. Before trying to access instances from the outside world, you need to make sure you have followed these steps:</p> <ul> <li> <p>You followed the instruction in Create a Key Pair     to set up a public ssh key.</p> </li> <li> <p>Your public ssh-key has selected (in \"Key Pair\" tab) while     launching the instance.</p> </li> <li> <p>Assign a Floating IP to the instance in order to     access it from outside world.</p> </li> <li> <p>Make sure you have added rules in the     Security Groups to     allow ssh using Port 22 access to the instance.</p> </li> </ul> <p>How to update New Security Group(s) on any running VM?</p> <p>If you want to attach/deattach any new Security Group(s) to/from a running VM after it has launched. First create all new Security Group(s) with all the rules required. Following this guide, you'll be able to attach created security group(s) with all the required rules to a running VM.</p> <p>Make a note of the Floating IP you have associated to your instance.</p> <p></p> <p>In our example, the IP is <code>199.94.60.66</code>.</p> <p>Default usernames for all the base images are:</p> <ul> <li> <p>all Ubuntu images: ubuntu</p> </li> <li> <p>all AlmaLinux images: almalinux</p> </li> <li> <p>all Rocky Linux images: rocky</p> </li> <li> <p>all Fedora images: fedora</p> </li> <li> <p>all Debian images: debian</p> </li> <li> <p>all RHEL images: cloud-user</p> </li> </ul> <p>Removed Centos Images</p> <p>If you still have VMs running with deleted CentOS images, you need to use the following default username for your CentOS images: <code>centos</code>.</p> <ul> <li>all CentOS images: centos</li> </ul> <p>Our example VM was launched with the ubuntu-22.04-x86_64 base image, the user we need is 'ubuntu'.</p> <p>Open a Terminal window and type:</p> <pre><code>ssh ubuntu@199.94.60.66\n</code></pre> <p>Since you have never connected to this VM before, you will be asked if you are sure you want to connect. Type <code>yes</code>.</p> <p></p> <p>Important Note</p> <p>If you haven't added your key to ssh-agent, you may need to specify the private key file, like this: <code>ssh -i ~/.ssh/cloud.key ubuntu@199.94.60.66</code></p> <p>To add your private key to the <code>ssh-agent</code> you can follow the following steps:</p> <ol> <li> <p><code>eval \"$(ssh-agent -s)\"</code></p> <p>Output: <code>Agent pid 59566</code></p> </li> <li> <p><code>ssh-add ~/.ssh/cloud.key</code></p> <p>If your private key is password protected, you'll be prompted to enter the passphrase.</p> </li> <li> <p>Verify that the key has been added by running <code>ssh-add -l</code>.</p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#ssh-to-the-vm-using-ssh-config","title":"SSH to the VM using SSH Config","text":"<p>Alternatively, You can also configure the setting for the remote instances in your SSH configuration file (typically found in <code>~/.ssh/config</code>). The SSH configuration file might include entry for your newly launched VM like this:</p> <pre><code>Host ExampleHostLabel\n    HostName 199.94.60.66\n    User ubuntu\n    IdentityFile ~/.ssh/cloud.key\n</code></pre> <p>Here, the <code>Host</code> value can be any label you want. The <code>HostName</code> value is the Floating IP you have associated to your instance that you want to access, the <code>User</code> value specifies the default account username based on your base OS image used for the VM and <code>IdentityFile</code> specify the path to your Private Key on your local machine. With this configuration defined, you can connect to the account by simply using the Host value set as \"ExampleHostLabel\". You do not have to type the username, hostname, and private key each time.</p> <p>So, you can SSH into your host VM by running:</p> <pre><code>ssh ExampleHostLabel\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#setting-a-password","title":"Setting a password","text":"<p>When the VMs are launched, a strong, randomly-generated password is created for the default user, and then discarded.</p> <p>Once you connect to your VM, you will want to set a password in case you ever need to log in via the console in the web dashboard.</p> <p>For example, if your network connections aren't working correctly.</p> <p>Setting a password is necessary to use Remote Desktop Protocol (RDP)</p> <p>Remote Desktop Protocol(RDP) is widely used for Windows remote connections, but you can also access and interact with the graphical user interface of a remote Linux server by using a tool like xrdp, an open-source implementation of the RDP server. You can use <code>xrdp</code> to remotely access the Linux desktop. To do so, you need to utilize the RDP client. Moreover, xrdp delivers a login to the remote machines employing Microsoft RDP. This is why a user with the password is necessary to access the VM. You can refer to this guide on how to install and configure a RDP server using xrdp on a Ubuntu server and access it using a RDP client from your local machine.</p> <p>Since you are not using it to log in over SSH or to sudo, it doesn't really matter how hard it is to type, and we recommend using a randomly-generated password.</p> <p>Create a random password like this:</p> <pre><code>ubuntu@test-vm:~$ cat /dev/urandom | base64 | dd count=14 bs=1\nT1W16HCyfZf8V514+0 records in\n14+0 records out\n14 bytes copied, 0.00110367 s, 12.7 kB/s\n</code></pre> <p>The 'count' parameter controls the number of characters.</p> <p>The first [count] characters of the output are your randomly generated output, followed immediately by \"[count]+0\", so in the above example the password is: <code>T1W16HCyfZf8V5</code>.</p> <p>Set the password for ubuntu using the command:</p> <pre><code>ubuntu@test-vm:~$ sudo passwd ubuntu\nNew password:\nRetype new password:\n... password updated successfully\n</code></pre> <p>Store the password in a secure place. Don't send it over email, post it on your wall on a sticky note, etc.</p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#adding-other-peoples-ssh-keys-to-the-instance","title":"Adding other people's SSH keys to the instance","text":"<p>You were able to log in using your own SSH key.</p> <p>Right now Openstack only permits one key to be added at launch, so you need to add your teammates keys manually.</p> <p>Get your teammates' public keys. If they used ssh-keygen to create their key, this will be in a file called .pub on their machine.</p> <p>If they created a key via the dashboard, or imported the key created with ssh-keygen, their public key is viewable from the Key Pairs tab.</p> <p>Click on the key pair name. The public key starts with 'ssh-rsa' and looks something like this:</p> <pre><code>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDL6O5qNZHfgFwf4vnnib2XBub7ZU6khy6z6JQl3XRJg6I6gZ\n+Ss6tNjz0Xgax5My0bizORcka/TJ33S36XZfzUKGsZqyEl/ax1Xnl3MfE/rgq415wKljg4\n+QvDznF0OFqXjDIgL938N8G4mq/\ncKKtRSMdksAvNsAreO0W7GZi24G1giap4yuG4XghAXcYxDnOSzpyP2HgqgjsPdQue919IYvgH8shr\n+sPa48uC5sGU5PkTb0Pk/ef1Y5pLBQZYchyMakQvxjj7hHZaT/\nLw0wIvGpPQay84plkjR2IDNb51tiEy5x163YDtrrP7RM2LJwXm+1vI8MzYmFRrXiqUyznd\ntest_user@demo\n</code></pre> <p>Create a file called something like 'teammates.txt' and paste in your team's public keys, one per line.</p> <p>Hang onto this file to save yourself from having to do all the copy/pasting every time you launch a new VM.</p> <p>Copy the file to the vm:</p> <pre><code>[you@your-laptop ~]$ scp teammates.txt ubuntu@199.94.60.66:~\n</code></pre> <p>If the copy works, you will see the output:</p> <pre><code>teammates.txt                  100%    0     0KB/s   00:00\n</code></pre> <p>Append the file's contents to authorized_keys:</p> <pre><code>[cloud-user@test-vm ~] #cat teammates.txt &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <p>Now your teammates should also be able to log in.</p> <p>Important Note</p> <p>Make sure to use <code>&gt;&gt;</code> instead of <code>&gt;</code> to avoid overwriting your own key.</p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#adding-users-to-the-instance","title":"Adding users to the instance","text":"<p>You may decide that each teammate should have their own user on the VM instead of everyone logging in to the default user.</p> <p>Once you log into the VM, you can create another user like this.</p> <p>Note</p> <p>The 'sudo_group' is different for different OS - in CentOS and Red Hat, the group is called 'wheel', while in Ubuntu, the group is called 'sudo'.</p> <pre><code>sudo su\n# useradd -m &lt;username&gt;\n# passwd &lt;username&gt;\n# usermod -aG &lt;sudo_group&gt; &lt;username&gt;    &lt;-- skip this step for users who\n# should not have root access\n# su username\ncd ~\nmkdir .ssh\nchmod 700 .ssh\ncd .ssh\nvi authorized_keys   &lt;-- paste the public key for that user in this file\nchmod 600 authorized_keys\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#how-to-enable-remote-desktop-protocol-using-xrdp-on-ubuntu","title":"How To Enable Remote Desktop Protocol Using xrdp on Ubuntu","text":""},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#log-in-to-the-server-with-sudo-access","title":"Log in to the server with Sudo access","text":"<p>In order to install the <code>xrdp</code>, you need to login to the server with <code>sudo</code> access to it.</p> <pre><code>ssh username@your_server_ip\n</code></pre> <p>For example:</p> <pre><code>ssh ubuntu@199.94.60.66\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#installing-a-desktop-environment","title":"Installing a Desktop Environment","text":"<p>After connecting to your server using SSH update the list of available packages using the following command:</p> <pre><code>sudo apt update\n</code></pre> <p>Next, install the <code>xfce</code> and <code>xfce-goodies</code> packages on your server:</p> <pre><code>sudo apt install xfce4 xfce4-goodies -y\n</code></pre> <p>Select Display Manager</p> <p>If prompted to choose a display manager, which manages graphical login mechanisms and user sessions, you can select any option from the list of available display managers. For instance, here we have <code>gdm3</code> as the default selection.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#installing-xrdp","title":"Installing xrdp","text":"<p>To install xrdp, run the following command in the terminal:</p> <pre><code>sudo apt install xrdp -y\n</code></pre> <p>After installing xrdp, verify the status of xrdp using systemctl:</p> <pre><code>sudo systemctl status xrdp\n</code></pre> <p>This command will show the status as active (running):</p> <p>Output:</p> <pre><code>\u25cf xrdp.service - xrdp daemon\n    Loaded: loaded (/lib/systemd/system/xrdp.service; enabled; vendor preset: enab&gt;\n    Active: active (running) since Mon 2024-02-12 21:33:01 UTC; 9s ago\n    ...\n    CGroup: /system.slice/xrdp.service\n            \u2514\u25008839 /usr/sbin/xrdp\n</code></pre> <p>What if xrdp is not Running?</p> <p>If the status of xrdp is not running, you may have to start the service manually with this command: <code>sudo systemctl start xrdp</code>. After executing the above command, verify the status again to ensure xrdp is in a running state.</p> <p>Make xrdp use the desktop environment we previously created:</p> <pre><code>sudo sed -i.bak '/fi/a #xrdp multiple users configuration \\n xfce-session \\n' /etc/xrdp/startwm.sh\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#configuring-xrdp-and-updating-security-groups","title":"Configuring xrdp and Updating Security Groups","text":"<p>If you want to customize the default xrdp configuration (optional), you will need to review the default configuration of xrdp, which is stored under <code>/etc/xrdp/xrdp.ini</code>. <code>xrdp.ini</code> is the default configuration file to set up RDP connections to the xrdp server. The configuration file can be modified and customized to meet the RDP connection requirements.</p> <p>Add a new security group with a RDP (port 3389) rule open to the public for a RDP connection and attach that security group to your instance as described here.</p> <p>How to Update Security Group(s) on a Running VM?</p> <p>Following this guide, you'll be able to attach created security group(s) with all the required rules to a running VM.</p> <p>Restart the xrdp server to make sure all the above changes are reflected:</p> <pre><code>sudo systemctl restart xrdp\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#testing-the-rdp-connection","title":"Testing the RDP Connection","text":"<p>You should now be able to connect to the Ubuntu VM via xrdp.</p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#testing-the-rdp-connection-on-windows","title":"Testing the RDP Connection on Windows","text":"<p>If you are using Windows as a local desktop, Windows users have a RDP connection application by default on their machines.</p> <p>Enter your VM's Floating IP and username into the fillable text boxes for Computer and User name.</p> <p></p> <p>You may need to press the down arrow for \"Show Options\" to input the username i.e. <code>ubuntu</code>:</p> <p></p> <p>Press the Connect button. If you receive an alert that the \"Remote Desktop can't connect to the remote computer\", check that you have properly attached the security group with a RDP (port 3389) rule open to the public to your VM as described here.</p> <p>Press Yes if you receive the identity verification popup:</p> <p></p> <p>Then, enter your VM's username (ubuntu) and the password you created for user ubuntu following this steps.</p> <p>Press Ok.</p> <p></p> <p>Once you have logged in, you should be able to access your Ubuntu Desktop environment:</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#testing-the-rdp-connection-on-macos","title":"Testing the RDP Connection on macOS","text":"<p>To test the connection using the Remote Desktop Connection client on macOS, first launch the Microsoft Remote Desktop Connection app.</p> <p>Press Add PC, then enter your remote server's Floating IP in the <code>PC name</code> fillable box:</p> <p></p> <p>You can Add a user account when setting up the connection:</p> <p></p> <p>Once you have logged in, you can access your Ubuntu remote desktop. You can close it with the exit button.</p>"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-the-VM/#testing-the-rdp-connection-on-linux","title":"Testing the RDP Connection on Linux","text":"<p>If you are using Linux as your Local desktop you can connect to the server via Remmina.</p>"},{"location":"openstack/create-and-connect-to-the-VM/bastion-host-based-ssh/","title":"Bastion Host based SSH","text":""},{"location":"openstack/create-and-connect-to-the-VM/bastion-host-based-ssh/#bastion-host","title":"Bastion Host","text":"<p>A bastion host is a server that provides secure access to private networks over SSH from an external network, such as the Internet. We can leverage a bastion host to record all SSH sessions established with private network instances which enables auditing and can help us in efforts to comply with regulatory requirements.</p> <p>The following diagram illustrates the concept of using an SSH bastion host to provide access to Linux instances running inside OpenStack cloud network.</p> <p></p> <p>In OpenStack, users can deploy instances in a private tenant network. In order to make these instances to be accessible externally via internet, the tenant must assign each instance a Floating IP address i.e., an external public IP. Nevertheless, users may still want a way to deploy instances without having to assign a Floating IP address for every instance.</p> <p>This is useful in the context of an OpenStack project as you don't necessarily want to reserve a Floating IP for all your instances. This way you can isolate certain resources so that there is only a single point of access to them and conserve Floating IP addresses so that you don't need as big of a quota.</p> <p>Leveraging an SSH bastion host allows this sort of configuration while still enabling SSH access to the private instances.</p> <p>Before trying to access instances from the outside world using SSH tunneling via Bastion Host, you need to make sure you have followed these steps:</p> <ul> <li>You followed the instruction in Create a Key Pair     to set up a public ssh key. You can use the same key for both the bastion     host and the remote instances, or different keys; you'll just need to ensure     that the keys are loaded by ssh-agent appropriately so they can be used as     needed. Please read this instruction     on how to add ssh-agent and load your private key using ssh-add command to     access the bastion host.</li> </ul> <p>Verify you have an SSH agent running. This should match whatever you built your cluster with.</p> <pre><code>ssh-add -l\n</code></pre> <p>If you need to add the key to your agent:</p> <pre><code>ssh-add path/to/private/key\n</code></pre> <p>Now you can SSH into the bastion host:</p> <pre><code>ssh -A &lt;user&gt;@&lt;bastion-floating-IP&gt;\n</code></pre> <ul> <li> <p>Your public ssh-key was selected (in the Access and Security tab) while     launching the instance.</p> </li> <li> <p>Add two Security Groups, one will be used by the Bastion host and another one     will be used by any private instances.</p> </li> </ul> <p></p> <p>i. Bastion Host Security Group:</p> <p>Allow inbound SSH (optional ICMP) for this security group. Make sure you have added rules in the Security Groups to allow ssh to the bastion host.</p> <p></p> <p>ii. Private Instances Security Group:</p> <p>You need to select \"Security Group\" in Remote dropdown option, and then select the \"Bastion Host Security Group\" under Security Group option as shown below:</p> <p></p> <p></p> <ul> <li>Assign a Floating IP to the Bastion host instance     in order to access it from outside world.</li> </ul> <p>Make a note of the Floating IP you have associated to your instance.</p> <p></p> <p>While adding the Bastion host and private instance, please select appropriate Security Group as shown below:</p> <p>private1:</p> <p></p> <p>bastion_host_demo:</p> <p></p> <p>Finally, you'll want to configure the ProxyJump setting for the remote instances in your SSH configuration file (typically found in <code>~/.ssh/config</code>). In SSH configuration file, we can define multiple hosts by pet names, specify custom ports, hostnames, users, etc. For example, let's say that you had a remote instance named \"private1\" and you wanted to run SSH connections through a bastion host called \"bastion\". The appropriate SSH configuration file might look something like this:</p> <pre><code>Host bastion\n  HostName 140.247.152.139\n  User ubuntu\n\nHost private1\n  Hostname 192.168.0.40\n  User ubuntu\n  ProxyJump bastion\n</code></pre> <p>ProxyJump makes it super simple to jump from one host to another totally transparently.</p> <p>OR,</p> <p>if you don't have keys loaded by <code>ssh-add</code> command starting <code>ssh-agent</code> on your local machine. you can load the private key using IdentityFile variable in SSH configuration file as shown below:</p> <pre><code>Host private1\n  Hostname 192.168.0.40\n  User ubuntu\n  IdentityFile ~/.ssh/cloud.key\n  ProxyJump bastion\n\nHost bastion\n  HostName 140.247.152.139\n  User ubuntu\n  IdentityFile ~/.ssh/cloud.key\n</code></pre> <p>With this configuration in place, when you type <code>ssh private1</code> SSH will establish a connection to the bastion host and then through the bastion host connect to \"private1\", using the agent added keys or specified private keys.</p> <p>In this sort of arrangement, SSH traffic to private servers that are not directly accessible via SSH is instead directed through a bastion host, which proxies the connection between the SSH client and the remote servers. The bastion host runs on an instance that is typically in a public subnet with attached floating public IP. Private instances are in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying instance running the bastion host.</p> <p>The user won't see any of this; he or she will just see a shell for \"private1\" appear. If you dig a bit further, though (try running who on the remote node), you'll see the connections are coming from the bastion host, not the original SSH client.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/","title":"About OpenVPN","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/#openvpn","title":"OpenVPN","text":"<p>OpenVPN is a full-featured SSL VPN which implements OSI layer 2 or 3 secure network extension using the industry standard SSL/TLS protocol, supports flexible client authentication methods based on certificates, smart cards, and/ or username/password credentials, and allows user or group-specific access control policies using firewall rules applied to the VPN virtual interface.</p> <p>OpenVPN offers a scalable client/server mode, allowing multiple clients to connect to a single OpenVPN server process over a single TCP or UDP port.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/#installing-openvpn-server","title":"Installing OpenVPN Server","text":"<p>You can read official documentation here.</p> <p>You can spin up a new instance with \"ubuntu-22.04-x86_64\" or any available Ubuntu OS image, named \"openvpn_server\" on OpenStack, with \"default\" and \"ssh_only\" Security Groups attached to it.</p> <p></p> <p>Also, attach a Floating IP to this instance so you can ssh into it from outside.</p> <p>Create a new Security Group i.e. \"openvpn\" that is listening on UDP port 1194 as shown below:</p> <p></p> <p>The Security Groups attached to the OpenVPN server should look similar to the image below:</p> <p>The Security Groups attached to the OpenVPN server includes \"default\", \"ssh_only\" and \"openvpn\". It should look similar to the image shown below:</p> <p></p> <p>Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in <code>~/.ssh/config</code>). The SSH configuration file might include entry for your newly created OpenVPN server like this:</p> <pre><code>Host openvpn\n  HostName 199.94.60.66\n  User ubuntu\n  IdentityFile ~/.ssh/cloud.key\n</code></pre> <ol> <li> <p>Then you can ssh into the OpenVPN Server running: <code>ssh openvpn</code></p> <p></p> </li> <li> <p>Also note that OpenVPN must be installed and run by a user who has    administrative/root privileges. So, we need to run the command: <code>sudo su</code></p> </li> <li> <p>We are using this repo to install    OpenVPN server on this ubuntu server.</p> <p>For that, run the script and follow the assistant:</p> <pre><code>wget https://git.io/vpn -O openvpn-install.sh &amp;&amp; bash openvpn-install.sh\n</code></pre> <p></p> <p>You can press Enter for all default values. And, while entering a name for the first client you can give \"nerc\" as the client name, this will generate a new configuration file (.ovpn file) named as \"nerc.ovpn\". Based on your client's name it will name the config file as \".ovpn\"</p> <p></p> </li> <li> <p>Copy the generated config file from \"/root/nerc.ovpn\" to \"/home/ubuntu/    nerc.ovpn\" by running: <code>cp /root/nerc.ovpn .</code></p> </li> <li> <p>Update the ownership of the config file to ubuntu user and ubuntu group by    running the following command: <code>chown ubuntu:ubuntu nerc.ovpn</code></p> </li> <li> <p>You can exit from the root and ssh session all together and then copy the    configuration file to your local machine by running the following script on    your local machine's terminal: <code>scp openvpn:nerc.ovpn .</code></p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/#to-add-a-new-client-user","title":"To add a new client user","text":"<p>Once it ends, you can run it again to add more users, remove some of them or even completely uninstall OpenVPN.</p> <p>For this, run the script and follow the assistant:</p> <pre><code>wget https://git.io/vpn -O openvpn-install.sh &amp;&amp; bash openvpn-install.sh\n</code></pre> <p></p> <p>Here, you are giving client name as \"mac_client\" and that will generate a new configuration file at \"/root/mac_client.ovpn\". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client.</p> <p>Important Note</p> <p>You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.</p> <p>A OpenVPN client or compatible software is needed to connect to the OpenVPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the OpenVPN server.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/#windows","title":"Windows","text":"<p>OpenVPN source code and Windows installers can be downloaded here. The OpenVPN executable should be installed on both server and client machines since the single executable provides both client and server functions. Please see the OpenVPN client setup guide for Windows.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/#mac-os-x","title":"Mac OS X","text":"<p>The client we recommend and support for Mac OS is Tunnelblick. To install Tunnelblick, download the dmg installer file from the Tunnelblick site, mount the dmg, and drag the Tunnelblick application to Applications. Please refer to this guide for more information.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/#linux","title":"Linux","text":"<p>OpenVPN is available through the package management system on most Linux distributions.</p> <p>On Debian/Ubuntu:</p> <pre><code>sudo apt-get install openvpn\n</code></pre> <p>On RedHat/Rocky/AlmaLinux:</p> <pre><code>sudo dnf install openvpn\n</code></pre> <p>Then, to run OpenVPN using the client profile:</p> <p>Move the VPN client profile (configuration) file to /etc/openvpn/ :</p> <pre><code>sudo mv nerc.ovpn /etc/openvpn/client.conf\n</code></pre> <p>Restart the OpenVPN daemon (i.e., This will start OpenVPN connection and will automatically run on boot):</p> <pre><code>sudo /etc/init.d/openvpn start\n</code></pre> <p>OR,</p> <pre><code>sudo systemctl enable --now openvpn@client\nsudo systemctl start openvpn@client\n</code></pre> <p>Checking the status:</p> <pre><code>systemctl status openvpn@client\n</code></pre> <p>Alternatively, if you want to run OpenVPN manually each time, then run:</p> <pre><code>sudo openvpn --config /etc/openvpn/client.ovpn\n</code></pre> <p>OR,</p> <pre><code>sudo openvpn --config nerc.ovpn\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/","title":"OpenVPN GUI for Windows","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/#openvpn-gui","title":"OpenVPN-GUI","text":"<p>Official OpenVPN Windows installers include a Windows OpenVPN-GUI, which allows managing OpenVPN connections from a system tray applet.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/#find-your-client-account-credentials","title":"Find your client account credentials","text":"<p>You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/#download-and-install-openvpn-gui","title":"Download and install OpenVPN-GUI","text":"<ol> <li> <p>Download the OpenVPN client installer:</p> <p>OpenVPN for Windows can be installed from the self-installing exe file on the OpenVPN download page. Also note that OpenVPN must be installed and run by a user who has administrative privileges (this restriction is imposed by Windows, not OpenVPN)</p> </li> <li> <p>Launch the installer and follow the prompts as directed.</p> <p></p> </li> <li> <p>Clicking \"Customize\" button we can see settings and features of OpenVPN    GUI client.</p> <p></p> </li> <li> <p>Click \"Install Now\" to continue.</p> <p></p> </li> <li> <p>Click \"Close\"button.</p> </li> <li> <p>For the newly installed OpenVPN GUI there will be no configuration profile    for the client so it will show a pop up that alerts:</p> <p></p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/#set-up-the-vpn-with-openvpn-gui","title":"Set up the VPN with OpenVPN GUI","text":"<p>After you've run the Windows installer, OpenVPN is ready for use and will associate itself with files having the .ovpn extension.</p> <ol> <li> <p>You can use the previously downloaded .ovpn file from your Downloads folder    to setup the connection profiles.</p> <p>a. Either you can Right click on the OpenVPN configuration file (.ovpn) and select \"Start OpenVPN on this config file\":</p> <p></p> <p>b. OR, you can use \"Import file\u2026\" menu to select the previously downloaded .ovpn file.</p> <p></p> <p>Once, done it will show:</p> <p></p> <p>c. OR, you can manually copy the config file to one of OpenVPN's configuration directories:</p> <pre><code>C:\\Program Files\\OpenVPN\\config (global configs)\nC:\\Program Files\\OpenVPN\\config-auto (autostarted global configs)\n%USERPROFILE%\\OpenVPN\\config (per-user configs)\n</code></pre> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/#connect-to-a-vpn-server-location","title":"Connect to a VPN server location","text":"<p>For launching OpenVPN Connections you click on OpenVPN GUI (tray applet). OpenVPN GUI is used to launching VPN connections on demand. OpenVPN GUI is a system-tray applet, so an icon for the GUI will appear in the lower-right corner of the screen located at the taskbar notification area. Right click on the system tray icon, and if you have multiple configurations then a menu should appear showing the names of your OpenVPN configuration profiles and giving you the option to connect. If you have only one configuration then you can just click on \"Connect\" menu.</p> <p></p> <p></p> <p>When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN.</p> <p></p> <p>Once you are connected to the OpenVPN server, you can run commands like shown below in your terminal to connect to the private instances: <code>ssh ubuntu@192.168. 0.40 -A -i cloud.key</code></p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/openvpn_gui_for_windows/#disconnect-vpn-server","title":"Disconnect VPN server","text":"<p>To disconnect, right click on the system tray icon, in your status bar and select Disconnect from the menu.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/","title":"Tunnelblick for MacOS","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/#tunnelblick","title":"Tunnelblick","text":"<p>Tunnelblick is a free, open-source GUI (graphical user interface) for OpenVPN on macOS and OS X: More details can be found here. Access to a VPN server  -  your computer is one end of the tunnel and the VPN server is the other end.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/#find-your-client-account-credentials","title":"Find your client account credentials","text":"<p>You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/#download-and-install-tunnelblick","title":"Download and install Tunnelblick","text":"<ol> <li> <p>Download Tunnelblick, a free and    user-friendly app for managing OpenVPN connections on macOS.</p> <p></p> </li> <li> <p>Navigate to your Downloads folder and double-click the Tunnelblick    installation file (.dmg installer file) you have just downloaded.</p> <p></p> </li> <li> <p>In the window that opens, double-click on the Tunnelblick icon.</p> <p></p> </li> <li> <p>A new dialogue box will pop up, asking you if you are sure you want to open    the app. Click Open.</p> <p></p> <p></p> </li> <li> <p>You will be asked to enter your device password. Enter it and click OK:</p> <p></p> </li> <li> <p>Select Allow or Don't Allow for your notification preference.</p> <p></p> </li> <li> <p>Once the installation is complete, you will see a pop-up notification asking    you if you want to launch Tunnelblick now. (An administrator username and    password will be required to secure Tunnelblick). Click Launch.</p> <p>Alternatively, you can click on the Tunnelblick icon in the status bar and select VPN Details...:</p> <p></p> <p></p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/#set-up-the-vpn-with-tunnelblick","title":"Set up the VPN with Tunnelblick","text":"<ol> <li> <p>A new dialogue box will appear. Click I have configuration files.</p> <p></p> </li> <li> <p>Another notification will pop-up, instructing you how to import    configuration files. Click OK.</p> <p></p> </li> <li> <p>Drag and drop the previously downloaded .ovpn file from your Downloads    folder to the Configurations tab in Tunnelblick.</p> <p></p> <p>OR,</p> <p>You can just drag and drop the provided OpenVPN configuration file (file with .ovpn extension) directly to Tunnelblick icon in status bar at the top-right corner of your screen.</p> <p></p> </li> <li> <p>A pop-up will appear, asking you if you want to install the configuration    profile for your current user only or for all users on your Mac. Select your    preferred option. If the VPN is intended for all accounts on your Mac, select    All Users. If the VPN will only be used by your current account, select    Only Me.</p> <p></p> </li> <li> <p>You will be asked to enter your Mac password.</p> <p></p> <p></p> <p>Then the screen reads \"Tunnelblick successfully: installed one configuration\".</p> <p></p> </li> </ol> <p>You can see the configuration setting is loaded and installed successfully.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/#connect-to-a-vpn-server-location","title":"Connect to a VPN server location","text":"<ol> <li> <p>To connect to a VPN server location, click the Tunnelblick icon in status     bar at the top-right corner of your screen.</p> <p></p> </li> <li> <p>From the drop down menu select the server and click Connect [name of     the .ovpn configuration file]..</p> <p></p> <p>Alternatively, you can select \"VPN Details\" from the menu and then click the \"Connect\"button:</p> <p></p> <p>This will show the connection log on the dialog:</p> <p></p> </li> <li> <p>When you are connected to OpenVPN server successfully, you will see popup     message as shown below. That's it! You are now connected to a VPN.</p> <p></p> </li> <li> <p>Once you are connected to the OpenVPN server, you can run commands like     shown below to connect to the private instances:</p> <pre><code>ssh ubuntu@192.168.0.40 -A -i cloud.key\n</code></pre> <p></p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/openvpn/tunnelblick_for_macos/#disconnect-vpn-server","title":"Disconnect VPN server","text":"<p>To disconnect, click on the Tunnelblick icon in your status bar and select Disconnect in the drop-down menu.</p> <p></p> <p>While closing the log will be shown on popup as shown below: </p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/","title":"sshuttle","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#sshuttle","title":"sshuttle","text":"<p>sshuttle is a lightweight SSH-encrypted VPN. This is a Python based script that allows you to tunnel connections through SSH in a far more efficient way then traditional ssh proxying.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#installing-sshuttle-server","title":"Installing sshuttle Server","text":"<p>You can spin up a new instance with \"ubuntu-22.04-x86_64\" or any available Ubuntu OS image, named \"sshuttle_server\" on OpenStack, with \"default\" and \"ssh_only\" Security Groups attached to it.</p> <p></p> <p>Also, attach a Floating IP to this instance so you can ssh into it from outside.</p> <p></p> <p>Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in <code>~/.ssh/config</code>). The SSH configuration file might include entry for your newly created sshuttle server like this:</p> <pre><code>Host sshuttle\n\n  HostName 140.247.152.244\n  User ubuntu\n  IdentityFile ~/.ssh/cloud.key\n</code></pre> <ol> <li>Then you can ssh into the sshuttle Server running: <code>ssh sshuttle</code></li> </ol> <p></p> <p>Note</p> <p>Unlike other VPN servers, for sshuttle you don't need to install anything on the server side. As long as you have an SSH server (with python3 installed) you're good to go.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#to-connect-from-a-new-client-install-sshuttle","title":"To connect from a new client Install sshuttle","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#windows","title":"Windows","text":"<p>Currently there is no built in support for running sshuttle directly on Microsoft Windows. What you can do is to create a Linux VM with Vagrant (or simply Virtualbox if you like) and then try to connect via that VM. For more details read here</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#mac-os-x","title":"Mac OS X","text":"<p>Install using Homebrew:</p> <pre><code>brew install sshuttle\n</code></pre> <p>OR, via MacPorts</p> <pre><code>sudo port selfupdate\nsudo port install sshuttle\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#linux","title":"Linux","text":"<p>sshuttle is available through the package management system on most Linux distributions.</p> <p>On Debian/Ubuntu:</p> <pre><code>sudo apt-get install sshuttle\n</code></pre> <p>On RedHat/Rocky/AlmaLinux:</p> <pre><code>sudo dnf install sshuttle\n</code></pre> <p>It is also possible to install into a virtualenv as a non-root user.</p> <ul> <li>From PyPI:</li> </ul> <pre><code>virtualenv -p python3 /tmp/sshuttle\n. /tmp/sshuttle/bin/activate\npip install sshuttle\n</code></pre> <ul> <li>Clone:</li> </ul> <pre><code>virtualenv -p python3 /tmp/sshuttle\n. /tmp/sshuttle/bin/activate\ngit clone [https://github.com/sshuttle/sshuttle.git](https://github.com/sshuttle/sshuttle.git)\ncd sshuttle\n./setup.py install\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/sshuttle/#how-to-connect","title":"How to Connect","text":"<p>Tunnel to all networks (0.0.0.0/0):</p> <pre><code>sshuttle -r ubuntu @140.247.152.244 0.0.0.0/0\n</code></pre> <p>OR, shorthand:</p> <pre><code>sudo sshuttle -r ubuntu@140.247.152.244 0/0\n</code></pre> <p>If you would also like your DNS queries to be proxied through the DNS server of the server, you are connected to:</p> <pre><code>sshuttle --dns -r ubuntu@140.247.152.244 0/0\n</code></pre> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/","title":"WireGuard","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#wireguard","title":"WireGuard","text":"<p>WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography.</p> <p>Here's what it will look like:</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#installing-wireguard-server","title":"Installing WireGuard Server","text":"<p>You can spin up a new instance with \"ubuntu-22.04-x86_64\" or any available Ubuntu OS image, named \"wireguard_server\" on OpenStack, with \"default\" and \"ssh_only\" Security Groups attached to it.</p> <p></p> <p>Also, attach a Floating IP to this instance so you can ssh into it from outside.</p> <p>Create a new Security Group i.e. \"wireguard\" that is listening on UDP port 51820 as shown below:</p> <p></p> <p>The Security Groups attached to the WireGuard server includes \"default\", \"ssh_only\" and \"wireguard\". It should look similar to the image shown below:</p> <p></p> <p>Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in <code>~/.ssh/config</code>). The SSH configuration file might include entry for your newly created WireGuard server like this:</p> <pre><code>Host wireguard\n  HostName 140.247.152.188\n  User ubuntu\n  IdentityFile ~/.ssh/cloud.key\n</code></pre> <ol> <li> <p>Then you can ssh into the WireGuard Server running: <code>ssh wireguard</code></p> <p></p> </li> <li> <p>Also note that WireGuard must be installed and run by a user who has    administrative/root privileges. So, we need to run the command: <code>sudo su</code></p> </li> <li> <p>We are using this repo to    install WireGuard server on this ubuntu server.</p> <p>For that, run the script and follow the assistant:</p> <pre><code>wget https://git.io/wireguard -O wireguard-install.sh &amp;&amp; bash wireguard-install.sh\n</code></pre> <p></p> <p>You can press Enter for all default values. And, while entering a name for the first client you can give \"nerc\" as the client name, this will generate a new configuration file (.conf file) named as \"nerc.conf\". Based on your client's name it will name the config file as \".conf\"</p> <p></p> <p>NOTE: For each peers the client configuration files comply with the following template:</p> <p></p> </li> <li> <p>Copy the generated config file from \"/root/nerc.conf\" to    \"/home/ubuntu/nerc.conf\" by running: <code>cp /root/nerc.conf .</code></p> </li> <li> <p>Update the ownership of the config file to ubuntu user and ubuntu group by    running the following command: <code>chown ubuntu:ubuntu nerc.conf</code></p> </li> <li> <p>You can exit from the root and ssh session all together and then copy the    configuration file to your local machine by running the following script on    your local machine's terminal: <code>scp wireguard:nerc.conf .</code></p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#to-add-a-new-client-user","title":"To add a new client user","text":"<p>Once it ends, you can run it again to add more users, remove some of them or even completely uninstall WireGuard.</p> <p>For this, run the script and follow the assistant:</p> <pre><code>wget https://git.io/wireguard -O wireguard-install.sh &amp;&amp; bash wireguard-install.sh\n</code></pre> <p></p> <p>Here, you are giving client name as \"mac_client\" and that will generate a new configuration file at \"/root/mac_client.conf\". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#authentication-mechanism","title":"Authentication Mechanism","text":"<p>It would be kind of pointless to have our VPN server allow anyone to connect. This is where our public &amp; private keys come into play.</p> <ul> <li> <p>Each client's **public** key needs to be added to the     SERVER'S configuration file</p> </li> <li> <p>The server's **public** key added to the CLIENT'S     configuration file</p> </li> </ul>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#useful-commands","title":"Useful commands","text":"<p>To view server config: <code>wg show</code> or, <code>wg</code></p> <p>To activateconfig: <code>wg-quick up /path/to/file_name.config</code></p> <p>To deactivate config: <code>wg-quick down /path/to/file_name.config</code></p> <p>Read more:</p> <p>https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8</p> <p>https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8</p> <p>Important Note</p> <p>You need to contact your project administrator to get your own WireGUard configuration file (file with .conf extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.</p> <p>A WireGuard client or compatible software is needed to connect to the WireGuard VPN server. Please installone of these clients depending on your device. The client program must be configured with a client profile to connect to the WireGuard VPN server.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#windows","title":"Windows","text":"<p>WireGuard client can be downloaded here. The WireGuard executable should be installed on client machines. After the installation, you should see the WireGuard icon in the lower-right corner of the screen located at the taskbar notification area.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#set-up-the-vpn-with-wireguard-gui","title":"Set up the VPN with WireGuard GUI","text":"<p>Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys.</p> <p>Open the WireGuard GUI and either click on Add Tunnel -&gt; Import tunnel(s) from file\u2026 OR,</p> <p>click on \"Import tunnel(s) from file\" button located at the center.</p> <p></p> <p>The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen.</p> <p></p> <p>Either, Right Click on your tunnel name and select \"Edit selected tunnel\u2026\" menu OR, click on \"Edit\" button at the lower left.</p> <p></p> <p>Checking Block untunneled traffic (kill-switch) will make sure that all your traffic is being routed through this new VPN server.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#test-your-connection","title":"Test your connection","text":"<p>On your Windows machine, press the \"Activate\" button. You should see a successful connection be made:</p> <p></p> <p>After a few seconds, the status should change to Active.</p> <p>If the connection is routed through the VPN, it should show the IP address of the WireGuard server as the public address.</p> <p>If that's not the case, to troubleshoot please check the \"Log\" tab and verify and validate the client and server configuration.</p> <p>Clicking \" Deactivate\" button closes the VPN connection.</p> <p></p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#mac-os-x","title":"Mac OS X","text":""},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#i-using-homebrew","title":"I. Using HomeBrew","text":"<p>This allows more than one Wireguard tunnel active at a time unlike the WireGuard GUI app.</p> <ol> <li> <p>Install WireGuard CLI on macOS through brew: <code>brew install wireguard-tools</code></p> </li> <li> <p>Copy the \".conf\" file to    \"/usr/local/etc/wireguard/\" (or \"/etc/wireguard/\").    You'll need to create the \" wireguard\" directory first. For your    example, you will have your config file located at: \" /usr/local/etc    /wireguard/mac_client.conf\" or, \"/etc/wireguard/mac_client.conf\"</p> </li> <li> <p>To activate the VPN: \"wg-quick up [name of the conf file without    including .conf extension]\". For example, in your case, running    <code>wg-quick up mac_client</code> - If the peer system is already configured    and its interface is up, then the VPN connection should establish    automatically, and you should be able to start routing traffic through the peer.</p> </li> </ol> <p>Use <code>wg-quick down mac_client</code> to take the VPN connection down.</p>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#ii-using-wireguard-gui-app","title":"II. Using WireGuard GUI App","text":"<ol> <li> <p>Download WireGuard Client from the macOS App Store</p> <p>You can find the official WireGuard Client app on the App Store here.</p> <p></p> </li> <li> <p>Set up the VPN with WireGuard</p> <p>Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys.</p> <p>Open the WireGuard GUI by directly clicking WireGuard icon in status bar at the top-right corner of your screen.</p> <p></p> <p>And then click on \"Import tunnel(s) from file\" menu to load your client config file.</p> <p></p> <p>OR,</p> <p>Find and click the WireGUard GUI from your Launchpad and then either click on Add Tunnel -&gt; Import tunnel(s) from file\u2026 or, just click on \"Import tunnel(s) from file\" button located at the center.</p> <p></p> <p>Browse to the configuration file:</p> <p></p> <p>The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen.</p> <p></p> <p></p> <p>If you would like your computer to automatically connect to the WireGuard VPN server as soon as either (or both) Ethernet or Wi-Fi network adapter becomes active, check the relevant 'On-Demand' checkboxes for \"Ethernet\" and \" Wi-Fi\".</p> <p>Checking Exclude private IPs will generate a list of networks which excludes the server IP address and add them to the AllowedIPs list. This setting allows you to pass all your traffic through your Wireguard VPN EXCLUDING private address ranges like 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16.</p> <p></p> </li> <li> <p>Test your connection</p> <p>On your Windows machine, press the \"Activate\" button. You should see a successful connection be made:</p> <p></p> <p>After a few seconds, the status should change to Active.</p> <p>Clicking \"Deactivate\" button from the GUI's interface or directly clicking \"Deactivate\" menu from the WireGuard icon in status bar at the top-right corner of your screen closes the VPN connection.</p> <p></p> </li> </ol>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#linux","title":"Linux","text":"<p>WireGuard is available through the package management system on most Linux distributions.</p> <p>On Debian/Ubuntu:</p> <pre><code>sudo apt update\nsudo apt-get install wireguard resolvconf -y\n</code></pre> <p>On RedHat/Rocky/AlmaLinux:</p> <pre><code>sudo dnf install wireguard\n</code></pre> <p>Then, to run WireGuard using the client profile: Move the VPN client profile (configuration) file to /etc/wireguard/:</p> <pre><code>sudo mv nerc.conf /etc/wireguard/client.conf\n</code></pre> <p>Restart the WireGuard daemon (i.e., This will start WireGuard connection and will automatically run on boot):</p> <pre><code>sudo /etc/init.d/wireguard start\n</code></pre> <p>OR,</p> <pre><code>sudo systemctl enable --now wg-quick@client\nsudo systemctl start wg-quick@client\n</code></pre> <p>OR,</p> <pre><code>wg-quick up /etc/wireguard/client.conf\n</code></pre> <p>Checking the status:</p> <pre><code>systemctl status wg-quick@client\n</code></pre> <p>Alternatively, if you want to run WireGuard manually each time, then run:</p> <pre><code>sudo wireguard --config /etc/wireguard/client.conf\n</code></pre> <p>OR,</p> <pre><code>sudo wireguard --config nerc.conf\n</code></pre>"},{"location":"openstack/create-and-connect-to-the-VM/using-vpn/wireguard/#to-test-the-connection","title":"To test the connection","text":"<p>Once you are connected to the WireGuard server, you can run commands like shown below in your terminal to connect to the private instances: <code>ssh ubuntu@192.168. 0.40 -A -i cloud.key</code></p> <p></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/","title":"Data Transfer","text":""},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#data-transfer-tofrom-nerc-vm","title":"Data Transfer To/From NERC VM","text":""},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#transfer-using-volume","title":"Transfer using Volume","text":"<p>You may wish to transfer a volume which includes all data to a different project which can be your own (with access in project dropdown list) or external collaborators with on NERC. For this you can follow this guide.</p> <p>Very Important Note</p> <p>If you transfer the volume then that will be removed from the source and will only be available on destination project.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-globus","title":"Using Globus","text":"<p>Globus is a web-based service that is the preferred method for transferring substantial data between NERC VM and other locations. It effectively tackles the typical obstacles researchers encounter when moving, sharing, and storing vast quantities of data. By utilizing Globus, you can delegate data transfer tasks to a managed service that oversees the entire process. This service monitors performance and errors, retries failed transfers, automatically resolves issues whenever feasible, and provides status updates to keep you informed. This allows you to concentrate on your research while relying on Globus to handle data movement efficiently. For information on the user-friendly web interface of Globus and its flexible REST/API for creating scripted tasks and operations, please visit Globus.org.</p> <p>Important Information</p> <p>For large data sets and/or for access by external users, consider using Globus. An institutional endpoint/collection is not required to use Globus - you can set up a personal endpoint on your NERC VM and also on your local machine if you need to transfer large amounts of data.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#setting-up-a-personal-globus-endpoint-on-nerc-vm","title":"Setting up a Personal Globus Endpoint on NERC VM","text":"<p>You can do this using Globus Connect Personal to configure an endpoint on your NERC VM. In general, it is always fastest to setup a Personal endpoint on your NERC VM, and then use that endpoint for transfers to/from a local machine or any other shared or private Globus endpoints.</p> <p>You can find instructions for downloading and installing the Globus Connect Personal on the Globus web site.</p> <p>Helpful Tip</p> <p>You may get a \"Permission Denied\" error for certain paths with Globus Connect Personal. If you do, you may need to add this path to your list of allowed paths for Globus Connect Personal. You can do this by editing the <code>~/.globusonline/lta/config-paths</code> file and adding the new path as a line in the end of the list. The path must be followed by sharing (<code>0</code>/<code>1</code>) and R/W (<code>0</code>/<code>1</code>) flags.</p> <p>For example, to enable read-write access to the /data/tables directory, add the following line i.e. <code>/data/tables,0,1</code>.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#usage-of-globus","title":"Usage of Globus","text":"<p>Once a Personal Endpoint is set up on a NERC VM, you will be able to find that named collection on Globus file explorer and then can be chosen as source or destination for data transfer to/from another Guest Collection (Globus Shared Endpoints). Login into the Globus web interface, select your organization which will allow you to log in to Globus, and land on File Manager page.</p> <p>If your account belong to Globus Subscription that you will be able to use data transfers between two personal endpoints i.e. you can setup your local machine as another personal endpoint.</p> <p></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-scp","title":"Using SCP","text":"<p>Important Information</p> <p>SCP is suggested for smaller files (&lt;~10GB), otherwise use Globus. When you want to transfer many small files in a directory, we recommend Globus.</p> <p>We generally recommend using SCP (Secure Copy) to copy data to and from your VM. SCP is used to securely transfer files between two hosts using the Secure Shell (ssh) protocol. It's usage is simple, but the order that file locations are specified is crucial. SCP always expects the 'from' location first, then the 'to' destination. Depending on which is the remote system, you will prefix your username and Floating IP of your NERC VM.</p> <p><code>scp [username@Floating_IP:][location of file] [destination of file]</code></p> <p>or,</p> <p><code>scp [location of file] [username@Floating_IP:][destination of file]</code></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#usage","title":"Usage","text":"<p>Below are some examples of the two most common scenarios of SCP to copy to and from various sources.</p> <p>Helpful Tip</p> <p>We use '~' in the examples. The tilde '~' is a Unix short-hand that means \"my home directory\". So if user <code>almalinux</code> uses <code>~/</code> this is the same as typing out the full path to almalinux user's home directory (easier to remember than <code>/home/almalinux/</code>). You can, of course, specify other paths (ex. \u2013 <code>/user/almalinux/output/files.zip</code>) Also, we use <code>.</code> in the examples to specify the current directory path from where the command is issued. This can be replaced with the actual path.</p> <p>i. Copying Files From the NERC VM to Another Computer:</p> <p>From a terminal/shell from your local machine, you'll issue your SCP command by specifying the SSH Private Key to connect with the VM that has included corresponding SSH Public Key. The syntax is:</p> <pre><code>scp -i &lt;Your SSH Private Key including Path&gt; &lt;Default User name based on OS&gt;@&lt;Your Floating IP of VM&gt;:~/&lt;File In VM&gt; .\n</code></pre> <p>This copies the file <code>&lt;File In VM&gt;</code> from your VM's default user's directory (<code>~</code> is a Unix shortcut for <code>my home directory</code>) on your VM to your current directory (<code>.</code> is a Unix shortcut the current directory) on your computer from where the command is issued or you can specify the actual path instead of <code>.</code>.</p> <p>For e.g.</p> <pre><code>scp -i ~/.ssh/your_pem_key_file.pem almalinux@199.94.60.219:~/myfile.zip /my_local_directory/\n</code></pre> <p>ii. Copying Files From Another Computer to the NERC VM:</p> <p>From a terminal/shell on your computer (or another server or cluster) where you have access to the SSH Private Key, you'll issue your SCP command. The syntax is:</p> <pre><code>scp -i &lt;Your SSH Private Key including Path&gt; ./&lt;Your Local File&gt; &lt;Default User name based on OS&gt;@&lt;Your Floating IP of VM&gt;:~/`\n</code></pre> <p>This copies the file <code>&lt;Your Local File&gt;</code> from the current directory on the computer you issued the command from, to your home directory on your NERC VM. (recall that <code>.</code> is a Unix shortcut for the current directory path and <code>~</code> is a Unix shortcut for <code>my home directory</code>)</p> <p>For e.g.</p> <pre><code>scp -i ~/.ssh/your_pem_key_file.pem ./myfile.zip almalinux@199.94.60.219:~/myfile.zip\n</code></pre> <p>Important Note</p> <p>While it's probably best to compress all the files you intend to transfer into one file, this is not always an option. To copy the contents of an entire directory, you can use the <code>-r</code> (for recursive) flag.</p> <p>For e.g.</p> <pre><code>scp -i ~/.ssh/your_pem_key_file.pem -r almalinux@&lt;Floating_IP&gt;:~/mydata/ ./destination_directory/\n</code></pre> <p>This copies all the files from <code>~/mydata/</code> on the cluster to the current directory (i.e. <code>.</code>) on the computer you issued the command from. Here we can replace <code>./</code> with actual full path on you local machine and also <code>~/</code> with actual full path on your NERC VM.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-tarssh","title":"Using tar+ssh","text":"<p>When you want to transfer many small files in a directory, we recommend Globus. If you don't wish to use Globus, you can consider using ssh piped with tar.</p> <p>i. Send a directory to NERC VM:</p> <pre><code>tar cz /local/path/dirname | ssh -i &lt;Your SSH Private Key including Path&gt; &lt;Default User name based on OS&gt;@&lt;Your Floating IP of VM&gt; tar zxv -C /remote/path\n</code></pre> <p>ii. Get a directory from NERC VM:</p> <pre><code>ssh -i &lt;Your SSH Private Key including Path&gt; &lt;Default User name based on OS&gt;@&lt;Your Floating IP of VM&gt; tar cz /remote/path/dirname | tar zxv -C /local/path\n</code></pre>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-rsync","title":"Using rsync","text":"<p>Rsync is a fast, versatile, remote (and local) file-copying tool. It is famous for its delta-transfer algorithm, which reduces the amount of data sent over the network by sending only the differences between the source files and the existing files in the destination. This can often lead to efficiencies in repeat-transfer scenarios, as rsync only copies files that are different between the source and target locations (and can even transfer partial files when only part of a file has changed). This can be very useful in reducing the amount of copies you may perform when synchronizing two datasets.</p> <p>The basic syntax is: <code>rsync SOURCE DESTINATION</code> where <code>SOURCE</code> and <code>DESTINATION</code> are filesystem paths. They can be local, either absolute or relative to the current working directory, or they can be remote but prefixing something like <code>USERNAME@HOSTNAME:</code> to the front of them.</p> <p>i. Synchronizing from a local machine to NERC VM:</p> <pre><code>rsync -avxz ./source_directory/ -e \"ssh -i ~/.ssh/your_pem_key_file.pem\" &lt;user_name&gt;@&lt;Floating_IP&gt;:~/destination_directory/\n</code></pre> <p>ii. Synchronizing from NERC VM to a local machine:</p> <pre><code>rsync -avz -e \"ssh -i ~/.ssh/your_pem_key_file.pem\" -r &lt;user_name&gt;@&lt;Floating_IP&gt;:~/source_directory/ ./destination_directory/\n</code></pre> <p>iii. Update a previously made copy of \"foo\" on the NERC VM after you've made changes to the local copy:</p> <pre><code>rsync -avz --delete foo/ -e \"ssh -i ~/.ssh/your_pem_key_file.pem\" &lt;user_name&gt;@&lt;Floating_IP&gt;:~/foo/\n</code></pre> <p>Be careful with this option!</p> <p>The <code>--delete</code> option has no effect when making a new copy, and therefore can be used in the previous example too (making the commands identical), but since it recursively deletes files, it's best to use it sparingly. If you want to maintain a mirror (i.e. the <code>DESTINATION</code> is to be an exact copy of the <code>SOURCE</code>) then you will want to add the <code>--delete</code> option. This deletes files/directories in the <code>DESTINATION</code> that are no longer in the <code>SOURCE</code>.</p> <p>iv. Update a previously made copy of \"foo\" on the NERC VM after you or someone else has already updated it from a different source:</p> <pre><code>rsync -aAvz --update foo/ -e \"ssh -i ~/.ssh/your_pem_key_file.pem\" &lt;user_name&gt;@&lt;Floating_IP&gt;:~/foo/\n</code></pre> <p>Information</p> <p>The <code>--update</code> option has no effect when making a new copy and can also be specified in that case. If you're updating a master copy (i.e. the <code>DESTINATION</code> may have files that are newer than the version(s) in <code>SOURCE</code>) then you will also want to add the <code>--update</code> option. This will leave those files alone and not revert them to the older copy in <code>SOURCE</code>.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#progress-verbosity-statistics","title":"Progress, Verbosity, Statistics","text":"<p><code>-v</code> Verbose mode  -  list each file transferred. Adding more vs makes it more verbose.</p> <p><code>--progress</code> Show a progress meter for each individual file transfer that is part of the entire operation. If you have many small files then this option can significantly slow down the transfer.</p> <p><code>--stats</code> Print a short paragraph of statistics at the end of the session (e.g. average transfer rate, total number of files transferred, etc).</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#other-useful-options","title":"Other Useful Options","text":"<p><code>--dry-run</code> Perform a dry-run of the session instead of actually modifying the <code>DESTINATION</code>. Mostly useful when adding multiple <code>-v</code> options, especially for verifying <code>--delete</code> is doing what you want.</p> <p><code>--exclude PATTERN</code> Skip files/directories in the <code>SOURCE</code> that match a given pattern (supports regular expressions)</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-rclone","title":"Using Rclone","text":"<p><code>rclone</code> is a convenient and performant command-line tool for transferring files and synchronizing directories directly between your local file systems and a given NERC VM.</p> <p>Prerequisites:</p> <p>To run the <code>rclone</code> commands, you need to have:</p> <ul> <li>To run the <code>rclone</code> commands you will need to have <code>rclone</code> installed.     See Downloading and Installing the latest version of Rclone     for more information.</li> </ul>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#configuring-rclone","title":"Configuring Rclone","text":"<p>First you'll need to configure <code>rclone</code>. The filesystem protocols, especially, can have complicated authentication parameters so it's best to place these details in a config file.</p> <p>If you run <code>rclone config file</code> you will see where the default location is for your current user.</p> <p>Note</p> <p>For Windows users, you may need to specify the full path to the Rclone executable file if it's not included in your system's <code>%PATH%</code> variable.</p> <p>Edit the config file's content on the path location described by <code>rclone config file</code> command and add the following entry with the name [nerc]:</p> <pre><code>[nerc]\ntype = sftp\nhost = 199.94.60.219\nuser = almalinux\nport =\npass =\nkey_file = C:\\Users\\YourName\\.ssh\\cloud.key\nshell_type = unix\n</code></pre> <p>More about the config for SFTP can be found here.</p> <p>OR, You can locally copy this content to a new config file and then use this flag to override the config location, e.g. <code>rclone --config=FILE</code></p> <p>Interactive Configuration</p> <p>Run <code>rclone config</code> to setup. See Rclone config docs for more details.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#how-to-use-rclone","title":"How to use Rclone","text":"<p><code>rclone</code> supports many subcommands (see the complete list of Rclone subcommands). A few commonly used subcommands (assuming you configured the NERC VM filesystem as <code>nerc</code>):</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#listing-files-and-folders","title":"Listing Files and Folders","text":"<p>Once your NERC VM filesystem has been configured in Rclone, you can then use the Rclone interface to List all the directories with the \"lsd\" command:</p> <pre><code>rclone lsd \"nerc:\"\n</code></pre> <p>or,</p> <pre><code>rclone lsd \"nerc:\" --config=rclone.conf\n</code></pre> <p>For e.g.</p> <pre><code>rclone lsd \"nerc:\" --config=rclone.conf\n        -1 2023-07-06 12:18:24        -1 .ssh\n        -1 2023-07-06 19:27:19        -1 destination_directory\n</code></pre> <p>To list the files and folders available within the directory (i.e. \"destination_directory\") we can use the \"ls\" command:</p> <pre><code>rclone ls \"nerc:destination_directory/\"\n  653 README.md\n    0 image.png\n   12 test-file\n</code></pre>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#uploading-and-downloading-files-and-folders","title":"Uploading and Downloading Files and Folders","text":"<p><code>rclone</code> support a variety of options to allow you to copy, sync, and move files from one destination to another.</p> <p>A simple example of this can be seen below where we copy/upload the file <code>upload.me</code> to the <code>&lt;your-directory&gt;</code> directory:</p> <pre><code>rclone copy \"./upload.me\" \"nerc:&lt;your-directory&gt;/\"\n</code></pre> <p>Another example, to copy/download the file <code>upload.me</code> from the remote directory, <code>&lt;your-directory&gt;</code>, to your local machine:</p> <pre><code>rclone -P copy \"nerc:&lt;your-directory&gt;/upload.me\" \"./\"\n</code></pre> <p>Also, to Sync files into the <code>&lt;your-directory&gt;</code> directory it's recommended to first try with <code>--dry-run</code> first. This will give you a preview of what would be synced without actually performing any transfers.</p> <pre><code>rclone --dry-run sync /path/to/files nerc:&lt;your-directory&gt;\n</code></pre> <p>Then sync for real</p> <pre><code>rclone sync --interactive /path/to/files nerc:&lt;your-directory&gt;\n</code></pre>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#mounting-vm-filesystem-on-local-filesystem","title":"Mounting VM filesystem on local filesystem","text":"<p>Linux:</p> <p>First, you need to create a directory on which you will mount your filesystem:</p> <p><code>mkdir ~/mnt-rclone</code></p> <p>Then you can simply mount your filesystem with:</p> <p><code>rclone -vv --vfs-cache-mode writes mount nerc: ~/mnt-rclone</code></p> <p>Windows:</p> <p>First you have to download Winfsp:</p> <p>WinFsp is an open source Windows File System Proxy which provides a FUSE emulation layer.</p> <p>Then you can simply mount your VM's filesystem with (no need to create the directory in advance):</p> <p><code>rclone -vv --vfs-cache-mode writes mount nerc: C:/mnt-rclone</code></p> <p>The <code>vfs-cache-mode</code> flag enables file caching. You can use either the <code>writes</code> or <code>full</code> option. For further explanation you can see the official documentation.</p> <p>Now that your VM's filesystem is mounted locally, you can list, create, and delete files in it.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#unmount-nerc-vm-filesystem","title":"Unmount NERC VM filesystem","text":"<p>To unmount, simply press <code>CTRL-C</code> and the mount will be interrupted.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-graphical-user-interface-gui-tools","title":"Using Graphical User Interface (GUI) Tools","text":""},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#i-winscp","title":"i. WinSCP","text":"<p>WinSCP is a popular and free open-source SFTP client, SCP client, and FTP client for Windows. Its main function is file transfer between a local and a remote computer, with some basic file management functionality using FTP, FTPS, SCP, SFTP, WebDAV, or S3 file transfer protocols.</p> <p>Prerequisites:</p> <ul> <li> <p>WinSCP installed, see Download and Install the latest version of the WinSCP     for more information.</p> </li> <li> <p>Go to WinSCP menu and open \"View &gt; Preferences\".</p> </li> <li> <p>When the \"Preferences\" dialog window appears, select \"Transfer\" in the options     on the left pane.</p> </li> <li> <p>Click on the \"Edit\" button.</p> </li> <li> <p>Then, in the popup dialog box, review the \"Common options\" group and uncheck     the \"Preserve timestamp\" option as shown below:</p> </li> </ul> <p></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#configuring-winscp","title":"Configuring WinSCP","text":"<ul> <li>Click on the \"New Tab\" button, as shown below:</li> </ul> <ul> <li>Select either \"SFTP\" or \"SCP\" from the \"File protocol\" dropdown options     as shown below:</li> </ul> <ul> <li>Provide the following required information:</li> </ul> <p>\"File protocol\": Choose either \"\"SFTP\" or \"SCP\"\"</p> <p>\"Host name\": \"<code>&lt;Your Floating IP of VM&gt;</code>\"</p> <p>\"Port number\": \"22\"</p> <p>\"User name\": \"<code>&lt;Default User name based on OS&gt;</code>\"</p> <p>Default User name based on OS</p> <ul> <li> <p>all Ubuntu images: ubuntu</p> </li> <li> <p>all AlmaLinux images: almalinux</p> </li> <li> <p>all Rocky Linux images: rocky</p> </li> <li> <p>all Fedora images: fedora</p> </li> <li> <p>all Debian images: debian</p> </li> <li> <p>all RHEL images: cloud-user</p> </li> </ul> <p>If you still have VMs running with deleted CentOS images, you need to use the following default username for your CentOS images: <code>centos</code>.</p> <p>\"Password\": \"<code>&lt;Leave blank as you using SSH key&gt;</code>\"</p> <ul> <li>Change Authentication Options</li> </ul> <p>Before saving, click the \"Advanced\" button. In the \"Advanced Site Settings\", under \"SSH &gt;&gt; Authentication\" settings, check \"Allow agent forwarding\" and select the private key file with <code>.ppk</code> extension from the file picker.</p> <p></p> <p>Helpful Tip</p> <p>You can save your above configured site with some preferred name by clicking the \"Save\" button and then giving a proper name to your site. This prevents needing to manually enter all of your configuration again the next time you need to use WinSCP.</p> <p></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-winscp","title":"Using WinSCP","text":"<p>You can follow the above steps to manually add a new site the next time you open WinSCP, or you can connect to your previously saved site. Saved sites will be listed in the popup dialog and can be selected by clicking on the site name.</p> <p>Then click the \"Login\" button to connect to your NERC project's VM as shown below:</p> <p></p> <p></p> <p>You should now be connected to the VM's remote directories/files. You can drag and drop your files to/from file windows to begin transfer. When you're finished, click the \"X\" icon in the top right to disconnect.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#ii-cyberduck","title":"ii. Cyberduck","text":"<p>Cyberduck is a libre server and cloud storage browser for Mac and Windows. Its user-friendly interface enables seamless connections to servers, enterprise file sharing, and various cloud storage platforms.</p> <p>Prerequisites:</p> <ul> <li>Cyberduck installed, see Download and Install the latest version of the Cyberduck     for more information.</li> </ul>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#configuring-cyberduck","title":"Configuring Cyberduck","text":"<ul> <li>Click on the \"Open Connection\" button, as shown below:</li> </ul> <ul> <li>Select either \"SFTP\" or \"FTP\" from the dropdown options as shown below:</li> </ul> <ul> <li>Provide the following required information:</li> </ul> <p>\"Server\": \"<code>&lt;Your Floating IP of VM&gt;</code>\"</p> <p>\"Port\": \"22\"</p> <p>\"User name\": \"<code>&lt;Default User name based on OS&gt;</code>\"</p> <p>Default User name based on OS</p> <ul> <li> <p>all Ubuntu images: ubuntu</p> </li> <li> <p>all AlmaLinux images: almalinux</p> </li> <li> <p>all Rocky Linux images: rocky</p> </li> <li> <p>all Fedora images: fedora</p> </li> <li> <p>all Debian images: debian</p> </li> <li> <p>all RHEL images: cloud-user</p> </li> </ul> <p>\"Password\": \"<code>&lt;Leave blank as you using SSH key&gt;</code>\"</p> <p>\"SSH Private Key\": \"Choose the appropriate SSH Private Key from your local machine that has the corresponding public key attached to your VM\"</p> <p></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-cyberduck","title":"Using Cyberduck","text":"<p>Then click the \"Connect\" button to connect to your NERC VM as shown below:</p> <p></p> <p>You should now be connected to the VM's remote directories/files. You can drag and drop your files to/from file windows to begin transfer. When you're finished, click the \"X\" icon in the top right to disconnect.</p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#iii-filezilla","title":"iii. Filezilla","text":"<p>Filezilla is a free and open source SFTP client which is built on modern standards. It is available cross-platform (Mac, Windows and Linux) and is actively maintained. You can transfer files to/from the cluster from your computer or any resources connected to your computer (shared drives, Dropbox, etc.)</p> <p>Prerequisites:</p> <ul> <li>Filezilla installed, see     Download and Install the latest version of the Filezilla     for more information.</li> </ul>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#configuring-filezilla","title":"Configuring Filezilla","text":"<ul> <li>Click on \"Site Manager\" icon as shown below:</li> </ul> <ul> <li>Click on \"New Site\" as shown below:</li> </ul> <ul> <li>Select either \"SFTP\" or \"FTP\" from the dropdown options as shown below:</li> </ul> <ul> <li>Provide the following required information:</li> </ul> <p>\"Server\": \"<code>&lt;Your Floating IP of VM&gt;</code>\"</p> <p>\"Port\": \"22\"</p> <p>\"Logon Type\": \"Key file\" from the dropdown option</p> <p>\"User\": \"<code>&lt;Default User name based on OS&gt;</code>\"</p> <p>Default User name based on OS</p> <ul> <li> <p>all Ubuntu images: ubuntu</p> </li> <li> <p>all AlmaLinux images: almalinux</p> </li> <li> <p>all Rocky Linux images: rocky</p> </li> <li> <p>all Fedora images: fedora</p> </li> <li> <p>all Debian images: debian</p> </li> <li> <p>all RHEL images: cloud-user</p> </li> </ul> <p>If you still have VMs running with deleted CentOS images, you need to use the following default username for your CentOS images: <code>centos</code>.</p> <p>\"Key file\": \"Browse and choose the appropriate SSH Private Key from you local machine that has corresponding Public Key attached to your VM\"</p> <p></p>"},{"location":"openstack/data-transfer/data-transfer-from-to-vm/#using-filezilla","title":"Using Filezilla","text":"<p>Then click \"Connect\" button to connect to your NERC VM as shown below:</p> <p></p> <p>You should now be connected to the VM and see your local files in the left-hand pane and the remote files in the right-hand pane. You can drag and drop between them or drag and drop to/from file windows on your computer. When you're finished, click the \"X\" icon in the top right to disconnect.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/","title":"Decommission your OpenStack Resources","text":""},{"location":"openstack/decommission/decommission-openstack-resources/#decommission-your-nerc-openstack-resources","title":"Decommission Your NERC OpenStack Resources","text":"<p>You can decommission all of your NERC OpenStack resources sequentially as outlined below.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Backup: Back up any critical data or configurations stored on the resources     that going to be decommissioned. This ensures that important information is not     lost during the process. You can refer to this guide     to initiate and carry out data transfer to and from the virtual machine.</p> </li> <li> <p>Shutdown Instances: If applicable, Shut Off any running instances     to ensure they are not actively processing data during decommissioning.</p> </li> <li> <p>Setup OpenStack CLI, see OpenStack Command Line setup     for more information.</p> </li> </ul>"},{"location":"openstack/decommission/decommission-openstack-resources/#delete-all-vms","title":"Delete all VMs","text":"<p>For instructions on deleting instance(s), please refer to this documentation.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#delete-volumes-and-snapshots","title":"Delete volumes and snapshots","text":"<p>For instructions on deleting volume(s), please refer to this documentation.</p> <p>To delete snapshot(s), if that snapshot is not used for any running instance.</p> <p>Navigate to Project -&gt; Volumes -&gt; Snapshots.</p> <p></p> <p>Unable to Delete Snapshots</p> <p>First delete all volumes and instances (and its attached volumes) that are created using the snapshot first, you will not be able to delete the volume snapshots.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#delete-all-custom-built-images-and-instance-snapshot-built-images","title":"Delete all custom built Images and Instance Snapshot built Images","text":"<p>Navigate to Project -&gt; Compute -&gt; Images.</p> <p>Select all of the custom built that have Visibility set as \"Private\" images to delete.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#delete-your-all-private-networks-routers-and-internal-interfaces-on-the-routers","title":"Delete your all private Networks, Routers and Internal Interfaces on the Routers","text":"<p>To review all Network and its connectivities, you need to:</p> <p>Navigate to Project -&gt; Network -&gt; Network Topology.</p> <p>This will shows all view of current Network in your project in Graph or Topology view. Make sure non instances are connected to your private network, which is setup by following this documentation. If there are any instances then refer this to delete those VMs.</p> <p></p> <p>First, delete all other Routers used to create private networks, which is setup by following this documentation except <code>default_router</code> from:</p> <p>Navigate to Project -&gt; Network -&gt; Routers.</p> <p>First, delete all other Routers used to create private networks except <code>default_network</code> and <code>provider</code> then only you will be able to delete the Networks from:</p> <p>Navigate to Project -&gt; Network -&gt; Networks.</p> <p>Unable to Delete Networks</p> <p>First delete all instances and then delete all routers then only you will be able to delete the associated private networks.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#release-all-floating-ips","title":"Release all Floating IPs","text":"<p>Navigate to Project -&gt; Network -&gt; Floating IPs.</p> <p></p> <p>For instructions on releasing your allocated Floating IP back into the NERC floating IP pool, please refer to this documentation.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#clean-up-all-added-security-groups","title":"Clean up all added Security Groups","text":"<p>First, delete all other security groups except <code>default</code> also make sure the <code>default</code> security group does not have any extra rules. To view all Security Groups:</p> <p>Navigate to Project -&gt; Network -&gt; Security Groups.</p> <p>Unable to Delete Security Groups</p> <p>First delete all instances and then only you will be able to delete the security groups. If a security group is attached to a VM, that security group will not be allowed to delete.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#delete-all-of-your-stored-key-pairs","title":"Delete all of your stored Key Pairs","text":"<p>Navigate to Project -&gt; Compute -&gt; Key Pairs.</p> <p>Unable to Delete Key Pairs</p> <p>First delete all instances that are using the selected Key Pairs then only you will be able to delete them.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#delete-all-buckets-and-objects","title":"Delete all buckets and objects","text":"<p>For instructions on deleting bucket(s) along with all objects, please refer to this documentation.</p> <p>To delete snapshot(s), if that snapshot is not used for any running instance.</p> <p>Navigate to Project -&gt; Object Store -&gt; Containers.</p> <p></p> <p>Unable to Delete Container with Objects inside</p> <p>First delete all objects inside a Container first, then only you will be able to delete the container. Please make sure any critical objects data are already been remotely backed up before deleting them. You can also use openstack client to recursively delete the containers which has multi-level objects inside as described here. So, you don't need to manually delete all objects inside a container prior deleting the container. This will save a lot of your time and effort.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#optional-remove-users-from-your-coldfront-project","title":"Optional: Remove User(s) from your ColdFront Project","text":"<p>Removing users from your ColdFront project via the NERC's ColdFront interface is straightforward. Simply click the \"Remove Users\" button for the corresponding project. This will display the following interface:</p> <p></p> <p>PI or project managers can select the user(s) and then click on the \"Remove Selected Users From Project\" button.</p> <p>Very Important</p> <p>If you remove a user (or users) from a project, they will automatically be removed from all allocations they were previously assigned to within that project.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#use-coldfront-to-reduce-the-storage-quota-to-zero","title":"Use ColdFront to reduce the Storage Quota to Zero","text":"<p>Each allocation, whether requested or approved, will be billed based on the pay-as-you-go model. The exception is for Storage quotas, where the cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool. For NERC (OpenStack) Resource Allocations, storage quotas are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes.</p> <p>Even if you have deleted all volumes, snapshots, and object storage buckets and objects in your OpenStack project. It is very essential to adjust the approved values for your NERC (OpenStack) resource allocations to zero (0) otherwise you still be incurring a charge for the approved storage as explained in Billing FAQs.</p> <p>To achieve this, you must submit a final change request to reduce the Storage Quotas for the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" to zero (0) for your NERC (OpenStack) resource type. You can review and manage these resource allocations by visiting the resource allocations. Here, you can filter the allocation of your interest and then proceed to request a change request.</p> <p>Please make sure your change request looks like this:</p> <p></p> <p>Wait until the requested resource allocation gets approved by the NERC's admin.</p> <p>After approval, kindly review and verify that the quotas are accurately reflected in your resource allocation and OpenStack project. Please ensure that the approved quota values are accurately displayed as explained here.</p>"},{"location":"openstack/decommission/decommission-openstack-resources/#review-your-block-storagevolumecinder-quota","title":"Review your Block Storage(Volume/Cinder) Quota","text":"<p>Please confirm and verify that the <code>gigabytes</code> resource value that specifies total space in external volumes is set to a limit of zero (0) in correspondence with the approved \"OpenStack Volume Quota (GiB)\" of your allocation when running <code>openstack quota show</code> openstack client command as shown below:</p> <pre><code>openstack quota show\n+-----------------------+--------+\n| Resource              |  Limit |\n+-----------------------+--------+\n...\n| gigabytes             |      0 |\n...\n+-----------------------+--------+\n</code></pre>"},{"location":"openstack/decommission/decommission-openstack-resources/#review-your-object-storageswift-quota","title":"Review your Object Storage(Swift) Quota","text":"<p>To check the overall space used, you can use the following command</p> <p>Also, please confirm and verify that the <code>Quota-Bytes</code> property value is set to a limit of zero (0) in correspondence with the approved \"OpenStack Swift Quota (GiB)\" of your allocation and also check the overall space used in <code>Bytes</code> is one (1) along with no Containers and Objects, when running <code>openstack object store account show</code> openstack client command as shown below:</p> <pre><code>openstack object store account show\n+------------+---------------------------------------+\n| Field      | Value                                 |\n+------------+---------------------------------------+\n| Account    | AUTH_5e1cbcfe729a4c7e8fb2fd5328456eea |\n| Bytes      | 0                                     |\n| Containers | 0                                     |\n| Objects    | 0                                     |\n| properties | Quota-Bytes='1'                       |\n+------------+---------------------------------------+\n</code></pre>"},{"location":"openstack/decommission/decommission-openstack-resources/#review-your-project-usage","title":"Review your Project Usage","text":"<p>Several commands are available to access project-level resource utilization details. The <code>openstack limits show --absolute</code> command offers a comprehensive view of the most crucial resources and also allows you to view your current resource consumption.</p> <p>Multiple commands are at your disposal to access project resource utilization details. The openstack limits show --absolute command offers a comprehensive view of critical resources and allows you to assess your current resource consumption.</p> <p>Very Important: Ensure No Resources that will be Billed are Used</p> <p>Most importantly, ensure that there is no active usage for any of your currently allocated project resources.</p> <p>Please ensure the output appears as follows, with all used resources having a value of zero (0), except for <code>totalSecurityGroupsUsed</code>.</p> <pre><code>openstack limits show --absolute\n+--------------------------+-------+\n| Name                     | Value |\n+--------------------------+-------+\n...\n| totalRAMUsed             |     0 |\n| totalCoresUsed           |     0 |\n| totalInstancesUsed       |     0 |\n| totalFloatingIpsUsed     |     0 |\n| totalSecurityGroupsUsed  |     1 |\n| totalServerGroupsUsed    |     0 |\n...\n| totalVolumesUsed         |     0 |\n| totalGigabytesUsed       |     0 |\n| totalSnapshotsUsed       |     0 |\n| totalBackupsUsed         |     0 |\n| totalBackupGigabytesUsed |     0 |\n+--------------------------+-------+\n</code></pre>"},{"location":"openstack/decommission/decommission-openstack-resources/#review-your-projects-resource-quota-from-the-openstack-dashboard","title":"Review your Project's Resource Quota from the OpenStack Dashboard","text":"<p>After removing all OpenStack resources and updating the Storage Quotas to set them to zero (0), you can review and verify that these changes are reflected in your Horizon Dashboard Overview.</p> <p>Navigate to Project -&gt; Compute -&gt; Overview.</p> <p></p>"},{"location":"openstack/decommission/decommission-openstack-resources/#finally-archive-your-coldfront-project","title":"Finally, Archive your ColdFront Project","text":"<p>As a PI, you will now be able to Archive your ColdFront Project via accessing NERC's ColdFront interface. Please refer to these intructions on how to archive your projects that need to be decommissioned.</p>"},{"location":"openstack/logging-in/access-the-openstack-dashboard/","title":"Access the OpenStack Dashboard","text":""},{"location":"openstack/logging-in/access-the-openstack-dashboard/#access-the-openstack-dashboard","title":"Access the OpenStack Dashboard","text":"<p>The OpenStack Dashboard which is a web-based graphical interface, code named Horizon, is located at https://stack.nerc.mghpcc.org.</p> <p>The NERC Authentication supports CILogon using Keycloak for gateway authentication and authorization that provides federated login via your institution accounts and it is the recommended authentication method.</p> <p>Make sure you are selecting \"OpenID Connect\" (which is selected by default) as shown here:</p> <p></p> <p>Next, you will be redirected to CILogon welcome page as shown below:</p> <p></p> <p>MGHPCC Shared Services (MSS) Keycloak will request approval of access to the following information from the user:</p> <ul> <li> <p>Your CILogon user identifier</p> </li> <li> <p>Your name</p> </li> <li> <p>Your email address</p> </li> <li> <p>Your username and affiliation from your identity provider</p> </li> </ul> <p>which are required in order to allow access your account on NERC's OpenStack dashboard.</p> <p>From the \"Selected Identity Provider\" dropdown option, please select your institution's name. If you would like to remember your selected institution name for future logins please check the \"Remember this selection\" checkbox this will bypass the CILogon welcome page on subsequent visits and proceed directly to the selected insitution's identity provider(IdP). Click \"Log On\". This will redirect to your respective institutional login page where you need to enter your institutional credentials.</p> <p>Important Note</p> <p>The NERC does not see or have access to your institutional account credentials, it points to your selected insitution's identity provider and redirects back once authenticated.</p> <p>Once you successfully authenticate, you should see an overview of the resources like Compute (instances, VCPUs, RAM, etc.), Volume and Network. You can also see usage summary for provided date range.</p> <p></p> <p>I can't find my virtual machine</p> <p>If you are a member of several projects i.e. ColdFront NERC (OpenStack) allocations, you may need to switch the project before you can see and use the OpenStack resources you or your team has created. Clicking on the project dropdown which is displayed near the top right side will popup the list of projects you are in. You can select the new project by hovering and clicking on the project name in that list as shown below:</p> <p></p>"},{"location":"openstack/logging-in/dashboard-overview/","title":"Dashboard Overview","text":""},{"location":"openstack/logging-in/dashboard-overview/#dashboard-overview","title":"Dashboard Overview","text":"<p>When you are logged-in, you will be redirected to the Compute panel which is under the Project tab. In the top bar, you can see the two small tabs: \"Project\" and \"Identity\".</p> <p>Beneath that you can see six panels in larger print: \"Project\", \"Compute\", \"Volumes\", \"Network\", \"Orchestration\", and \"Object Store\".</p>"},{"location":"openstack/logging-in/dashboard-overview/#project-panel","title":"Project Panel","text":"<p>Navigate: Project -&gt; Project</p> <ul> <li>API Access: View API endpoints.</li> </ul> <p></p>"},{"location":"openstack/logging-in/dashboard-overview/#compute-panel","title":"Compute Panel","text":"<p>Navigate: Project -&gt; Compute</p> <ul> <li>Overview: View reports for the project.</li> </ul> <p></p> <ul> <li> <p>Instances: View, launch, create a snapshot from, stop, pause, or reboot     instances, or connect to them through VNC.</p> </li> <li> <p>Images: View images and instance snapshots created by project users, plus any     images that are publicly available. Create, edit, and delete images, and launch     instances from images and snapshots.</p> </li> <li> <p>Key Pairs: View, create, edit, import, and delete key pairs.</p> </li> <li> <p>Server Groups: View, create, edit, and delete server groups.</p> </li> </ul>"},{"location":"openstack/logging-in/dashboard-overview/#volume-panel","title":"Volume Panel","text":"<p>Navigate: Project -&gt; Volume</p> <ul> <li> <p>Volumes: View, create, edit, delete volumes, and accept volume trnasfer.</p> </li> <li> <p>Backups: View, create, edit, and delete backups.</p> </li> <li> <p>Snapshots: View, create, edit, and delete volume snapshots.</p> </li> <li> <p>Groups: View, create, edit, and delete groups.</p> </li> <li> <p>Group Snapshots: View, create, edit, and delete group snapshots.</p> </li> </ul>"},{"location":"openstack/logging-in/dashboard-overview/#network-panel","title":"Network Panel","text":"<p>Navigate: Project -&gt; Network</p> <ul> <li>Network Topology: View the network topology.</li> </ul> <p></p> <ul> <li> <p>Networks: Create and manage public and private networks.</p> </li> <li> <p>Routers: Create and manage routers.</p> </li> <li> <p>Security Groups: View, create, edit, and delete security groups and security     group rules..</p> </li> <li> <p>Load Balancers: View, create, edit, and delete load balancers.</p> </li> <li> <p>Floating IPs: Allocate an IP address to or release it from a project.</p> </li> <li> <p>Trunks: View, create, edit, and delete trunk.</p> </li> </ul>"},{"location":"openstack/logging-in/dashboard-overview/#orchestration-panel","title":"Orchestration Panel","text":"<p>Navigate: Project-&gt;Orchestration</p> <ul> <li> <p>Stacks: Use the REST API to orchestrate multiple composite cloud applications.</p> </li> <li> <p>Resource Types: view various resources types and their details.</p> </li> <li> <p>Template Versions: view different heat templates.</p> </li> <li> <p>Template Generator: GUI to generate and save template using drag and drop resources.</p> </li> </ul>"},{"location":"openstack/logging-in/dashboard-overview/#object-store-panel","title":"Object Store Panel","text":"<p>Navigate: Project-&gt;Object Store</p> <ul> <li>Containers: Create and manage containers and objects. In future you would use     this tab to create Swift object storage     for your projects on a need basis.</li> </ul> <p></p>"},{"location":"openstack/management/vm-management/","title":"VM Management","text":""},{"location":"openstack/management/vm-management/#vm-management","title":"VM Management","text":"<p>RedHat OpenStack offers numerous functionalities for handling virtual machines, and comprehensive information can be found in the official OpenStack site user guide, please keep in mind that certain features may not be fully implemented at NERC OpenStack.</p>"},{"location":"openstack/management/vm-management/#instance-management-actions","title":"Instance Management Actions","text":"<p>After launching an instance (On the left side bar, click on Project -&gt; Compute -&gt; Instances), several options are available under the Actions menu located on the right hand side of your screen as shown here:</p> <p></p>"},{"location":"openstack/management/vm-management/#renaming-vm","title":"Renaming VM","text":"<p>Once a VM is created, its name is set based on user specified Instance Name while launching an instance using Horizon dashboard or  specified in <code>openstack server create ...</code> command using openstack client.</p> <p>To rename a VM, navigate to Project -&gt; Compute -&gt; Instances.</p> <p>Select an instance.</p> <p>In the menu list in the actions column, select \"Edit Instance\" by clicking on the arrow next to \"Create Snapshot\" as shown below:</p> <p></p> <p>Then edit the Name and also Description(Optional) in \"Information\" tab and save it:</p> <p></p>"},{"location":"openstack/management/vm-management/#stopping-and-starting","title":"Stopping and Starting","text":"<p>Virtual machines can be stopped and initiated using various methods, and these actions are executed through the openstack command with the relevant parameters.</p> <ol> <li> <p>Reboot is equivalent to powering down the machine and then restarting it. A    complete boot sequence takes place and thus the machine returns to use in a few    minutes.</p> <p>Soft Reboot:</p> <ul> <li> <p>A soft reboot attempts a graceful shut down and restart of the instance. It   sends an ACPI Restart request to the VM. Similar to sending a reboot command   to a physical computer.</p> </li> <li> <p>Click Action -&gt; Soft Reboot Instance.</p> </li> <li> <p>Status will change to <code>Reboot</code>.</p> </li> </ul> <p>Hard Reboot:</p> <ul> <li> <p>A hard reboot power cycles the instance. This forcibly restart your VM. Similar   to cycling the power on a physical computer.</p> </li> <li> <p>Click Action -&gt; Hard Reboot Instance.</p> </li> <li> <p>Status will change to <code>Hard Reboot</code>.</p> </li> </ul> </li> <li> <p>The <code>Pause</code> &amp; <code>Resume</code> feature enables the temporary suspension of the VM. While    in this state, the VM is retained in memory but doesn't receive any allocated    CPU time. This proves handy when conducting interventions on a group of servers,    preventing the VM from processing during the intervention.</p> <ul> <li> <p>Click Action -&gt; Pause Instance.</p> </li> <li> <p>Status will change to <code>Paused</code>.</p> </li> <li> <p>The Resume operation typically completes in less than a second by clicking   Action -&gt; Resume Instance.</p> </li> </ul> </li> <li> <p>The <code>Suspend</code> &amp; <code>Resume</code> function saves the VM onto disk and swiftly restores    it (in less than a minute). This process is quicker than the stop/start method,    and the VM resumes from where it was suspended, avoiding a new boot cycle.</p> <ul> <li> <p>Click Action -&gt; Suspend Instance.</p> </li> <li> <p>Status will change to <code>Suspended</code>.</p> </li> <li> <p>The Resume operation typically completes in less than a second by clicking   Action -&gt; Resume Instance.</p> </li> </ul> </li> <li> <p><code>Shelve</code> &amp; <code>Unshelve</code></p> <ul> <li> <p>Click Action -&gt; Shelve Instance.</p> </li> <li> <p>Releases all computing resources (i.e., GPU, vCPU, RAM), while preserving   the disk and metadata.</p> </li> <li> <p>We strongly recommend detaching volumes before shelving.</p> </li> <li> <p>Status will change to <code>Shelved Offloaded</code>.</p> </li> <li> <p>To unshelve the instance, click Action -&gt; Unshelve Instance.</p> </li> </ul> </li> <li> <p><code>Shut Off</code> &amp; <code>Start Instance</code></p> <ul> <li> <p>Click Action -&gt; Shut Off Instance.</p> </li> <li> <p>When shut off it stops active computing, consuming fewer resources than a Suspend.</p> </li> <li> <p>Status will change to <code>Shutoff</code>.</p> </li> <li> <p>To start the shut down VM, click Action -&gt; Start Instance.</p> </li> </ul> </li> </ol>"},{"location":"openstack/management/vm-management/#using-openstack-client-commands","title":"Using openstack client commands","text":"<p>The above mentioned actions can all be performed running the openstack client commands with the following syntax:</p> <pre><code>openstack server &lt;operation&gt; &lt;INSTANCE_NAME_OR_ID&gt;\n</code></pre> <p>such as,</p> <pre><code>openstack server shutoff my-vm\n\nopenstack server restart my-vm\n</code></pre> <p>Pro Tip</p> <p>If your instance name <code>&lt;INSTANCE_NAME_OR_ID&gt;</code> includes spaces, you need to enclose the name of your instance in quotes, i.e. <code>\"&lt;INSTANCE_NAME_OR_ID&gt;\"</code></p> <p>For example: <code>openstack server restart \"My Test Instance\"</code>.</p>"},{"location":"openstack/management/vm-management/#create-snapshot","title":"Create Snapshot","text":"<ul> <li> <p>Click Action -&gt; Create Snapshot.</p> </li> <li> <p>Instances must have status <code>Active</code>, <code>Suspended</code>, or <code>Shutoff</code> to create snapshot.</p> </li> <li> <p>This creates an image template from a VM instance also known as \"Instance Snapshot\"     as described here.</p> </li> <li> <p>The menu will automatically shift to Project -&gt; Compute -&gt; Images once the     image is created.</p> </li> <li> <p>The sole distinction between an image directly uploaded to the image data     service, glance and an image generated     through a snapshot is that the snapshot-created image possesses additional     properties in the glance database and defaults to being private.</p> </li> </ul> <p>Glance Image Service</p> <p>Glance is a central image repository which provides discovering, registering, retrieving for disk and server images. More about this service can be found here.</p>"},{"location":"openstack/management/vm-management/#rescue-a-vm","title":"Rescue a VM","text":"<p>There are instances where a virtual machine may encounter boot failure due to reasons like misconfiguration or issues with the system disk. To diagnose and address the problem, the virtual machine console offers valuable diagnostic information on the underlying cause.</p> <p>Alternatively, utilizing OpenStack's rescue functions involves booting the virtual machine using the original image, with the system disk provided as a secondary disk. This allows manipulation of the disk, such as using <code>fsck</code> to address filesystem issues or mounting and editing the configuration.</p> <p>Important Note</p> <p>We cannot rescue a volume-backed instance that means ONLY instance running using Ephemeral disk can be rescued. Also, this procedure has not been tested for Windows virtual machines.</p> <p>VMs can be rescued using either the OpenStack dashboard by clicking Action -&gt; Rescue Instance or via the openstack client using <code>openstack server rescue ...</code> command.</p> <p>If however, the virtual machine is no longer required and no data on the associated system or ephemeral disk needs to be preserved, the following command can be run:</p> <pre><code>openstack server rescue &lt;INSTANCE_NAME_OR_ID&gt;\n</code></pre> <p>or, using Horizon dashboard:</p> <p>Navigate to Project -&gt; Compute -&gt; Instances.</p> <p>Select an instance.</p> <p>Click Action -&gt; Rescue Instance.</p> <p>When to use Rescue Instance</p> <p>The rescue mode is only for emergency purpose, for example in case of a system or access failure. This will shut down your instance and mount the root disk to a temporary server. Then, you will be able to connect to this server, repair the system configuration or recover your data. You may optionally select an image and set a password on the rescue instance server.</p> <p></p>"},{"location":"openstack/management/vm-management/#troubleshoot-the-disk","title":"Troubleshoot the disk","text":"<p>This will reboot the virtual machine and you can then log in using the key pair previously defined. You will see two disks, <code>/dev/vda</code> which is the new system disk and <code>/dev/vdb</code> which is the old one to be repaired.</p> <pre><code>ubuntu@my-vm:~$ lsblk\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nloop0     7:0    0   62M  1 loop /snap/core20/1587\nloop1     7:1    0 79.9M  1 loop /snap/lxd/22923\nloop2     7:2    0   47M  1 loop /snap/snapd/16292\nvda     252:0    0  2.2G  0 disk\n\u251c\u2500vda1  252:1    0  2.1G  0 part /\n\u251c\u2500vda14 252:14   0    4M  0 part\n\u2514\u2500vda15 252:15   0  106M  0 part /boot/efi\nvdb     252:16   0   20G  0 disk\n\u251c\u2500vdb1  252:17   0 19.9G  0 part\n\u251c\u2500vdb14 252:30   0    4M  0 part\n\u2514\u2500vdb15 252:31   0  106M  0 part\n</code></pre> <p>The old one can be mounted and configuration files edited or <code>fsck</code>'d.</p> <pre><code># lsblk\n# cat /proc/diskstats\n# mkdir /tmp/rescue\n# mount /dev/vdb1 /tmp/rescue\n</code></pre>"},{"location":"openstack/management/vm-management/#unrescue-the-vm","title":"Unrescue the VM","text":"<p>On completion, the VM can be returned to active state with <code>openstack server unrescue ...</code> openstack client command, and rebooted.</p> <pre><code>openstack server unrescue &lt;INSTANCE_NAME_OR_ID&gt;\n</code></pre> <p>Then the secondary disk is removed as shown below:</p> <pre><code>ubuntu@my-vm:~$ lsblk\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nloop0     7:0    0   47M  1 loop /snap/snapd/16292\nvda     252:0    0   20G  0 disk\n\u251c\u2500vda1  252:1    0 19.9G  0 part /\n\u251c\u2500vda14 252:14   0    4M  0 part\n\u2514\u2500vda15 252:15   0  106M  0 part /boot/efi\n</code></pre> <p>Alternatively, using Horizon dashboard:</p> <p>Navigate to Project -&gt; Compute -&gt; Instances.</p> <p>Select an instance.</p> <p>Click Action -&gt; Unrescue Instance.</p> <p>And then Action -&gt; Soft Reboot Instance.</p>"},{"location":"openstack/management/vm-management/#delete-instance","title":"Delete Instance","text":"<p>VMs can be deleted using either the OpenStack dashboard by clicking Action -&gt; Delete Instance or via the openstack client <code>openstack server delete</code> command.</p> <p>How can I delete multiple instances at once?</p> <p>Using the Horizon dashboard, navigate to Project -&gt; Compute -&gt; Instances. In the Instances panel, you should see a list of all instances running in your project. Select the instances you want to delete by ticking the checkboxes next to their names. Then, click on \"Delete Instances\" button located on the top right side, as shown below:</p> <p></p> <p>Important Note</p> <p>This will immediately terminate the instance, delete all contents of the virtual machine and erase the disk. This operation is not recoverable.</p> <p>There are other options available if you wish to keep the virtual machine for future usage. These do, however, continue to use quota for the project even though the VM is not running.</p> <ul> <li>Snapshot the VM to keep an offline copy of the virtual machine that can be     performed as described here.</li> </ul> <p>If however, the virtual machine is no longer required and no data on the associated system or ephemeral disk needs to be preserved, the following command can be run:</p> <pre><code>openstack server delete &lt;INSTANCE_NAME_OR_ID&gt;\n</code></pre> <p>or, using Horizon dashboard:</p> <p>Navigate to Project -&gt; Compute -&gt; Instances.</p> <p>Select an instance.</p> <p>Click Action -&gt; Delete Instance.</p> <p>Important Note: Unmount volumes first</p> <p>Ensure to unmount any volumes attached to your instance before initiating the deletion process, as failure to do so may lead to data corruption in both your data and the associated volume.</p> <ul> <li> <p>If the instance is using Ephemeral disk:     It stops and removes the instance along with the ephemeral disk.     All data will be permanently lost!</p> </li> <li> <p>If the instance is using Volume-backed disk:     It stops and removes the instance. If \"Delete Volume on Instance Delete\"     was explicitely set to Yes, All data will be permanently lost!. If set     to No (which is default selected while launching an instance), the volume     may be used to boot a new instance, though any data stored in memory will be     permanently lost. For more in-depth information on making your VM setup and     data persistent, you can explore the details here.</p> </li> <li> <p>Status will briefly change to Deleting while the instance is being removed.</p> </li> </ul> <p>The quota associated with this virtual machine will be returned to the project and you can review and verify that looking at your OpenStack dashboard overview.</p> <ul> <li>Navigate to Project -&gt; Compute -&gt; Overview.</li> </ul>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/","title":"Launch a VM using OpenStack CLI","text":""},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#launch-a-vm-using-openstack-cli","title":"Launch a VM using OpenStack CLI","text":"<p>First find the following details using openstack command, we would required these details during the creation of virtual machine.</p> <ul> <li> <p>Flavor</p> </li> <li> <p>Image</p> </li> <li> <p>Network</p> </li> <li> <p>Security Group</p> </li> <li> <p>Key Name</p> </li> </ul> <p>Get the flavor list using below openstack command:</p> <pre><code>openstack flavor list\n+--------------------------------------+-------------+--------+------+-----------+-------+-----------+\n| ID                                   | Name        |    RAM | Disk | Ephemeral | VCPUs | Is Public |\n+--------------------------------------+-------------+--------+------+-----------+-------+-----------+\n| 12ded228-1a7f-4d35-b994-7dd394a6ca90 |gpu-su-a100.2| 196608 |   20 |         0 |    24 | True      |\n| 15581358-3e81-4cf2-a5b8-c0fd2ad771b4 | mem-su.8    |  65536 |   20 |         0 |     8 | True      |\n| 17521416-0ecf-4d85-8d4c-ec6fd1bc5f9d | cpu-su.1    |   2048 |   20 |         0 |     1 | True      |\n| 2b1dbea2-736d-4b85-b466-4410bba35f1e | cpu-su.8    |  16384 |   20 |         0 |     8 | True      |\n| 2f33578f-c3df-4210-b369-84a998d77dac | mem-su.4    |  32768 |   20 |         0 |     4 | True      |\n| 4498bfdb-5342-4e51-aa20-9ee74e522d59 | mem-su.1    |   8192 |   20 |         0 |     1 | True      |\n| 7f2f5f4e-684b-4c24-bfc6-3fce9cf1f446 | mem-su.16   | 131072 |   20 |         0 |    16 | True      |\n| 8c05db2f-6696-446b-9319-c32341a09c41 | cpu-su.16   |  32768 |   20 |         0 |    16 | True      |\n| 9662b5b2-aeaa-4d56-9bd3-450deee668af | cpu-su.4    |   8192 |   20 |         0 |     4 | True      |\n| b3377fdd-fd0f-4c88-9b4b-3b5c8ada0732 |gpu-su-a100.1|  98304 |   20 |         0 |    12 | True      |\n| e9125ab0-c8df-4488-a252-029c636cbd0f | mem-su.2    |  16384 |   20 |         0 |     2 | True      |\n| ee6417bd-7cd4-4431-a6ce-d09f0fba3ba9 | cpu-su.2    |   4096 |   20 |         0 |     2 | True      |\n+--------------------------------------+------------+--------+------+-----------+-------+------------+\n</code></pre> <p>Get the image name and its ID,</p> <pre><code>openstack image list  | grep almalinux-9\n| 263f045e-86c6-4344-b2de-aa475dbfa910 | almalinux-9-x86_64  | active |\n</code></pre> <p>Get Private Virtual network details, which will be attached to the VM:</p> <pre><code>openstack network list\n+--------------------------------------+-----------------+--------------------------------------+\n| ID                                   | Name            | Subnets                              |\n+--------------------------------------+-----------------+--------------------------------------+\n| 43613b84-e1fb-44a4-b1ea-c530edc49018 | provider        | 1cbbb98d-3b57-4f6d-8053-46045904d910 |\n| 8a91900b-d43c-474d-b913-930283e0bf43 | default_network | e62ce2fd-b11c-44ce-b7cc-4ca943e75a23 |\n+--------------------------------------+-----------------+--------------------------------------+\n</code></pre> <p>Find the Security Group:</p> <pre><code>openstack security group list\n+--------------------------------------+----------------------------------+-----------------------+----------------------------------+------+\n| ID                                   | Name                             | Description            |Project                          | Tags |\n+--------------------------------------+----------------------------------+-----------------------+----------------------------------+------+\n| 8285530a-34e3-4d96-8e01-a7b309a91f9f | default                          | Default security group |8ae3ae25c3a84c689cd24c48785ca23a | []   |\n| bbb738d0-45fb-4a9a-8bc4-a3eafeb49ba7 | ssh_only                         |                        |8ae3ae25c3a84c689cd24c48785ca23a | []   |\n+--------------------------------------+----------------------------------+-----------------------+----------------------------------+------+\n</code></pre> <p>Find the Key pair, in my case you can choose your own,</p> <pre><code>openstack keypair list | grep -i cloud_key\n| cloud_key | d5:ab:dc:1f:e5:08:44:7f:a6:21:47:23:85:32:cc:04 | ssh  |\n</code></pre> <p>Note</p> <p>Above details will be different for you based on your project and env.</p>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#launch-an-instance-from-an-image","title":"Launch an instance from an Image","text":"<p>Now we have all the details, let's create a virtual machine using \"openstack server create\" command</p> <p>Syntax :</p> <pre><code>openstack server create --flavor {Flavor-Name-Or-Flavor-ID } \\\n    --image {Image-Name-Or-Image-ID} \\\n    --nic net-id={Network-ID} \\\n    --user-data USER-DATA-FILE \\\n    --security-group {Security_Group_ID} \\\n    --key-name {Keypair-Name} \\\n    --property KEY=VALUE \\\n    &lt;Instance_Name&gt;\n</code></pre> <p>Important Note</p> <p>If you boot an instance with an \"Instance_Name\" greater than 63 characters, Compute truncates it automatically when turning it into a hostname to ensure the correct functionality of <code>dnsmasq</code>.</p> <p>Optionally, you can provide a key name for access control and a security group for security.</p> <p>You can also include metadata key and value pairs: <code>--key-name {Keypair-Name}</code>. For example, you can add a description for your server by providing the <code>--property description=\"My Server\"</code> parameter.</p> <p>You can pass user data in a local file at instance launch by using the <code>--user-data USER-DATA-FILE</code> parameter. If you do not provide a key pair, you will be unable to access the instance.</p> <p>You can also place arbitrary local files into the instance file system at creation time by using the <code>--file &lt;dest-filename=source-filename&gt;</code> parameter. You can store up to five files. For example, if you have a special authorized keys file named special_authorized_keysfile that you want to put on the instance rather than using the regular SSH key injection, you can add the \u2013file option as shown in the following example.</p> <pre><code>--file /root/.ssh/authorized_keys=special_authorized_keysfile\n</code></pre> <p>To create a VM in Specific \"Availability Zone and compute Host\" specify <code>--availability-zone {Availbility-Zone-Name}:{Compute-Host}</code> in above syntax.</p> <p>Example:</p> <pre><code>openstack server create --flavor cpu-su.2 \\\n    --image almalinux-8-x86_64 \\\n    --nic net-id=8ee63932-464b-4999-af7e-949190d8fe93 \\\n    --security-group default \\\n    --key-name cloud_key \\\n    --property description=\"My Server\" \\\n    my-vm\n</code></pre> <p>NOTE: To get more help on \"openstack server create\" command , use:</p> <pre><code>openstack -h server create\n</code></pre> <p>Detailed syntax:</p> <pre><code>openstack server create\n  (--image &lt;image&gt; | --volume &lt;volume&gt;)\n  --flavor &lt;flavor&gt;\n  [--security-group &lt;security-group&gt;]\n  [--key-name &lt;key-name&gt;]\n  [--property &lt;key=value&gt;]\n  [--file &lt;dest-filename=source-filename&gt;]\n  [--user-data &lt;user-data&gt;]\n  [--availability-zone &lt;zone-name&gt;]\n  [--block-device-mapping &lt;dev-name=mapping&gt;]\n  [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;]\n  [--network &lt;network&gt;]\n  [--port &lt;port&gt;]\n  [--hint &lt;key=value&gt;]\n  [--config-drive &lt;config-drive-volume&gt;|True]\n  [--min &lt;count&gt;]\n  [--max &lt;count&gt;]\n  [--wait]\n  &lt;server-name&gt;\n</code></pre> <p>Note</p> <p>Similarly, we can lauch a VM using bootable \"Volume\" as described here.</p> <p>Now verify the test vm \"my-vm\" is \"Running\" using the following commands:</p> <pre><code>openstack server list | grep my-vm\n</code></pre> <p>OR,</p> <pre><code>openstack server show my-vm\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#check-console-of-virtual-machine","title":"Check console of virtual machine","text":"<p>The console for a Linux VM can be displayed using console log.</p> <pre><code>openstack console log show --line 20 my-vm\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#associating-a-floating-ip-to-vm","title":"Associating a Floating IP to VM","text":"<p>To Associate a Floating IP to VM, first get the unused Floating IP using the following command:</p> <pre><code>openstack floating ip list | grep None | head -2\n| 071f08ac-cd10-4b89-aee4-856ead8e3ead | 169.144.107.154 | None |\nNone                                 |\n| 1baf4232-9cb7-4a44-8684-c604fa50ff60 | 169.144.107.184 | None |\nNone                                 |\n</code></pre> <p>Now Associate the first IP to the server using the following command:</p> <pre><code>openstack server add floating ip my-vm 169.144.107.154\n</code></pre> <p>Use the following command to verify whether Floating IP is assigned to the VM or not:</p> <pre><code>openstack server list | grep my-vm\n| 056c0937-6222-4f49-8405-235b20d173dd | my-vm | ACTIVE  | ...\nnternal=192.168.15.62, 169.144.107.154 |\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-floating-ip-from-the-vm","title":"Remove existing floating ip from the VM","text":"<pre><code>openstack server remove floating ip &lt;INSTANCE_NAME_OR_ID&gt; &lt;FLOATING_IP_ADDRESS&gt;\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#get-all-available-security-group-in-your-project","title":"Get all available security group in your project","text":"<pre><code>openstack security group list\n+--------------------------------------+----------+-----------------------+----------------------------------+------+\n| 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default  | Default security group |\nf01df1439b3141f8b76e68a3b58ef74a | []   |\n| 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access.     |\nf01df1439b3141f8b76e68a3b58ef74a | []   |\n+--------------------------------------+----------+-----------------------+----------------------------------+------+\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#add-existing-security-group-to-the-vm","title":"Add existing security group to the VM","text":"<pre><code>openstack server add security group &lt;INSTANCE_NAME_OR_ID&gt; &lt;SECURITY_GROUP&gt;\n</code></pre> <p>Example:</p> <pre><code>openstack server add security group my-vm ssh_only\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-security-group-from-the-vm","title":"Remove existing security group from the VM","text":"<pre><code>openstack server remove security group &lt;INSTANCE_NAME_OR_ID&gt; &lt;SECURITY_GROUP&gt;\n</code></pre> <p>Example:</p> <pre><code>openstack server remove security group my-vm ssh_only\n</code></pre> <p>Alternatively, you can use the openstack port unset command to remove the group from a port:</p> <pre><code>openstack port unset --security-group &lt;SECURITY_GROUP&gt; &lt;PORT&gt;\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#adding-volume-to-the-vm","title":"Adding volume to the VM","text":"<pre><code>openstack server add volume\n  [--device &lt;device&gt;]\n  &lt;INSTANCE_NAME_OR_ID&gt;\n  &lt;VOLUME_NAME_OR_ID&gt;\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-volume-from-the-vm","title":"Remove existing volume from the VM","text":"<pre><code>openstack server remove volume &lt;INSTANCE_NAME_OR_ID&gt; &lt;volume&gt;\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#reboot-a-virtual-machine","title":"Reboot a virtual machine","text":"<pre><code>openstack server reboot my-vm\n</code></pre>"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#deleting-virtual-machine-from-command-line","title":"Deleting Virtual Machine from Command Line","text":"<pre><code>openstack server delete my-vm\n</code></pre>"},{"location":"openstack/openstack-cli/openstack-CLI/","title":"OpenStack CLI","text":""},{"location":"openstack/openstack-cli/openstack-CLI/#openstack-cli","title":"OpenStack CLI","text":""},{"location":"openstack/openstack-cli/openstack-CLI/#references","title":"References","text":"<p>OpenStack Command Line Client(CLI) Cheat Sheet</p> <p>The OpenStack CLI is designed for interactive use. OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure. OpenStackClient is primarily configured using command line options and environment variables. Most of those settings can also be placed into a configuration file to simplify managing multiple cloud configurations. Most global options have a corresponding environment variable that may also be used to set the value. If both are present, the command-line option takes priority.</p> <p>It's also possible to call it from a bash script or similar, but typically it is too slow for heavy scripting use.</p>"},{"location":"openstack/openstack-cli/openstack-CLI/#command-line-setup","title":"Command Line setup","text":"<p>To use the CLI, you must create an application credentials and set the appropriate environment variables.</p> <p>You can download the environment file with the credentials from the OpenStack dashboard.</p> <ul> <li> <p>Log in to the NERC's OpenStack dashboard, choose     the project for which you want to download the OpenStack RC file.</p> </li> <li> <p>Navigate to Identity -&gt; Application Credentials.</p> </li> <li> <p>Click on \"Create Application Credential\" button and provide a Name and Roles     for the application credential. All other fields are optional and leaving the     \"Secret\" field empty will set it to autogenerate (recommended).</p> </li> </ul> <p></p> <p>Important Note</p> <p>Please note that an application credential is only valid for a single project to access multiple projects you need to create an application credential for each. You can switch projects by clicking on the project name at the top right corner and choosing from the dropdown under \"Project\".</p> <p>After clicking \"Create Application Credential\" button, the ID and Secret will be displayed and you will be prompted to <code>Download openrc file</code> or to <code>Download clouds.yaml</code>. Both of these are different methods of configuring the client for CLI access. Please save the file.</p>"},{"location":"openstack/openstack-cli/openstack-CLI/#configuration","title":"Configuration","text":"<p>The CLI is configured via environment variables and command-line options as listed in Authentication.</p>"},{"location":"openstack/openstack-cli/openstack-CLI/#configuration-files","title":"Configuration Files","text":""},{"location":"openstack/openstack-cli/openstack-CLI/#openstack-rc-file","title":"OpenStack RC File","text":"<p>Find the file (by default it will be named the same as the application credential name with the suffix <code>-openrc.sh</code> where project is the name of your OpenStack project).</p> <p>Source your downloaded OpenStack RC File:</p> <pre><code>source app-cred-&lt;Credential_Name&gt;-openrc.sh\n</code></pre> <p>Important Note</p> <p>When you source the file, environment variables are set for your current shell. The variables enable the openstack client commands to communicate with the OpenStack services that run in the cloud. This just stores your entry into the environment variable - there's no validation at this stage. You can inspect the downloaded file to retrieve the ID and Secret if necessary and see what other environment variables are set.</p>"},{"location":"openstack/openstack-cli/openstack-CLI/#cloudsyaml","title":"clouds.yaml","text":"<p><code>clouds.yaml</code> is a configuration file that contains everything needed to connect to one or more clouds. It may contain private information and is generally considered private to a user.</p> <p>For more information on configuring the OpenStackClient with <code>clouds.yaml</code> please see the OpenStack documentation.</p>"},{"location":"openstack/openstack-cli/openstack-CLI/#install-the-openstack-command-line-clients","title":"Install the OpenStack command-line clients","text":"<p>For more information on configuring the OpenStackClient please see the OpenStack documentation.</p> <p>Tip</p> <p>Install the OpenStack CLI by following the instructions specific to your operating system:</p> <p>For Ubuntu (Debian-based):</p> <pre><code>sudo apt update\nsudo apt install python3-openstackclient\n</code></pre> <p>For CentOS (Red Hat-based):</p> <pre><code>sudo yum install python3-openstackclient\n</code></pre> <p>Alternatively, you can install it via pip:</p> <pre><code>pip install python-openstackclient\n</code></pre>"},{"location":"openstack/openstack-cli/openstack-CLI/#openstack-hello-world","title":"OpenStack Hello World","text":"<p>Generally, the OpenStack terminal client offers the following methods:</p> <ul> <li> <p>list: Lists information about objects currently in the cloud.</p> </li> <li> <p>show: Displays information about a single object currently in the cloud.</p> </li> <li> <p>create: Creates a new object in the cloud.</p> </li> <li> <p>set: Edits an existing object in the cloud.</p> </li> </ul> <p>To test that you have everything configured, try out some commands. The following command lists all the images available to your project:</p> <pre><code>openstack image list\n+--------------------------------------+---------------------+--------+\n| ID                                   | Name                | Status |\n+--------------------------------------+---------------------+--------+\n| a9b48e65-0cf9-413a-8215-81439cd63966 | MS-Windows-2022     | active |\n| cfecb5d4-599c-4ffd-9baf-9cbe35424f97 | almalinux-8-x86_64  | active |\n| 263f045e-86c6-4344-b2de-aa475dbfa910 | almalinux-9-x86_64  | active |\n| 41fa5991-89d5-45ae-8268-b22224c772b2 | debian-10-x86_64    | active |\n| 99194159-fcd1-4281-b3e1-15956c275692 | fedora-36-x86_64    | active |\n| 74a33f77-fc42-4dd1-a5a2-55fb18fc50cc | rocky-8-x86_64      | active |\n| d7d41e5f-58f4-4ba6-9280-7fef9ac49060 | rocky-9-x86_64      | active |\n| 75a40234-702b-4ab7-9d83-f436b05827c9 | ubuntu-18.04-x86_64 | active |\n| 8c87cf6f-32f9-4a4b-91a5-0d734b7c9770 | ubuntu-20.04-x86_64 | active |\n| da314c41-19bf-486a-b8da-39ca51fd17de | ubuntu-22.04-x86_64 | active |\n+--------------------------------------+---------------------+--------+\n</code></pre> <p>If you have launched some instances already, the following command shows a list of your project's instances:</p> <pre><code>openstack server list --fit-width\n+--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+\n| ID                                   | Name             | Status | Networks                                     | Image                    |  Flavor      |\n+--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+\n| 1c96ba49-a20f-4c88-bbcf-93e2364365f5 |    vm-test       | ACTIVE | default_network=192.168.0.146, 199.94.60.4   | N/A (booted from volume) |  cpu-su.4     |\n| dd0d8053-ab88-4d4f-b5bc-97e7e2fe035a |    gpu-test      | ACTIVE | default_network=192.168.0.146, 199.94.60.4   | N/A (booted from volume) |  gpu-su-a100.1  |\n+--------------------------------------+------------------+--------+----------------------------------------------+--------------------------+--------------+\n</code></pre> <p>How to fit the CLI output to your terminal?</p> <p>You can use <code>--fit-width</code> at the end of the command to fit the output to your terminal.</p> <p>If you don't have any instances, you will get the error <code>list index out of range</code>, which is why we didn't suggest this command for your first test:</p> <pre><code>openstack server list\nlist index out of range\n</code></pre> <p>If you see this error:</p> <pre><code>openstack server list\nThe request you have made requires authentication. (HTTP 401) (Request-ID: req-6a827bf3-d5e8-47f2-984c-b6edeeb2f7fb)\n</code></pre> <p>Then your environment variables are likely not configured correctly.</p> <p>The most common reason is that you made a typo when entering your password. Try sourcing the OpenStack RC file again and retyping it.</p> <p>You can type <code>openstack -h</code> to see a list of available commands.</p> <p>Note</p> <p>This includes some admin-only commands.</p> <p>If you try one of these by mistake, you might see this output:</p> <pre><code>openstack user list\nYou are not authorized to perform the requested action: identity:list_users.\n(HTTP 403) (Request-ID: req-cafe1e5c-8a71-44ab-bd21-0e0f25414062)\n</code></pre> <p>Depending on your needs for API interaction, this might be sufficient.</p> <p>If you just occasionally want to run 1 or 2 of these commands from your terminal, you can do it manually or write a quick bash script that makes use of this CLI.</p> <p>However, this isn't a very optimized way to do complex interactions with OpenStack. For that, you want to write scripts that interact with the python SDK bindings directly.</p> <p>Pro Tip</p> <p>If you find yourself fiddling extensively with awk and grep to extract things like project IDs from the CLI output, it's time to move on to using the client libraries or the RESTful API directly in your scripts.</p>"},{"location":"openstack/persistent-storage/attach-the-volume-to-an-instance/","title":"Attach The Volume To An Instance","text":""},{"location":"openstack/persistent-storage/attach-the-volume-to-an-instance/#attach-the-volume-to-an-instance","title":"Attach The Volume To An Instance","text":""},{"location":"openstack/persistent-storage/attach-the-volume-to-an-instance/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p>In the Actions column, click the dropdown and select \"Manage Attachments\".</p> <p></p> <p>From the menu, choose the instance you want to connect the volume to from Attach to Instance, and click \"Attach Volume\".</p> <p></p> <p>The volume now has a status of \"In-use\" and \"Attached To\" column shows which instance it is attached to, and what device name it has.</p> <p>This will be something like <code>/dev/vdb</code> but it can vary depending on the state of your instance, and whether you have attached volumes before.</p> <p>Make note of the device name of your volume.</p> <p></p>"},{"location":"openstack/persistent-storage/attach-the-volume-to-an-instance/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul> <p>To attach the volume to an instance using the CLI, do this:</p>"},{"location":"openstack/persistent-storage/attach-the-volume-to-an-instance/#using-the-openstack-client","title":"Using the openstack client","text":"<p>When the status is 'available', the volume can be attached to a virtual machine using the following openstack client command syntax:</p> <pre><code>openstack server add volume &lt;INSTANCE_NAME_OR_ID&gt; &lt;VOLUME_NAME_OR_ID&gt;\n</code></pre> <p>For example:</p> <pre><code>openstack server add volume test-vm my-volume\n+-----------------------+--------------------------------------+\n| Field                 | Value                                |\n+-----------------------+--------------------------------------+\n| ID                    | 5b5380bd-a15b-408b-8352-9d4219cf30f3 |\n| Server ID             | 8a876a17-3407-484c-85c4-8a46fbac1607 |\n| Volume ID             | 5b5380bd-a15b-408b-8352-9d4219cf30f3 |\n| Device                | /dev/vdb                             |\n| Tag                   | None                                 |\n| Delete On Termination | False                                |\n+-----------------------+--------------------------------------+\n</code></pre> <p>where \"test-vm\" is the virtual machine and the second parameter, \"my-volume\" is the volume created before.</p> <p>Pro Tip</p> <p>If your instance name <code>&lt;INSTANCE_NAME_OR_ID&gt;</code> and volume name <code>&lt;VOLUME_NAME_OR_ID&gt;</code> include spaces, you need to enclose them in quotes, i.e. <code>\"&lt;INSTANCE_NAME_OR_ID&gt;\"</code> and <code>\"&lt;VOLUME_NAME_OR_ID&gt;\"</code>.</p> <p>For example: <code>openstack server remove volume \"My Test Instance\" \"My Volume\"</code>.</p>"},{"location":"openstack/persistent-storage/attach-the-volume-to-an-instance/#to-verify-the-volume-is-attached-to-the-vm","title":"To verify the volume is attached to the VM","text":"<pre><code>openstack volume list\n+--------------------------------------+-----------------+--------+------+----------------------------------+\n| ID                                   | Name            | Status | Size | Attached to                      |\n+--------------------------------------+-----------------+--------+------+----------------------------------+\n| 563048c5-d27b-4397-bb4e-034e0f4d9fa7 |                 | in-use |   20 | Attached to test-vm on /dev/vda  |\n| 5b5380bd-a15b-408b-8352-9d4219cf30f3 | my-volume       | in-use |   20 | Attached to test-vm on /dev/vdb  |\n+--------------------------------------+-----------------+--------+------+----------------------------------+\n</code></pre> <p>The volume now has a status of \"in-use\" and \"Attached To\" column shows which instance it is attached to, and what device name it has.</p> <p>This will be something like <code>/dev/vdb</code> but it can vary depending on the state of your instance, and whether you have attached volumes before.</p>"},{"location":"openstack/persistent-storage/create-an-empty-volume/","title":"Create An Empty Volume","text":""},{"location":"openstack/persistent-storage/create-an-empty-volume/#create-an-empty-volume","title":"Create An Empty Volume","text":"<p>An empty volume is like an unformatted USB stick. We'll attach it to an instance, create a filesystem on it, and mount it to the instance.</p>"},{"location":"openstack/persistent-storage/create-an-empty-volume/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard, you can create a volume via the \"Volumes -&gt; Volumes\" page by clicking on the \"Create Volume\" button.</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p></p> <p>Click \"Create Volume\".</p> <p>In the Create Volume dialog box, give your volume a name. The description field is optional.</p> <p></p> <p>Choose \"empty volume\" from the Source dropdown. This will create a volume that is like an unformatted hard disk. Choose a size (In GiB) for your volume. Leave Type and Availibility Zone as it as. Only admin to the NERC OpenStack will be able to manage volume types.</p> <p>Click \"Create Volume\" button.</p> <p>Checking the status of created volume will show:</p> <p>\"downloading\" means that the volume contents is being transferred from the image service to the volume service</p> <p>In a few moments, the newly created volume will appear in the Volumes list with the Status \"available\". \"available\" means the volume can now be used for booting. A set of volume_image meta data is also copied from the image service.</p> <p></p>"},{"location":"openstack/persistent-storage/create-an-empty-volume/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul> <p>To create a volume using the CLI, do this:</p>"},{"location":"openstack/persistent-storage/create-an-empty-volume/#using-the-openstack-client","title":"Using the openstack client","text":"<p>This allows an arbitrary sized disk to be attached to your virtual machine, like plugging in a USB stick. The steps below create a disk of 20 gibibytes (GiB) with name \"my-volume\".</p> <pre><code>openstack volume create --size 20 my-volume\n\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | false                                |\n| consistencygroup_id | None                                 |\n| created_at          | 2024-02-03T17:06:05.000000           |\n| description         | None                                 |\n| encrypted           | False                                |\n| id                  | 5b5380bd-a15b-408b-8352-9d4219cf30f3 |\n| multiattach         | False                                |\n| name                | my-volume                            |\n| properties          |                                      |\n| replication_status  | None                                 |\n| size                | 20                                   |\n| snapshot_id         | None                                 |\n| source_volid        | None                                 |\n| status              | creating                             |\n| type                | tripleo                              |\n| updated_at          | None                                 |\n| user_id             | 938eb8bfc72e4ca3ad2b94e2eb4059f7     |\n+---------------------+--------------------------------------+\n</code></pre>"},{"location":"openstack/persistent-storage/create-an-empty-volume/#to-view-newly-created-volume","title":"To view newly created volume","text":"<pre><code>openstack volume list\n+--------------------------------------+-----------------+-----------+------+----------------------------------+\n| ID                                   | Name            | Status    | Size | Attached to                      |\n+--------------------------------------+-----------------+-----------+------+----------------------------------+\n| 563048c5-d27b-4397-bb4e-034e0f4d9fa7 |                 | in-use    |   20 | Attached to test-vm on /dev/vda  |\n| 5b5380bd-a15b-408b-8352-9d4219cf30f3 | my-volume       | available |   20 |                                  |\n+--------------------------------------+-----------------+-----------+------+----------------------------------+\n</code></pre>"},{"location":"openstack/persistent-storage/delete-volumes/","title":"Delete Volumes","text":""},{"location":"openstack/persistent-storage/delete-volumes/#delete-volumes","title":"Delete Volumes","text":""},{"location":"openstack/persistent-storage/delete-volumes/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p>Select the volume or volumes that you want to delete.</p> <p>Click \"Delete Volumes\" button.</p> <p>In the Confirm Delete Volumes window, click the Delete Volumes button to confirm the action.</p> <p>Unable to Delete Volume</p> <p>You cannot delete a bootable volume that is actively in use by a running VM. If you really want to delete such volume then first delete the insance and then you are allowed to delete the detached volume. Before deleting Please make sure during the launch of this insance is using the default selected No for \"Delete Volume on Instance Delete\" configuration option. If you had set this configuration \"Yes\" for \"Delete Volume on Instance Delete\", then deleting the instance will automatically remove the associated volume. </p>"},{"location":"openstack/persistent-storage/delete-volumes/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul> <p>To delete a volume using the CLI, do this:</p>"},{"location":"openstack/persistent-storage/delete-volumes/#using-the-openstack-client","title":"Using the openstack client","text":"<p>The following openstack client command syntax can be used to delete a volume:</p> <pre><code>openstack volume delete &lt;VOLUME_NAME_OR_ID&gt;\n</code></pre> <p>For example:</p> <pre><code>openstack volume delete my-volume\n</code></pre> <p>Pro Tip</p> <p>If your volume name <code>&lt;VOLUME_NAME_OR_ID&gt;</code> include spaces, you need to enclose them in quotes, i.e. <code>\"&lt;VOLUME_NAME_OR_ID&gt;\"</code>.</p> <p>For example: <code>openstack volume delete \"My Volume\"</code>.</p> <p>Your volume will now go into state 'deleting' and completely disappear from the <code>openstack volume list</code> output.</p>"},{"location":"openstack/persistent-storage/detach-a-volume/","title":"Detach a Volume","text":""},{"location":"openstack/persistent-storage/detach-a-volume/#detach-a-volume-and-attach-it-to-an-instance","title":"Detach A Volume and Attach it to an instance","text":""},{"location":"openstack/persistent-storage/detach-a-volume/#detach-a-volume","title":"Detach A Volume","text":""},{"location":"openstack/persistent-storage/detach-a-volume/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p>To detach a mounted volume by going back to \"Manage Attachments\" and choosing Detach Volume.</p> <p>This will popup the following interface to proceed:</p> <p></p> <p>Unable to Detach Volume</p> <p>If your bootable volume that is attached to a VM then that volume cannot be detached as this is a root device volume. This bootable volume is created when you launch an instance from an Image or an Instance Snapshot, and the choice for utilizing persistent storage is configured by selecting the Yes option for \"Create New Volume\". If you explicitly chosen as \"No\" for this option then there will be no attach volume created for the instance instead an Ephemeral disk storage is used.</p> <p></p>"},{"location":"openstack/persistent-storage/detach-a-volume/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/detach-a-volume/#using-the-openstack-client","title":"Using the openstack client","text":"<p>The following openstack client command syntax can be used to detach a volume from a VM:</p> <pre><code>openstack server remove volume &lt;INSTANCE_NAME_OR_ID&gt; &lt;VOLUME_NAME_OR_ID&gt;.\n</code></pre> <p>For example:</p> <pre><code>openstack server remove volume test-vm my-volume.\n</code></pre> <p>where \"test-vm\" is the virtual machine and the second parameter, \"my-volume\" is the volume created before and attached to the VM and can be shown in <code>openstack volume list</code>.</p> <p>Pro Tip</p> <p>If your instance name <code>&lt;INSTANCE_NAME_OR_ID&gt;</code> and volume name <code>&lt;VOLUME_NAME_OR_ID&gt;</code> include spaces, you need to enclose them in quotes, i.e. <code>\"&lt;INSTANCE_NAME_OR_ID&gt;\"</code> and <code>\"&lt;VOLUME_NAME_OR_ID&gt;\"</code>.</p> <p>For example: <code>openstack server remove volume \"My Test Instance\" \"My Volume\"</code>.</p> <p>Check that the volume is in state 'available' again.</p> <p>If that's the case, the volume is now ready to either be attached to another virtual machine or, if it is not needed any longer, to be completely deleted (please note that this step cannot be reverted!).</p>"},{"location":"openstack/persistent-storage/detach-a-volume/#attach-the-detached-volume-to-an-instance","title":"Attach the detached volume to an instance","text":"<p>Once it is successfully detached, you can use \"Manage Attachments\" to attach it to another instance if desired as explained here.</p> <p>OR,</p> <p>You can attach the existing volume (Detached!) to the new instance as shown below:</p> <p></p> <p>After this run the following commands as <code>root</code> user to mount it:</p> <pre><code>mkdir /mnt/test_volume\nmount /dev/vdb /mnt/test_volume\n</code></pre> <p>All the previous data from previous instance will be available under the mounted folder at <code>/mnt/test_volume</code>.</p> <p>Very Important Note</p> <p>Also, a given volume might not get the same device name the second time you attach it to an instance.</p>"},{"location":"openstack/persistent-storage/extending-volume/","title":"Extending Volume","text":""},{"location":"openstack/persistent-storage/extending-volume/#extending-volume","title":"Extending Volume","text":"<p>A volume can be made larger while maintaining the existing contents, assuming the file system supports resizing. We can extend a volume that is not attached to any VM and in \"Available\" status.</p> <p>The steps are as follows:</p> <ul> <li> <p>Extend the volume to its new size</p> </li> <li> <p>Extend the filesystem to its new size</p> </li> </ul>"},{"location":"openstack/persistent-storage/extending-volume/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p></p> <p>Specify, the new extened size in GiB:</p> <p></p>"},{"location":"openstack/persistent-storage/extending-volume/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/extending-volume/#using-the-openstack-client","title":"Using the openstack client","text":"<p>The following openstack client command syntax can be used to extend any existing volume from its previous size to a new size of :</p> <pre><code>openstack volume set --size &lt;NEW_SIZE_IN_GiB&gt; &lt;VOLUME_NAME_OR_ID&gt;\n</code></pre> <p>For example:</p> <pre><code>openstack volume set --size 100 my-volume\n</code></pre> <p>where \"my-volume\" is the existing volume with a size of 80 GiB and is going to be extended to a new size of 100 GiB.\"</p> <p>Pro Tip</p> <p>If your volume name <code>&lt;VOLUME_NAME_OR_ID&gt;</code> includes spaces, you need to enclose them in quotes, i.e. <code>\"&lt;VOLUME_NAME_OR_ID&gt;\"</code>.</p> <p>For example: <code>openstack volume set --size 100 \"My Volume\"</code>.</p> <p>For windows systems, please follow the provider documentation.</p> <p>Please note</p> <ul> <li> <p>Volumes can be made larger, but not smaller. There is no support for shrinking existing volumes.</p> </li> <li> <p>The procedure given above has been tested with ext4 and XFS filesystems only.</p> </li> </ul>"},{"location":"openstack/persistent-storage/format-and-mount-the-volume/","title":"Format And Mount The Volume","text":""},{"location":"openstack/persistent-storage/format-and-mount-the-volume/#format-and-mount-the-volume","title":"Format And Mount The Volume","text":"<p>Prerequisites:</p> <p>Before formatting and mounting the volume, you need to have already created a new volume as referred here and attached it to any running VM, as described here.</p>"},{"location":"openstack/persistent-storage/format-and-mount-the-volume/#for-linux-based-virtual-machine","title":"For Linux based virtual machine","text":"<p>To verify that the newly created volume, \"my-volume\", exists and is attached to a VM, \"test-vm\", run this openstack client command:</p> <pre><code>openstack volume list\n+--------------------------------------+-----------------+--------+------+----------------------------------+\n| ID                                   | Name            | Status | Size | Attached to                      |\n+--------------------------------------+-----------------+--------+------+----------------------------------+\n| 563048c5-d27b-4397-bb4e-034e0f4d9fa7 |                 | in-use |   20 | Attached to test-vm on /dev/vda  |\n| 5b5380bd-a15b-408b-8352-9d4219cf30f3 | my-volume       | in-use |   20 | Attached to test-vm on /dev/vdb  |\n+--------------------------------------+-----------------+--------+------+----------------------------------+\n</code></pre> <p>The volume has a status of \"in-use\" and \"Attached To\" column shows which instance it is attached to, and what device name it has.</p> <p>This will be something like <code>/dev/vdb</code> but it can vary depending on the state of your instance, and whether you have attached volumes before.</p> <p>Make note of the device name of your volume.</p> <p>SSH into your instance. You should now see the volume as an additional disk in the output of <code>sudo fdisk -l</code> or <code>lsblk</code> or <code>cat /proc/partitions</code>.</p> <pre><code># lsblk\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n...\nvda     254:0    0   10G  0 disk\n\u251c\u2500vda1  254:1    0  9.9G  0 part /\n\u251c\u2500vda14 254:14   0    4M  0 part\n\u2514\u2500vda15 254:15   0  106M  0 part /boot/efi\nvdb     254:16   0    1G  0 disk\n</code></pre> <p>Here, we see the volume as the disk <code>vdb</code>, which matches the <code>/dev/vdb/</code> we previously noted in the \"Attached To\" column.</p> <p>Create a filesystem on the volume and mount it. In this example, we will create an <code>ext4</code> filesystem:</p> <p>Run the following commands as <code>root</code> user:</p> <pre><code>mkfs.ext4 /dev/vdb\nmkdir /mnt/test_volume\nmount /dev/vdb /mnt/test_volume\ndf -H\n</code></pre> <p>The volume is now available at the mount point:</p> <pre><code>lsblk\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n...\nvda     254:0    0   10G  0 disk\n\u251c\u2500vda1  254:1    0  9.9G  0 part /\n\u251c\u2500vda14 254:14   0    4M  0 part\n\u2514\u2500vda15 254:15   0  106M  0 part /boot/efi\nvdb     254:16   0    1G  0 disk /mnt/test_volume\n</code></pre> <p>Very Important Information</p> <p>Please note that this mounted drive is not persistent if your VM is stopped or rebooted in the future. After each reboot, you will need to run the mounting command <code>mount /dev/vdb /mnt/test_volume</code> again. To enable automatic mounting at system startup, please follow the steps as outlined here.</p> <p>If you place data in the directory <code>/mnt/test_volume</code>, detach the volume, and mount it to another instance, the second instance will have access to the data.</p> <p>Important Note</p> <p>In this case it's easy to spot because there is only one additional disk attached to the instance, but it's important to keep track of the device name, especially if you have multiple volumes attached.</p>"},{"location":"openstack/persistent-storage/format-and-mount-the-volume/#mounting-on-system-startup","title":"Mounting on system startup","text":"<p>Mounts can be set to occur automatically during system initialization so that mounted file systems will persist even after the VM reboot.</p>"},{"location":"openstack/persistent-storage/format-and-mount-the-volume/#configure-mounting-of-the-disk-to-your-directory","title":"Configure mounting of the disk to your directory","text":"<p>To configure mounting of the <code>/dev/vdb</code> disk to your <code>/mnt/test_volume</code> directory:</p> <p>First, get the UUID of the device:</p> <pre><code>sudo blkid /dev/vdb\n</code></pre> <p>The output should look similar to:</p> <pre><code>/dev/vdb: UUID=\"1234-5678-ABCD-EF00\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\"\n</code></pre> <p>Note</p> <p>Please note the UUID.</p> <p>Now open the file <code>/etc/fstab</code> using your favorite command line text editor for editing. You will need sudo privileges for that. For example, if you want to use nano, execute this command:</p> <pre><code>sudo nano /etc/fstab\n</code></pre> <p>Add the following line to the <code>/etc/fstab</code> file at the end (replace the UUID with what you noted before). This will mount the disk automatically on every reboot.</p> <pre><code>UUID=1234-5678-ABCD-EF00 /mnt/test_volume ext4 defaults 0 2\n</code></pre> <p>Important Information</p> <p>If you just want to test your mounting command written in <code>/etc/fstab</code> without \"Rebooting\" the VM you can also do that by running <code>sudo mount -a</code>. If there's no error, your mount is working and will persist after reboot.</p> <p>Reboot your VM:</p> <pre><code>sudo reboot\n</code></pre> <p>Confirm the mount:</p> <pre><code>df -h | grep /mnt/test_volume\n</code></pre>"},{"location":"openstack/persistent-storage/format-and-mount-the-volume/#for-windows-virtual-machine","title":"For Windows virtual machine","text":"<p>Here, we create an empty volume following the steps outlined in this documentation.</p> <p>Please make sure you are creating volume of the size 100 GiB:</p> <p></p> <p>Then attach the newly created volume to a running Windows VM:</p> <p></p> <p>Login remote desktop using the Floating IP attached to the Windows VM:</p> <p></p> <p></p> <p>What is the user login for Windows Server 2022?</p> <p>The default username is \"Administrator,\" and the password is the one you set using the user data PowerShell script during the launch as described here.</p> <p></p> <p>Once connected search for \"Disk Management\" from Windows search box. This will show all attached disk as Unknown and Offline as shown here:</p> <p></p> <p>In Disk Management, select and hold (or right-click) the disk you want to initialize, and then select \"Initialize Disk\". If the disk is listed as Offline, first select and hold (or right-click) the disk, and then select \"Online\".</p> <p></p> <p></p> <p>In the Initialize Disk dialog box, make sure the correct disk is selected, and then choose OK to accept the default partition style. If you need to change the partition style (GPT or MBR), see Compare partition styles - GPT and MBR.</p> <p></p> <p>Format the New Volume:</p> <ul> <li> <p>Select and hold (or right-click) the unallocated space of the new disk.</p> </li> <li> <p>Select \"New Simple Volume\" and follow the wizard to create a new partition.</p> </li> </ul> <p></p> <ul> <li> <p>Choose the file system (usually NTFS for Windows).</p> </li> <li> <p>Assign a drive letter or mount point.</p> </li> </ul> <p>Complete Formatting:</p> <ul> <li> <p>Complete the wizard to format the new volume.</p> </li> <li> <p>Once formatting is complete, the new volume should be visible in File Explorer     as shown below:</p> </li> </ul> <p></p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/","title":"Mount The Object Storage","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-the-object-storage-to-an-instance","title":"Mount The Object Storage To An Instance","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#pre-requisite","title":"Pre-requisite","text":"<p>We are using following setting for this purpose to mount the object storage to an NERC OpenStack VM:</p> <ul> <li> <p>1 Linux machine, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu OS image,     <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage - also assign Floating IP     to this VM.</p> </li> <li> <p>The NERC's Swift Endpoint URL: <code>https://stack.nerc.mghpcc.org:13808</code>.</p> <p>Very Important: Endpoint for S3 API for the NERC Object Storage/ Swift</p> <p>The default endpoint for S3 API for the NERC Object Storage/ Swift is <code>https://stack.nerc.mghpcc.org:13808</code>.</p> </li> <li> <p>Setup and enable your S3 API credentials:</p> </li> </ul> <p>To access the API credentials, you must login through the OpenStack Dashboard and navigate to \"Projects &gt; API Access\" where you can download the \"Download OpenStack RC File\" as well as the \"EC2 Credentials\".</p> <p></p> <p>While clicking on \"EC2 Credentials\", this will download a file zip file including <code>ec2rc.sh</code> file that has content similar to shown below. The important parts are <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code>, keep them noted.</p> <pre><code>#!/bin/bash\n\nNOVARC=$(readlink -f \"${BASH_SOURCE:-${0}}\" 2&gt;/dev/null) || NOVARC=$(python -c 'import os,sys; print os.path.abspath(os.path.realpath(sys.argv[1]))' \"${BASH_SOURCE:-${0}}\")\nNOVA_KEY_DIR=${NOVARC%/*}\nexport EC2_ACCESS_KEY=...\nexport EC2_SECRET_KEY=...\nexport EC2_URL=https://localhost/notimplemented\nexport EC2_USER_ID=42 # nova does not use user id, but bundling requires it\nexport EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem\nexport EC2_CERT=${NOVA_KEY_DIR}/cert.pem\nexport NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem\nexport EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set\n\nalias ec2-bundle-image=\"ec2-bundle-image --cert ${EC2_CERT} --privatekey ${EC2_PRIVATE_KEY} --user 42 --ec2cert ${NOVA_CERT}\"\nalias ec2-upload-bundle=\"ec2-upload-bundle -a ${EC2_ACCESS_KEY} -s ${EC2_SECRET_KEY} --url ${S3_URL} --ec2cert ${NOVA_CERT}\"\n</code></pre> <p>Alternatively, you can obtain your EC2 access keys using the openstack client:</p> <pre><code>sudo apt install python3-openstackclient\n\nopenstack ec2 credentials list\n+------------------+------------------+--------------+-----------+\n| Access           | Secret           | Project ID   | User ID   |\n+------------------+------------------+--------------+-----------+\n| &lt;EC2_ACCESS_KEY&gt; | &lt;EC2_SECRET_KEY&gt; | &lt;Project_ID&gt; | &lt;User_ID&gt; |\n+------------------+------------------+--------------+-----------+\n</code></pre> <p>OR, you can even create a new one by running:</p> <pre><code>openstack ec2 credentials create\n</code></pre> <ul> <li> <p>Source the downloaded OpenStack RC File from Projects &gt; API Access by using:     <code>source *-openrc.sh</code> command. Sourcing the RC File will set the required environment     variables.</p> </li> <li> <p>Allow Other User option by editing fuse config by editing <code>/etc/fuse.conf</code> file     and uncomment \"user_allow_other\" option.</p> <pre><code>sudo nano /etc/fuse.conf\n</code></pre> </li> </ul> <p>The output going to look like this:</p> <p></p> <p>A comparative analysis of Mountpoint for S3, Goofys, and S3FS.</p> <p>When choosing between S3 clients that enable the utilization of an object store with applications expecting files, it's essential to consider the specific use case and whether the convenience and compatibility provided by FUSE clients match the project's requirements.</p> <p>To delve into a comparative analysis of Mountpoint for S3, Goofys, and S3FS, please read this blog post.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#1-using-mountpoint-for-amazon-s3","title":"1. Using Mountpoint for Amazon S3","text":"<p>Mountpoint for Amazon S3 is a high-throughput open-source file client designed to mount an Amazon S3 bucket as a local file system. Mountpoint is optimized for workloads that need high-throughput read and write access to data stored in S3 Object Storage through a file system interface.</p> <p>Very Important Note</p> <p>Mountpoint for Amazon S3 intentionally does not implement the full POSIX standard specification for file systems. Mountpoint supports file-based workloads that perform sequential and random reads, sequential (append only) writes, and that don't need full POSIX semantics.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#install-mountpoint","title":"Install Mountpoint","text":"<p>Access your virtual machine using SSH. Update the packages on your system and install <code>wget</code> to be able to download the <code>mount-s3</code> binary directly to your VM:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install wget\n</code></pre> <p>Now, navigate to your home directory:</p> <pre><code>cd\n</code></pre> <ol> <li> <p>Download the Mountpoint for Amazon S3 package using <code>wget</code> command:</p> <pre><code>wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.deb\n</code></pre> </li> <li> <p>Install the package by entering the following command:</p> <pre><code>sudo apt-get install ./mount-s3.deb\n</code></pre> </li> <li> <p>Verify that Mountpoint for Amazon S3 is successfully installed by entering the     following command:</p> <pre><code>mount-s3 --version\n</code></pre> <p>You should see output similar to the following:</p> <pre><code>mount-s3 1.6.0\n</code></pre> </li> </ol>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#configuring-and-using-mountpoint","title":"Configuring and using Mountpoint","text":"<p>Make a folder to store your credentials:</p> <pre><code>mkdir ~/.aws/\n</code></pre> <p>Create file <code>~/.aws/credentials</code> using your favorite text editor (for example <code>nano</code> or <code>vim</code>). Add the following contents to it which requires the <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file (during the \"Setup and enable your S3 API credentials\" step):</p> <pre><code>[nerc]\naws_access_key_id=&lt;EC2_ACCESS_KEY&gt;\naws_secret_access_key=&lt;EC2_SECRET_KEY&gt;\n</code></pre> <p>Save the file and exit the text editor.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-local-directory-as-a-mount-point","title":"Create a local directory as a mount point","text":"<pre><code>mkdir -p ~/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-the-container-locally-using-mountpoint","title":"Mount the Container locally using Mountpoint","text":"<p>The object storage container i.e. \"bucket1\" will be mounted in the directory <code>~/bucket1</code></p> <pre><code>mount-s3 --profile \"nerc\" --endpoint-url \"https://stack.nerc.mghpcc.org:13808\" --allow-other --force-path-style --debug bucket1 ~/bucket1/\n</code></pre> <p>In this command,</p> <ul> <li> <p><code>mount-s3</code> is the Mountpoint for Amazon S3 package as installed in <code>/usr/bin/</code>     path we don't need to specify the full path.</p> </li> <li> <p><code>--profile</code> corresponds to the name given on the <code>~/.aws/credentials</code> file i.e.     <code>[nerc]</code>.</p> </li> <li> <p><code>--endpoint-url</code> corresponds to the Object Storage endpoint url for NERC Object     Storage. You don't need to modify this url.</p> </li> <li> <p><code>--allow-other</code>: Allows other users to access the mounted filesystem. This is     particularly useful when multiple users need to access the mounted S3 bucket.     Only allowed if <code>user_allow_other</code> is set in <code>/etc/fuse.conf</code>.</p> </li> <li> <p><code>--force-path-style</code>: Forces the use of path-style URLs when accessing the S3     bucket. This is necessary when working with certain S3-compatible storage services     that do not support virtual-hosted-style URLs.</p> </li> <li> <p><code>--debug</code>: Enables debug mode, providing additional information about the mounting     process.</p> </li> <li> <p><code>bucket1</code> is the name of the container which contains the NERC Object Storage     resources.</p> </li> <li> <p><code>~/bucket1</code> is the location of the folder in which you want to mount the Object     Storage filesystem.</p> </li> </ul> <p>Important Note</p> <p>Mountpoint automatically configures reasonable defaults for file system settings such as permissions and performance. However, if you require finer control over how the Mountpoint file system behaves, you can adjust these settings accordingly. For further details, please refer to this resource.</p> <p>In order to test whether the mount was successful, navigate to the directory in which you mounted the NERC container repository, for example:</p> <pre><code>cd ~/bucket1\n</code></pre> <p>Use the <code>ls</code> command to list its content. You should see the output similar to this:</p> <pre><code>ls\n\nREADME.md   image.png   test-file\n</code></pre> <p>The NERC Object Storage container repository has now been mounted using Mountpoint.</p> <p>Very Important Information</p> <p>Please note that any of these Mountpoints is not persistent if your VM is stopped or rebooted in the future. After each reboot, you will need to execute the mounting command as mentioned above again. To enable automatic mounting at system startup, please follow the steps as outlined here.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#automatically-mounting-an-s3-bucket-at-boot","title":"Automatically mounting an S3 bucket at boot","text":"<p>Mountpoint does not currently support automatically mounting a bucket at system boot time by configuring them in the <code>/etc/fstab</code>. If you would like your bucket/s to automatically mount when the machine is started you will need to either set up a Cron Job in <code>crontab</code> or using a service manager like <code>systemd</code>.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#using-a-cron-job","title":"Using a Cron Job","text":"<p>You need to create a Cron job so that the script runs each time your VM reboots, remounting S3 Object Storage to your VM.</p> <pre><code>crontab -e\n</code></pre> <p>Add this command to the end of the file</p> <pre><code>@reboot sh /&lt;Path_To_Directory&gt;/script.sh\n</code></pre> <p>For example,</p> <pre><code>@reboot sh /home/ubuntu/script.sh\n</code></pre> <p>Create <code>script.sh</code> file paste the below code to it.</p> <pre><code>#!/bin/bash\nmount-s3 [OPTIONS] &lt;BUCKET_NAME&gt; &lt;DIRECTORY&gt;\n</code></pre> <p>For example,</p> <pre><code>#!/bin/bash\nmount-s3 --profile \"nerc\" --endpoint-url \"https://stack.nerc.mghpcc.org:13808\" --allow-other --force-path-style --debug bucket1 ~/bucket1/\n</code></pre> <p>Make the file executable by running the below command</p> <pre><code>chmod +x script.sh\n</code></pre> <p>Reboot your VM:</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#using-a-service-manager-like-systemd-by-creating-systemd-unit-file","title":"Using a service manager like <code>systemd</code> by creating systemd unit file","text":"<p>Create directory in <code>/root</code> folder in which you will store the credentials:</p> <pre><code>sudo mkdir /root/.aws\n</code></pre> <p>Copy the credentials you created in your local directory to the <code>.aws</code> directory in the <code>/root</code> folder:</p> <pre><code>sudo cp ~/.aws/credentials /root/.aws/\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-systemd-unit-file-ie-mountpoint-s3service","title":"Create systemd unit file i.e. <code>mountpoint-s3.service</code>","text":"<p>Create a systemd service unit file that is going to execute the above script and dynamically mount or unmount the container:</p> <pre><code>sudo nano /etc/systemd/system/mountpoint-s3.service\n</code></pre> <p>Edit the file to look like the below:</p> <pre><code>[Unit]\nDescription=Mountpoint for Amazon S3 mount\nDocumentation=https://docs.aws.amazon.com/AmazonS3/latest/userguide/mountpoint.html\n#Wants=network.target\nWants=network-online.target\n#Requires=network-online.target\nAssertPathIsDirectory=/home/ubuntu/bucket1\nAfter=network-online.target\n\n[Service]\nType=forking\nUser=root\nGroup=root\nExecStart=/usr/bin/mount-s3 bucket1 /home/ubuntu/bucket1 \\\n      --profile \"nerc\" \\\n      --endpoint-url \"https://stack.nerc.mghpcc.org:13808\" \\\n      --allow-other \\\n      --force-path-style \\\n      --debug\n\nExecStop=/bin/fusermount -u /home/ubuntu/bucket1\nRestart=always\nRestartSec=10\n\n[Install]\n#WantedBy=remote-fs.target\nWantedBy=default.target\n</code></pre> <p>Important Note</p> <p>The <code>network-online.target</code> lines ensure that mounting is not attempted until there's a network connection available. The service is launched as soon as the network is up and running, it mounts the bucket and remains active.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#launch-the-service","title":"Launch the service","text":"<p>Now reload systemd deamon:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p>Start your service</p> <pre><code>sudo systemctl start mountpoint-s3.service\n</code></pre> <p>To check the status of your service</p> <pre><code>sudo systemctl status mountpoint-s3.service\n</code></pre> <p>To enable your service on every reboot</p> <pre><code>sudo systemctl enable --now mountpoint-s3.service\n</code></pre> <p>Information</p> <p>The service name is based on the file name i.e. <code>/etc/systemd/system/mountpoint-s3.service</code> so you can just use <code>mountpoint-s3</code> instead of <code>mountpoint-s3.service</code> on all above <code>systemctl</code> commands.</p> <p>To debug you can use:</p> <p><code>sudo systemctl status mountpoint-s3.service -l --no-pager</code> or, <code>journalctl -u mountpoint-s3 --no-pager | tail -50</code></p> <p>Verify, the service is running successfully in background as <code>root</code> user:</p> <pre><code>ps aux | grep mount-s3\n</code></pre> <p>The output should look similar to:</p> <pre><code>root       13585  0.0  0.0 1060504 11672 ?       Sl   02:00   0:00 /usr/bin/mount-s3 bucket1 /home/ubuntu/bucket1 --profile nerc --endpoint-url https://stack.nerc.mghpcc.org:13808 --read-only --allow-other --force-path-style --debug\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#stopping-the-service","title":"Stopping the service","text":"<p>Stopping the service causes the container to unmount from the mount point.</p> <p>To disable your service on every reboot:</p> <pre><code>sudo systemctl disable --now mountpoint-s3.service\n</code></pre> <p>Confirm the Service is not in \"Active\" Status:</p> <pre><code>sudo systemctl status mountpoint-s3.service\n\n\u25cb mountpoint-s3.service - Mountpoint for Amazon S3 mount\n    Loaded: loaded (/etc/systemd/system/mountpoint-s3.service; disabled; vendor p&gt;\n    Active: inactive (dead)\n</code></pre> <p>Unmount the local mount point:</p> <p>If you have the local mounted directory \"bucket1\" already mounted, unmount it (replace <code>~/bucket1</code> with the location in which you have it mounted):</p> <pre><code>fusermount -u ~/bucket1\n</code></pre> <p>Or,</p> <pre><code>sudo umount -l ~/bucket1\n</code></pre> <p>Now reboot your VM:</p> <pre><code>sudo reboot\n</code></pre> <p>Further Reading</p> <p>For further details, including instructions for downloading and installing Mountpoint on various Linux operating systems, please refer to this resource.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#2-using-goofys","title":"2. Using Goofys","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#install-goofys","title":"Install goofys","text":"<p>Access your virtual machine using SSH. Update the packages on your system and install <code>wget</code> to be able to download the <code>goofys</code> binary directly to your VM:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install wget\n</code></pre> <p>Now, navigate to your home directory:</p> <pre><code>cd\n</code></pre> <p>Use <code>wget</code> to download the <code>goofys</code> binary:</p> <pre><code>wget https://github.com/kahing/goofys/releases/latest/download/goofys\n</code></pre> <p>Make the <code>goofys</code> binary executable:</p> <pre><code>chmod +x goofys\n</code></pre> <p>Copy the <code>goofys</code> binary to somewhere in your path</p> <pre><code>sudo cp goofys /usr/bin/\n</code></pre> <p>To update goofys in the future</p> <p>In order to update the newer version of <code>goofys</code> binary, you need to follow:</p> <ul> <li> <p>make sure that the data in the NERC Object Storage container is not actively used by any applications on your VM.</p> </li> <li> <p>remove the <code>goofys</code> binary from ubuntu's home directory as well as from <code>/usr/bin/</code>.</p> </li> <li> <p>execute the above commands (those starting with wget and chmod) from your home directory again and copy it to your path i.e. <code>/usr/bin/</code>.</p> </li> <li> <p>reboot your VM.</p> </li> </ul>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#provide-credentials-to-configure-goofys","title":"Provide credentials to configure goofys","text":"<p>Make a folder to store your credentials:</p> <pre><code>mkdir ~/.aws/\n</code></pre> <p>Create file <code>~/.aws/credentials</code> using your favorite text editor (for example <code>nano</code> or <code>vim</code>). Add the following contents to it which requires the <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file (during the \"Setup and enable your S3 API credentials\" step):</p> <pre><code>[nerc]\naws_access_key_id=&lt;EC2_ACCESS_KEY&gt;\naws_secret_access_key=&lt;EC2_SECRET_KEY&gt;\n</code></pre> <p>Save the file and exit the text editor.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-local-directory-as-a-mount-folder","title":"Create a local directory as a mount folder","text":"<pre><code>mkdir -p ~/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-the-container-locally-using-goofys","title":"Mount the Container locally using <code>goofys</code>","text":"<p>The object storage container i.e. \"bucket1\" will be mounted in the directory <code>~/bucket1</code></p> <pre><code>goofys -o allow_other --region RegionOne --profile \"nerc\" --endpoint \"https://stack.nerc.mghpcc.org:13808\" bucket1 ~/bucket1\n</code></pre> <p>In this command,</p> <ul> <li> <p><code>goofys</code> is the goofys binary as we already copied this in <code>/usr/bin/</code> path we     don't need to specify the full path.</p> </li> <li> <p><code>-o</code> stands for goofys options, and is handled differently.</p> </li> <li> <p><code>allow_other</code> Allows goofys with option <code>allow_other</code> only allowed if <code>user_allow_other</code>     is set in <code>/etc/fuse.conf</code>.</p> </li> <li> <p><code>--profile</code> corresponds to the name given on the <code>~/.aws/credentials</code> file i.e.     <code>[nerc]</code>.</p> </li> <li> <p><code>--endpoint</code> corresponds to the Object Storage endpoint url for NERC Object Storage.     You don't need to modify this url.</p> </li> <li> <p><code>bucket1</code> is the name of the container which contains the NERC Object Storage     resources.</p> </li> <li> <p><code>~/bucket1</code> is the location of the folder in which you want to mount the Object     Storage filesystem.</p> </li> </ul> <p>In order to test whether the mount was successful, navigate to the directory in which you mounted the NERC container repository, for example:</p> <pre><code>cd ~/bucket1\n</code></pre> <p>Use the <code>ls</code> command to list its content. You should see the output similar to this:</p> <pre><code>ls\n\nREADME.md   image.png   test-file\n</code></pre> <p>The NERC Object Storage container repository has now been mounted using <code>goofys</code>.</p> <p>Very Important Information</p> <p>Please note that any of these mounted buckets is not persistent if your VM is stopped or rebooted in the future. After each reboot, you will need to execute the mounting command as mentioned above again. To enable automatic mounting at system startup, please follow the steps as outlined here.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mounting-on-system-startup","title":"Mounting on system startup","text":"<p>Mounts can be set to occur automatically during system initialization so that mounted file systems will persist even after the VM reboot.</p> <p>Create directory in <code>/root</code> folder in which you will store the credentials:</p> <pre><code>sudo mkdir /root/.aws\n</code></pre> <p>Copy the credentials you created in your local directory to the <code>.aws</code> directory in the <code>/root</code> folder:</p> <pre><code>sudo cp ~/.aws/credentials /root/.aws/\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#configure-mounting-of-the-bucket1-container","title":"Configure mounting of the <code>bucket1</code> container","text":"<p>Open the file <code>/etc/fstab</code> using your favorite command line text editor for editing. You will need sudo privileges for that. For example, if you want to use nano, execute this command:</p> <pre><code>sudo nano /etc/fstab\n</code></pre> <p>Proceed with one of the methods below depending on whether you wish to have the \"bucket1\" repository automatically mounted at system startup:</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#method-1-mount-the-repository-automatically-on-system-startup","title":"Method 1: Mount the repository automatically on system startup","text":"<p>Add the following line to the <code>/etc/fstab</code> file:</p> <pre><code>/usr/bin/goofys#bucket1 /home/ubuntu/bucket1 fuse _netdev,allow_other,--dir-mode=0777,--file-mode=0666,--region=RegionOne,--profile=nerc,--endpoint=https://stack.nerc.mghpcc.org:13808 0 0\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#method-2-do-not-mount-the-repository-automatically-on-system-startup","title":"Method 2: Do NOT mount the repository automatically on system startup","text":"<p>Add the following line to the <code>/etc/fstab</code> file:</p> <pre><code>/usr/bin/goofys#bucket1 /home/ubuntu/bucket1 fuse noauto,_netdev,allow_other,--dir-mode=0777,--file-mode=0666,--region=RegionOne,--profile=nerc,--endpoint=https://stack.nerc.mghpcc.org:13808 0 0\n</code></pre> <p>The difference between this code and the code mentioned in Method 1 is the addition of the option <code>noauto</code>.</p> <p>Content of /etc/fstab</p> <p>In the <code>/etc/fstab</code> content as added above:</p> <pre><code>grep goofys /etc/fstab\n</code></pre> <p>The output should look similar to:</p> <pre><code>/usr/bin/goofys#bucket1 /home/ubuntu/bucket1 fuse _netdev,allow_other,--dir-mode=0777,--file-mode=0666,--region=RegionOne,--profile=nerc,--endpoint=https://stack.nerc.mghpcc.org:13808 0 0\n</code></pre> <ul> <li> <p><code>/usr/bin/goofys</code> with the location of your <code>goofys</code> binary.</p> </li> <li> <p><code>/home/ubuntu/bucket1</code> is the location in which you wish to mount <code>bucket1</code> container from your NERC Object Storage.</p> </li> <li> <p><code>--profile=nerc</code> is the name you mentioned on the <code>~/.aws/credentials</code> file i.e. <code>[nerc]</code>.</p> </li> </ul> <p>Once you have added that line to your <code>/etc/fstab</code> file, reboot the VM. After the system has restarted, check whether the NERC Object Storage repository i.e. <code>bucket1</code> is mounted in the directory specified by you i.e. in <code>/home/ubuntu/bucket1</code>.</p> <p>Important Information</p> <p>If you just want to test your mounting command written in <code>/etc/fstab</code> without \"Rebooting\" the VM you can also do that by running <code>sudo mount -a</code>. If there's no error, your mount is working and will persist after reboot.</p> <p>And if you want to stop automatic mounting of the container from the NERC Object Storage repository i.e. <code>bucket1</code>, remove the line you added in the <code>/etc/fstab</code> file. You can also comment it out by adding <code>#</code> character in front of that line. After that, reboot the VM. Optionally, you can also remove the <code>goofys</code> binary and the credentials file located at <code>~/.aws/credentials</code> if you no longer want to use <code>goofys</code>.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#3-using-s3fs","title":"3. Using S3FS","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#install-s3fs","title":"Install S3FS","text":"<p>Access your virtual machine using SSH. Update the packages on your system and install <code>s3fs</code>:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install s3fs\n</code></pre> <p>For RedHat/Rocky/AlmaLinux</p> <p>The RedHat/Rocky/AlmaLinux repositiories do not have <code>s3fs</code>. Therefore, you will need to compile it yourself.</p> <p>First, using your local computer, visit the following website (it contains the releases of <code>s3fs</code>): https://github.com/s3fs-fuse/s3fs-fuse/releases/latest.</p> <p>Then, in the section with the most recent release find the part Assets. From there, find the link to the zip version of the Source code.</p> <p></p> <p>Right click on one of the Source Code i.e. \"v1.94.zip\" and select the \"Copy link address\". You will need this link to use later as a parameter for the <code>wget</code> command to download it to your virtual machine.</p> <p>Access your VM on the NERC OpenStack using the web console or SSH.</p> <p>Update your packages:</p> <pre><code>sudo dnf update -y\n</code></pre> <p>Install the prerequisites including fuse, the C++ compiler and make:</p> <pre><code>sudo dnf config-manager --set-enabled crb\n\nsudo dnf install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel wget unzip\n\n# OR, sudo dnf --enablerepo=crb install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel wget unzip\n</code></pre> <p>Now, use <code>wget</code> to download the source code. Replace https://github.com/s3fs-fuse/s3fs-fuse/archive/refs/tags/v1.94.zip with the link to the source code you found previously:</p> <pre><code>wget https://github.com/s3fs-fuse/s3fs-fuse/archive/refs/tags/v1.94.zip\n</code></pre> <p>Use the <code>ls</code> command to verify that the zip archive has been downloaded:</p> <pre><code>ls\n</code></pre> <p>Unzip the archive (replace v1.94.zip with the name of the archive you downloaded):</p> <pre><code>unzip v1.94.zip\n</code></pre> <p>Use the ls command to find the name of the folder you just extracted:</p> <pre><code>ls\n</code></pre> <p>Now, navigate to that folder (replace s3fs-fuse-1.94 with the name of the folder you just extracted):</p> <pre><code>cd s3fs-fuse-1.94\n</code></pre> <p>Perform the compilation by executing the following commands in order:</p> <pre><code>./autogen.sh\n./configure\nmake\nsudo make install\n</code></pre> <p><code>s3fs</code> should now be installed in <code>/usr/local/bin/s3fs</code>.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-file-which-will-store-the-s3-credentials","title":"Create a file which will store the S3 Credentials","text":"<p>Store your S3 credentials in a file <code>${HOME}/.passwd-s3fs</code> and set \"owner-only\" permissions. Run the following command to create a pair of <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file (above) to store them in the file.</p> <pre><code>echo EC2_ACCESS_KEY:EC2_SECRET_KEY &gt; ${HOME}/.passwd-s3fs\n</code></pre> <p>Change the permissions of this file to 600 to set \"owner-only\" permissions:</p> <pre><code>chmod 600 ${HOME}/.passwd-s3fs\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-container-in-the-nerc-projects-object-storage","title":"Create a Container in the NERC Project's Object storage","text":"<p>We create it using the OpenStack Swift client:</p> <pre><code>sudo apt install python3-swiftclient\n</code></pre> <p>Let's call the Container \"bucket1\"</p> <pre><code>swift post bucket1\n</code></pre> <p>More about Swift Interface</p> <p>You can read more about using Swift Interface for NERC Object Storage here.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-local-directory-as-a-mount-point-in-your-vm","title":"Create a local directory as a mount point in your VM","text":"<pre><code>mkdir -p ~/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-the-container-locally-using-s3fs","title":"Mount the Container locally using <code>s3fs</code>","text":"<p>The object storage container i.e. \"bucket1\" will be mounted in the directory <code>~/bucket1</code></p> <pre><code>s3fs bucket1 ~/bucket1 -o passwd_file=~/.passwd-s3fs -o url=https://stack.nerc.mghpcc.org:13808 -o use_path_request_style -o umask=0002\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#unmount-the-local-mount-point","title":"Unmount the local mount point","text":"<p>If you have the local mounted directory \"bucket1\" already mounted, unmount it (replace <code>~/bucket1</code> with the location in which you have it mounted):</p> <pre><code>sudo umount -l ~/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#configure-mounting-of-the-bucket1-repository","title":"Configure mounting of the <code>bucket1</code> repository","text":"<p>Open the file <code>/etc/fstab</code> using your favorite command line text editor for editing. You will need sudo privileges for that. For example, if you want to use nano, execute this command:</p> <pre><code>sudo nano /etc/fstab\n</code></pre> <p>Proceed with one of the methods below depending on whether you wish to have the \"bucket1\" repository automatically mounted at system startup:</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#method-1-mount-the-repository-automatically-on-startup","title":"Method 1: Mount the repository automatically on startup","text":"<p>Add the following line to the <code>/etc/fstab</code> file:</p> <pre><code>/usr/bin/s3fs#bucket1 /home/ubuntu/bucket1 fuse passwd_file=/home/ubuntu/.passwd-s3fs,_netdev,allow_other,use_path_request_style,uid=0,umask=0222,mp_umask=0222,gid=0,url=https://stack.nerc.mghpcc.org:13808 0 0\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#method-2-do-not-mount-the-repository-automatically-on-startup","title":"Method 2: Do NOT mount the repository automatically on startup","text":"<p>Add the following line to the <code>/etc/fstab</code> file:</p> <pre><code>/usr/bin/s3fs#bucket1 /home/ubuntu/bucket1 fuse noauto,passwd_file=/home/ubuntu/.passwd-s3fs,_netdev,allow_other,use_path_request_style,uid=0,umask=0222,mp_umask=0222,gid=0,url=https://stack.nerc.mghpcc.org:13808 0 0\n</code></pre> <p>The difference between this code and the code mentioned in Method 1 is the addition of the option <code>noauto</code>.</p> <p>Content of /etc/fstab</p> <p>In the <code>/etc/fstab</code> content as added above:</p> <ul> <li> <p><code>/usr/bin/s3fs</code> is the location of your <code>s3fs</code> binary. If you installed it using <code>apt</code> on Debian or Ubuntu, you do not have to change anything here. If you are using a self-compiled version of <code>s3fs</code> created on RedHat/Rocky/AlmaLinux as explained above, that location is <code>/usr/local/bin/s3fs</code>.</p> </li> <li> <p><code>/home/ubuntu/.passwd-s3fs</code> is the location of the file which contains the key pair used for mounting the \"bucket1\" repository as we named it in previous step.</p> </li> </ul>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#4-using-rclone","title":"4. Using Rclone","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#installing-rclone","title":"Installing Rclone","text":"<p>Install rclone as described here or for our Ubuntu based VM we can just SSH into the VM and then run the following command using default <code>ubuntu</code> user:</p> <pre><code>curl -sSL https://rclone.org/install.sh | sudo bash\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#configuring-rclone","title":"Configuring Rclone","text":"<p>If you run <code>rclone config file</code> you will see where the default location is for you.</p> <pre><code>rclone config file\nConfiguration file doesn't exist, but rclone will use this path:\n/home/ubuntu/.config/rclone/rclone.conf\n</code></pre> <p>So create the config file as mentioned above path: <code>/home/ubuntu/.config/rclone/rclone.conf</code> and add the following entry with the name [nerc]:</p> <pre><code>[nerc]\ntype = s3\nenv_auth = false\nprovider = Other\nendpoint = https://stack.nerc.mghpcc.org:13808\nacl = public-read\naccess_key_id = &lt;YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE&gt;\nsecret_access_key = &lt;YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE&gt;\nlocation_constraint =\nserver_side_encryption =\n</code></pre> <p>More about the config for AWS S3 compatible API can be seen here.</p> <p>Important Information</p> <p>Mind that if set <code>env_auth = true</code> then it will take variables from environment, so you shouldn't insert it in this case.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#listing-the-containers-and-contents-of-a-container","title":"Listing the Containers and Contents of a Container","text":"<p>Once your Object Storage has been configured in Rclone, you can then use the Rclone interface to List all the Containers with the \"lsd\" command</p> <pre><code>rclone lsd \"nerc:\"\n</code></pre> <p>Or,</p> <pre><code>rclone lsd \"nerc:\" --config=rclone.conf\n</code></pre> <p>For e.g.,</p> <pre><code>rclone lsd \"nerc:\" --config=rclone.conf\n      -1 2024-04-23 20:21:43        -1 bucket1\n</code></pre> <p>To list the files and folders available within a container i.e. \"bucket1\" in this case, within a container we can use the \"ls\" command:</p> <pre><code>rclone ls \"nerc:bucket1/\"\n  653 README.md\n    0 image.png\n   12 test-file\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-mount-point-directory","title":"Create a mount point directory","text":"<pre><code>mkdir -p bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-the-container-with-rclone","title":"Mount the container with Rclone","text":"<p>Start the mount like this, where <code>home/ubuntu/bucket1</code> is an empty existing directory:</p> <pre><code>rclone -vv --vfs-cache-mode full mount nerc:bucket1 /home/ubuntu/bucket1 --allow-other --allow-non-empty\n</code></pre> <p>On Linux, you can run mount in either foreground or background (aka <code>daemon</code>) mode. Mount runs in <code>foreground</code> mode by default. Use the <code>--daemon</code> flag to force background mode i.e.</p> <pre><code>rclone mount remote:path/to/files /path/to/local/mount --daemon\n</code></pre> <p>When running in background mode the user will have to stop the mount manually:</p> <pre><code>fusermount -u /path/to/local/mount\n</code></pre> <p>Or,</p> <pre><code>sudo umount -l /path/to/local/mount\n</code></pre> <p>Now we have the mount running and we have background mode also enabled. Lets say there is a scenario where we want the mount to be persistent after a server/machine reboot. There are few ways to do it:</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-systemd-unit-file-ie-rclone-mountservice","title":"Create systemd unit file i.e. <code>rclone-mount.service</code>","text":"<p>Create a systemd service unit file that is going to execute the above script and dynamically mount or unmount the container:</p> <pre><code>sudo nano /etc/systemd/system/rclone-mount.service\n</code></pre> <p>Edit the file to look like the below:</p> <pre><code>[Unit]\nDescription=rclone mount\nDocumentation=http://rclone.org/docs/\nAssertPathIsDirectory=/home/ubuntu/bucket1\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=root\nGroup=root\nExecStart=/usr/bin/rclone mount \\\n      --config=home/ubuntu/.config/rclone/rclone.conf \\\n      --vfs-cache-mode full \\\n      nerc:bucket1 /home/ubuntu/bucket1 \\\n              --allow-other \\\n              --allow-non-empty\n\nExecStop=/bin/fusermount -u /home/ubuntu/bucket1\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>The service is launched as soon as the network is up and running, it mounts the bucket and remains active. Stopping the service causes the container to unmount from the mount point.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#launch-the-service-using-a-service-manager","title":"Launch the service using a service manager","text":"<p>Now reload systemd deamon:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p>Start your service</p> <pre><code>sudo systemctl start rclone-mount.service\n</code></pre> <p>To check the status of your service</p> <pre><code>sudo systemctl status rclone-mount.service\n</code></pre> <p>To enable your service on every reboot</p> <pre><code>sudo systemctl enable --now rclone-mount.service\n</code></pre> <p>Information</p> <p>The service name is based on the file name i.e. <code>/etc/systemd/system/rclone-mount.service</code> so you can just use <code>rclone-mount</code> instead of <code>rclone-mount.service</code> on all above <code>systemctl</code> commands.</p> <p>To debug you can use:</p> <p><code>sudo systemctl status rclone-mount.service -l --no-pager</code> or, <code>journalctl -u rclone-mount --no-pager | tail -50</code></p> <p>Verify, if the container is mounted successfully:</p> <pre><code>df -hT | grep rclone\n</code></pre> <p>The output should look similar to:</p> <pre><code>nerc:bucket1   fuse.rclone  1.0P     0  1.0P   0% /home/ubuntu/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#5-using-juicefs","title":"5. Using JuiceFS","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#preparation","title":"Preparation","text":"<p>A JuiceFS file system consists of two parts:</p> <ul> <li> <p>Object Storage: Used for data storage.</p> </li> <li> <p>Metadata Engine: A database used for storing metadata. In this case, we will     use a durable Redis in-memory database service that     provides extremely fast performance.</p> </li> </ul>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#installation-of-the-juicefs-client","title":"Installation of the JuiceFS client","text":"<p>Access your virtual machine using SSH. Update the packages on your system and install the JuiceFS client:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n# default installation path is /usr/local/bin\ncurl -sSL https://d.juicefs.com/install | sh -\n</code></pre> <p>Verify the JuiceFS client is running in background:</p> <pre><code>ps aux | grep juicefs\n</code></pre> <p>The output should look similar to:</p> <pre><code>ubuntu     16275  0.0  0.0   7008  2212 pts/0    S+   18:44   0:00 grep --color=auto juicefs\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#installing-and-configuring-redis-database","title":"Installing and Configuring Redis database","text":"<p>Install Redis by running:</p> <pre><code>sudo apt install redis-server\n</code></pre> <p>This will download and install Redis and its dependencies. Following this, there is one important configuration change to make in the Redis configuration file, which was generated automatically during the installation.</p> <p>You can check the line number where to find <code>supervised</code> by running:</p> <pre><code>sudo cat /etc/redis/redis.conf -n | grep supervised\n\n228  #   supervised no      - no supervision interaction\n229  #   supervised upstart - signal upstart by putting Redis into SIGSTOP mode\n231  #   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET\n232  #   supervised auto    - detect upstart or systemd method based on\n236  supervised no\n</code></pre> <p>Open this file with your preferred text editor:</p> <pre><code>sudo nano /etc/redis/redis.conf -l\n</code></pre> <p>Inside the config file, find the <code>supervised</code> directive. This directive allows you to declare an init system to manage Redis as a service, providing you with more control over its operation. The <code>supervised</code> directive is set to <code>no</code> by default. Since you are running <code>Ubuntu</code>, which uses the systemd init system, change this to <code>systemd</code> as shown here:</p> <p></p> <ul> <li>Binding to localhost:</li> </ul> <p>By default, Redis is only accessible from <code>localhost</code>. We need to verify that by locating this line by running:</p> <pre><code>sudo cat /etc/redis/redis.conf -n | grep bind\n\n...\n68  bind 127.0.0.1 ::1\n...\n</code></pre> <p>and make sure it is uncommented (remove the <code>#</code> if it exists) by editing this file with your preferred text editor.</p> <p>So save and close it when you are finished. If you used <code>nano</code> to edit the file, do so by pressing <code>CTRL + X</code>, <code>Y</code>, then <code>ENTER</code>.</p> <p>Then, restart the Redis service to reflect the changes you made to the configuration file:</p> <pre><code>sudo systemctl restart redis.service\n</code></pre> <p>With that, you've installed and configured Redis and it's running on your machine. Before you begin using it, you should first check whether Redis is functioning correctly.</p> <p>Start by checking that the Redis service is running:</p> <pre><code>sudo systemctl status redis\n</code></pre> <p>If it is running without any errors, this command will show \"active (running)\" Status.</p> <p>To test that Redis is functioning correctly, connect to the server using <code>redis-cli</code>, Redis's command-line client:</p> <pre><code>redis-cli\n</code></pre> <p>In the prompt that follows, test connectivity with the <code>ping</code> command:</p> <pre><code>ping\n</code></pre> <p>Output:</p> <pre><code>PONG\n</code></pre> <p>Also, check that binding to <code>localhost</code> is working fine by running the following <code>netstat</code> command:</p> <pre><code>sudo netstat -lnp | grep redis\n\ntcp        0      0 127.0.0.1:6379          0.0.0.0:*               LISTEN      16967/redis-server\ntcp6       0      0 ::1:6379                :::*                    LISTEN      16967/redis-server\n</code></pre> <p>Important Note</p> <p>The <code>netstat</code> command may not be available on your system by default. If this is the case, you can install it (along with a number of other handy networking tools) with the following command: <code>sudo apt install net-tools</code>.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#configuring-a-redis-password","title":"Configuring a Redis Password","text":"<p>Configuring a Redis password enables one of its two built-in security features  - the <code>auth</code> command, which requires clients to authenticate to access the database. The password is configured directly in Redis's configuration file, <code>/etc/redis/redis.conf</code>.</p> <p>First, we need to locate the line where the <code>requirepass</code> directive is mentioned:</p> <pre><code>sudo cat /etc/redis/redis.conf -n | grep requirepass\n\n...\n790  # requirepass foobared\n...\n</code></pre> <p>Then open the Redis's config file i.e. <code>/etc/redis/redis.conf</code> again with your preferred editor:</p> <pre><code>sudo nano /etc/redis/redis.conf -l\n</code></pre> <p>Uncomment it by removing the <code>#</code>, and change <code>foobared</code> to a secure password.</p> <p>How to generate random password?</p> <p>You can use <code>openssl</code> to generate random password by running the following command locally:</p> <p><code>openssl rand 12 | openssl base64 -A</code></p> <p><code>&lt;your_redis_password&gt;</code></p> <p>After saving and closing it when you are finished. You need to restart the Redis service to reflect the changes you made to the configuration file by running:</p> <pre><code>sudo systemctl restart redis.service\n</code></pre> <p>To test that the password works, open up the Redis client:</p> <pre><code>redis-cli\n</code></pre> <p>The following shows a sequence of commands used to test whether the Redis password works. The first command tries to set a key to a value before authentication:</p> <pre><code>127.0.0.1:6379&gt; set key1 10\n</code></pre> <p>That won't work because you didn't authenticate, so Redis returns an error:</p> <p>Output:</p> <pre><code>(error) NOAUTH Authentication required.\n</code></pre> <p>The next command authenticates with the password specified in the Redis configuration file:</p> <pre><code>127.0.0.1:6379&gt; auth &lt;your_redis_password&gt;\n</code></pre> <p>Redis acknowledges:</p> <p>Output:</p> <pre><code>OK\n</code></pre> <p>After that, running the previous command again will succeed:</p> <pre><code>127.0.0.1:6379&gt; set key1 10\n</code></pre> <p>Output:</p> <pre><code>OK\n</code></pre> <p><code>get key1</code> queries Redis for the value of the new key.</p> <pre><code>127.0.0.1:6379&gt; get key1\n</code></pre> <p>Output:</p> <pre><code>\"10\"\n</code></pre> <p>After confirming that you're able to run commands in the Redis client after authenticating, you can exit redis-cli:</p> <pre><code>127.0.0.1:6379&gt; quit\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#setting-authorizing-s3-access-using-juicefs-config","title":"Setting authorizing S3 access using <code>juicefs config</code>","text":"<p>You can store the S3 credentials using <code>juicefs config</code> that allows us to add the <code>Access Key</code> and <code>Secret Key</code> for the file system by running:</p> <pre><code>juicefs config \\\n--access-key=&lt;EC2_ACCESS_KEY&gt; \\\n--secret-key=&lt;EC2_SECRET_KEY&gt; \\\nredis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#formatting-file-system","title":"Formatting file system","text":"<pre><code>sudo juicefs format --storage s3 --bucket https://stack.nerc.mghpcc.org:13808/&lt;your_container&gt; redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1 myjfs\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mounting-file-system-manually","title":"Mounting file system manually","text":""},{"location":"openstack/persistent-storage/mount-the-object-storage/#create-a-local-directory-as-a-mount-point-folder","title":"Create a local directory as a mount point folder","text":"<pre><code>mkdir -p ~/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-the-container-locally-using-juicefs","title":"Mount the Container locally using <code>juicefs</code>","text":"<p>The formatted file system \"myjfs\" will be mounted in the directory <code>~/bucket1</code> by running the following command:</p> <pre><code>juicefs mount redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1 ~/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#mount-juicefs-at-boot-time","title":"Mount JuiceFS at Boot Time","text":"<p>After JuiceFS has been successfully formatted, follow this guide to set up auto-mount on boot.</p> <p>We can specify the <code>--update-fstab</code> option on the <code>mount</code> command that will automatically help you set up mount at boot:</p> <pre><code>sudo juicefs mount --update-fstab --max-uploads=50 --writeback --cache-size 204800 &lt;META-URL&gt; &lt;MOUNTPOINT&gt;\n</code></pre> <p>Make sure <code>/etc/fstab</code> contains an entry for the mount point by running:</p> <pre><code>grep &lt;MOUNTPOINT&gt; /etc/fstab\n</code></pre> <p>The output should look similar to:</p> <pre><code>&lt;META-URL&gt; &lt;MOUNTPOINT&gt; juicefs _netdev,max-uploads=50,writeback,cache-size=204800 0 0\n\nls -l /sbin/mount.juicefs\nlrwxrwxrwx 1 root root 22 Apr 24 20:25 /sbin/mount.juicefs -&gt; /usr/local/bin/juicefs\n</code></pre> <p>For example,</p> <pre><code>sudo juicefs mount --update-fstab --max-uploads=50 --writeback --cache-size 204800 redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1 ~/bucket1\n</code></pre> <p>Make sure <code>/etc/fstab</code> contains an entry for the mount point by running:</p> <pre><code>grep juicefs /etc/fstab\n</code></pre> <p>The output should look similar to:</p> <pre><code>redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1  /home/ubuntu/bucket1  juicefs  _netdev,cache-size=204800,max-uploads=50,writeback  0 0\n</code></pre> <p>Confirm that <code>/sbin/mount.juicefs</code> is correctly symlinked to the JuiceFS binary:</p> <pre><code>ls -l /sbin/mount.juicefs\nlrwxrwxrwx 1 root root 22 Apr 24 20:25 /sbin/mount.juicefs -&gt; /usr/local/bin/juicefs\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#automating-mounting-with-systemd-service-unit-file","title":"Automating Mounting with systemd service unit file","text":"<p>If you're using JuiceFS and need to apply settings like database access password, S3 access key, and secret key, which are hidden from the command line using environment variables for security reason, it may not be easy to configure them in the <code>/etc/fstab</code> file. In such cases, you can utilize <code>systemd</code> to mount your JuiceFS instance.</p> <p>Here's how you can set up your systemd configuration file:</p> <p>Create a systemd service unit file that is going to execute the above script and dynamically mount or unmount the container:</p> <pre><code>sudo nano /etc/systemd/system/juicefs-mount.service\n</code></pre> <p>Edit the file to look like the below:</p> <pre><code>[Unit]\nDescription=JuiceFS mount\nDocumentation=https://juicefs.com/docs/\nAssertPathIsDirectory=/home/ubuntu/bucket1\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=root\nGroup=root\nExecStart=/usr/local/bin/juicefs mount \\\n\"redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1\" \\\n/home/ubuntu/bucket1 \\\n--no-usage-report \\\n--writeback \\\n--cache-size 102400 \\\n--cache-dir /home/juicefs_cache \\\n--buffer-size 2048 \\\n--open-cache 0 \\\n--attr-cache 1 \\\n--entry-cache 1 \\\n--dir-entry-cache 1 \\\n--cache-partial-only false \\\n--free-space-ratio 0.1 \\\n--max-uploads 20 \\\n--max-deletes 10 \\\n--backup-meta 0 \\\n--log /var/log/juicefs.log \\\n--get-timeout 300 \\\n--put-timeout 900 \\\n--io-retries 90 \\\n--prefetch 1\n\nExecStop=/usr/local/bin/juicefs umount /home/ubuntu/bucket1\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Important Information</p> <p>Feel free to modify the options and environments according to your needs. Please make sure you change <code>&lt;your_redis_password&gt;</code> to your own Redis password that was setup by following this step.</p> <p>The service is launched as soon as the network is up and running, it mounts the bucket and remains active. Stopping the service causes the container to unmount from the mount point.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#launch-the-service-as-daemon","title":"Launch the service as daemon","text":"<p>Now reload systemd deamon:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p>Start your service</p> <pre><code>sudo systemctl start juicefs-mount.service\n</code></pre> <p>To check the status of your service</p> <pre><code>sudo systemctl status juicefs-mount.service\n</code></pre> <p>To enable your service on every reboot</p> <pre><code>sudo systemctl enable --now juicefs-mount.service\n</code></pre> <p>Information</p> <p>The service name is based on the file name i.e. <code>/etc/systemd/system/juicefs-mount.service</code> so you can just use <code>juicefs-mount</code> instead of <code>juicefs-mount.service</code> on all above <code>systemctl</code> commands.</p> <p>To debug you can use:</p> <p><code>sudo systemctl status juicefs-mount.service -l --no-pager</code> or, <code>journalctl -u juicefs-mount --no-pager | tail -50</code></p> <p>Verify, if the container is mounted successfully:</p> <pre><code>df -hT | grep juicefs\n</code></pre> <p>The output should look similar to:</p> <pre><code>JuiceFS:myjfs  fuse.juicefs  1.0P  4.0K  1.0P   1% /home/ubuntu/bucket1\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#data-synchronization","title":"Data Synchronization","text":"<p><code>juicefs sync</code> is a powerful data migration tool, which can copy data across all supported storages including object storage, JuiceFS itself, and local file systems, you can freely copy data between any of these systems.</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#command-syntax","title":"Command Syntax","text":"<p>To synchronize data from <code>SRC</code> i.e. the source data address or path to <code>DST</code> i.e. the destination address or path;, capable for both directories and files.</p> <pre><code>juicefs sync [command options] SRC DST\n</code></pre> <p>More Information</p> <p><code>[command options]</code> are synchronization options. See command reference for more details.</p> <p>Address format:</p> <pre><code>[NAME://][ACCESS_KEY:SECRET_KEY[:TOKEN]@]BUCKET[.ENDPOINT][/PREFIX]\n\n# MinIO only supports path style\nminio://[ACCESS_KEY:SECRET_KEY[:TOKEN]@]ENDPOINT/BUCKET[/PREFIX]\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#synchronize-between-object-storage-and-juicefs","title":"Synchronize between Object Storage and JuiceFS","text":"<p>The following command synchronizes <code>movies</code> container on Object Storage Container to your local JuiceFS File System i.e <code>~/jfs</code>:</p> <pre><code># create local folder\nmkdir -p ~/jfs\n# mount JuiceFS\njuicefs mount -d redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1 ~/jfs\n# synchronize\njuicefs sync --force-update s3://&lt;EC2_ACCESS_KEY&gt;:&lt;EC2_SECRET_KEY&gt;@movies.stack.nerc.mghpcc.org:13808/ ~/jfs/\n</code></pre> <p>The following command synchronizes <code>images</code> directory from your local JuiceFS File System i.e <code>~/jfs</code> to Object Storage Container i.e. <code>movies</code> container:</p> <pre><code># mount JuiceFS\njuicefs mount -d redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1 ~/jfs\n# create local folder and add some file to this folder\nmkdir -p ~/jfs/images/\ncp \"test.image\" ~/jfs/images/\n# synchronization\njuicefs sync --force-update ~/jfs/images/ s3://&lt;EC2_ACCESS_KEY&gt;:&lt;EC2_SECRET_KEY&gt;@movies.stack.nerc.mghpcc.org:13808/images/\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#how-to-destroy-a-file-system","title":"How to destroy a file system","text":"<p>After JuiceFS has been successfully formatted, follow this guide to clean up.</p> <p>JuiceFS client provides the destroy command to completely destroy a file system, which will result in:</p> <ul> <li> <p>Deletion of all metadata entries of this file system</p> </li> <li> <p>Deletion of all data blocks of this file system</p> </li> </ul> <p>Use this command in the following format:</p> <pre><code>juicefs destroy &lt;METADATA URL&gt; &lt;UUID&gt;\n</code></pre> <p>Here,</p> <p><code>&lt;METADATA URL&gt;</code>: The URL address of the metadata engine</p> <p><code>&lt;UUID&gt;</code>: The UUID of the file system</p>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#find-the-uuid-of-existing-mount-file-system","title":"Find the UUID of existing mount file system","text":"<p>You can run either <code>juicefs config redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1</code> or <code>juicefs status redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1</code> to get detailed information about mounted file system i.e. \"myjfs\" that is setup by following this step. The output looks like shown here:</p> <pre><code>{\n...\n\"Name\": \"myjfs\",\n\"UUID\": \"&lt;UUID&gt;\",\n...\n}\n</code></pre>"},{"location":"openstack/persistent-storage/mount-the-object-storage/#destroy-a-file-system","title":"Destroy a file system","text":"<p>Please note the \"UUID\" that you will need to run <code>juicefs destroy</code> command as shown below:</p> <pre><code>juicefs destroy redis://default:&lt;your_redis_password&gt;@127.0.0.1:6379/1 &lt;UUID&gt; --force\n</code></pre> <p>When destroying a file system, the client will issue a confirmation prompt. Please make sure to check the file system information carefully and enter <code>y</code> after confirming it is correct.</p> <p>Danger</p> <p>The destroy operation will cause all the data in the database and the object storage associated with the file system to be deleted. Please make sure to back up the important data before operating!</p>"},{"location":"openstack/persistent-storage/object-storage/","title":"Object Storage/ Swift","text":""},{"location":"openstack/persistent-storage/object-storage/#object-storage","title":"Object Storage","text":"<p>OpenStack Object Storage (Swift) is a highly available, distributed, eventually consistent object/blob store. Object Storage is used to manage cost-effective and long-term preservation and storage of large amounts of data across clusters of standard server hardware. The common use cases include the storage, backup and archiving of unstructured data, such as documents, static web content, images, video files, and virtual machine images, etc.</p> <p>The end-users can interact with the object storage system through a RESTful HTTP API i.e. the Swift API or use one of the many client libraries that exist for all of the popular programming languages, such as Java, Python, Ruby, and C# based on provisioned quotas. Swift also supports and is compatible with Amazon's Simple Storage Service (S3) API that makes it easier for the end-users to move data between multiple storage end points and supports hybrid cloud setup.</p>"},{"location":"openstack/persistent-storage/object-storage/#1-access-by-web-interface-ie-horizon-dashboard","title":"1. Access by Web Interface i.e. Horizon Dashboard","text":"<p>To get started, navigate to Project -&gt; Object Store -&gt; Containers.</p> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#create-a-container","title":"Create a Container","text":"<p>In order to store objects, you need at least one Container to put them in. Containers are essentially top-level directories. Other services use the terminology buckets.</p> <p>Click Create Container. Give your container a name.</p> <p></p> <p>Important Note</p> <p>The container name needs to be unique, not just within your project but across all of our OpenStack installation. If you get an error message after trying to create the container, try giving it a more unique name.</p> <p>For now, leave the \"Container Access\" set to Private.</p>"},{"location":"openstack/persistent-storage/object-storage/#upload-a-file","title":"Upload a File","text":"<p>Click on the name of your container, and click the Upload File icon as shown below:</p> <p></p> <p>Click Browse and select a file from your local machine to upload.</p> <p>It can take a while to upload very large files, so if you're just testing it out you may want to use a small text file or similar.</p> <p></p> <p>By default the File Name will be the same as the original file, but you can change it to another name. Click \"Upload File\". Your file will appear inside the container as shown below once successful:</p> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#using-folders","title":"Using Folders","text":"<p>Files stored by definition do not organize objects into folders, but you can use folders to keep your data organized.</p> <p>On the backend, the folder name is actually just prefixed to the object name, but from the web interface (and most other clients) it works just like a folder.</p> <p>To add a folder, click on the \"+ folder\" icon as shown below:</p> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#make-a-container-public","title":"Make a container public","text":"<p>Making a container public allows you to send your collaborators a URL that gives access to the container's contents.</p> <p>Hosting a static website using public Container</p> <p>You can use public Container to host a static website. On a static website, individual webpages include static website content (HTML, CSS etc.). They might also contain client-side scripts (e.g. JavaScript).</p> <p>Click on your container's name, then check the \"Public Access\" checkbox. Note that \"Public Access\" changes from \"Disabled\" to \"Link\".</p> <p></p> <p>Click \"Link\" to see a list of object in the container. This is the URL of your container.</p> <p>Important Note</p> <p>Anyone who obtains the URL will be able to access the container, so this is not recommended as a way to share sensitive data with collaborators.</p> <p>In addition, everything inside a public container is public, so we recommend creating a separate container specifically for files that should be made public.</p> <p>To download the file <code>test-file</code> we would use the following url.</p> <p>Very Important Information</p> <p>Here <code>4c5bccef73c144679d44cbc96b42df4e</code> is specific Tenant Id or Project Id. You can get this value when you click on the public container's Link on a new browser tab.</p> <p>Or, you can just click on \"Download\" next to the file's name as shown below:</p> <p></p> <p>You can also interact with public objects using a utility such as <code>curl</code>:</p> <pre><code>curl https://stack.nerc.mghpcc.org:13808/v1/AUTH_4c5bccef73c144679d44cbc96b42df4e/unique-container-test\ntest-file\n</code></pre> <p>To download a file:</p> <pre><code>curl -o local-file.txt https://stack.nerc.mghpcc.org:13808/v1/AUTH_4c5bccef73c144679d44cbc96b42df4e/unique-container-test/test-file\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#make-a-container-private","title":"Make a container private","text":"<p>You can make a public container private by clicking on your container's name, then uncheck the \"Public Access\" checkbox. Note that \"Public Access\" changes from \"Link\" to \"Disabled\".</p> <p>This will deactivate the public URL of the container and then it will show \"Disabled\".</p> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#2-access-by-using-apis","title":"2. Access by using APIs","text":""},{"location":"openstack/persistent-storage/object-storage/#i-openstack-cli","title":"i. OpenStack CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/object-storage/#some-object-storage-management-examples","title":"Some Object Storage management examples","text":""},{"location":"openstack/persistent-storage/object-storage/#create-a-container_1","title":"Create a container","text":"<p>In order to create a container in the Object Storage service, you can use the openstack client with the following command.</p> <pre><code>openstack container create mycontainer\n+---------------------------------------+-------------+------------------------------------+\n| account                               | container   | x-trans-id                         |\n+---------------------------------------+-------------+------------------------------------+\n| AUTH_4c5bccef73c144679d44cbc96b42df4e | mycontainer | txb875f426a011476785171-00624b37e8 |\n+---------------------------------------+-------------+------------------------------------+\n</code></pre> <p>Once created you can start adding objects.</p>"},{"location":"openstack/persistent-storage/object-storage/#manipulate-objects-in-a-container","title":"Manipulate objects in a container","text":"<p>To upload files to a container you can use the following command</p> <pre><code>openstack object create --name my_test_file mycontainer test_file.txt\n+--------------+-------------+----------------------------------+\n| object       | container   | etag                             |\n+--------------+-------------+----------------------------------+\n| my_test_file | mycontainer | e3024896943ee80422d1e5ff44423658 |\n+--------------+-------------+----------------------------------+\n</code></pre> <p>Once uploaded you can see the metadata through:</p> <pre><code>openstack object show mycontainer my_test_file\n+----------------+---------------------------------------+\n| Field          | Value                                 |\n+----------------+---------------------------------------+\n| account        | AUTH_4c5bccef73c144679d44cbc96b42df4e |\n| container      | mycontainer                           |\n| content-length | 26                                    |\n| content-type   | application/octet-stream              |\n| etag           | e3024896943ee80422d1e5ff44423658      |\n| last-modified  | Mon, 04 Apr 2022 18:27:14 GMT         |\n| object         | my_test_file                          |\n+----------------+---------------------------------------+\n</code></pre> <p>You can save the contents of the object from your container to your local machine by using:</p> <p><code>openstack object save mycontainer my_test_file --file test_file.txt</code></p> <p>Very Important</p> <p>Please note that this will overwrite the file in the local directory.</p> <p>Finally you can delete the object with the following command</p> <p><code>openstack object delete mycontainer my_test_file</code></p>"},{"location":"openstack/persistent-storage/object-storage/#delete-the-container","title":"Delete the container","text":"<p>If you want to delete the container, you can use the following command</p> <p><code>openstack container delete mycontainer</code></p> <p>If the container has some data, you can trigger the recursive option to delete the objects internally.</p> <pre><code>openstack container delete mycontainer\nConflict (HTTP 409) (Request-ID: tx6b53c2b3e52d453e973b4-00624b400f)\n</code></pre> <p>So, try to delete the container recursively using command</p> <p><code>openstack container delete --recursive mycontainer</code></p>"},{"location":"openstack/persistent-storage/object-storage/#list-existing-containers","title":"List existing containers","text":"<p>You can check the existing containers with</p> <pre><code>openstack container list\n+---------------+\n| Name          |\n+---------------+\n| mycontainer   |\n+---------------+\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#swift-quota-utilization","title":"Swift quota utilization","text":"<p>To check the overall space used, you can use the following command</p> <pre><code>openstack object store account show\n+------------+---------------------------------------+\n| Field      | Value                                 |\n+------------+---------------------------------------+\n| Account    | AUTH_4c5bccef73c144679d44cbc96b42df4e |\n| Bytes      | 665                                   |\n| Containers | 1                                     |\n| Objects    | 3                                     |\n+------------+---------------------------------------+\n</code></pre> <p>To check the space used by a specific container</p> <pre><code>openstack container show mycontainer\n+----------------+---------------------------------------+\n| Field          | Value                                 |\n+----------------+---------------------------------------+\n| account        | AUTH_4c5bccef73c144679d44cbc96b42df4e |\n| bytes_used     | 665                                   |\n| container      | mycontainer                           |\n| object_count   | 3                                     |\n| read_acl       | .r:*,.rlistings                       |\n| storage_policy | Policy-0                              |\n+----------------+---------------------------------------+\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#ii-swift-interface","title":"ii. Swift Interface","text":"<p>This is a python client for the Swift API. There's a Python API (the <code>swiftclient</code> module), and a command-line script (<code>swift</code>).</p> <ul> <li>This example uses a <code>Python3</code> virtual environment, but you are free to choose     any other method to create a local virtual environment like <code>Conda</code>.<pre><code>python3 -m venv venv\n</code></pre> </li> </ul> <p>Choosing Correct Python Interpreter</p> <p>Make sure you are able to use <code>python</code> or <code>python3</code> or <code>py -3</code> (For Windows Only) to create a directory named <code>venv</code> (or whatever name you specified) in your current working directory.</p> <ul> <li>Activate the virtual environment by running:</li> </ul> <p>on Linux/Mac: <code>source venv/bin/activate</code></p> <p>on Windows: <code>venv\\Scripts\\activate</code></p>"},{"location":"openstack/persistent-storage/object-storage/#install-python-swift-client-page-at-pypi","title":"Install Python Swift Client page at PyPi","text":"<ul> <li>Once virtual environment is activated, install <code>python-swiftclient</code> and <code>python-keystoneclient</code></li> </ul> <p>pip install python-swiftclient python-keystoneclient</p> <ul> <li>Swift authenticates using a user, tenant, and key, which map to your OpenStack     username, project,and password.</li> </ul> <p>For this, you need to download the \"NERC's OpenStack RC File\" with the credentials for your NERC project from the NERC's OpenStack dashboard. Then you need to source that RC file using: <code>source *-openrc.sh</code>. You can read here on how to do this.</p> <p>By sourcing the \"NERC's OpenStack RC File\", you will set the all required environmental variables.</p>"},{"location":"openstack/persistent-storage/object-storage/#check-your-authentication-variables","title":"Check your authentication variables","text":"<p>Check what the swift client will use as authentication variables:</p> <pre><code>swift auth\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#create-your-first-container","title":"Create your first container","text":"<p>Lets create your first container by using the following command:</p> <pre><code>swift post &lt;container_name&gt;\n</code></pre> <p>For example:</p> <pre><code>swift post unique-container-test\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#upload-files","title":"Upload files","text":"<p>Upload a file to your container:</p> <pre><code>swift upload &lt;container_name&gt; &lt;file_or_folder&gt;\n</code></pre> <p>To upload a file to the above listed i.e. <code>unique-container-test</code>, you can run the following command:</p> <pre><code>swift upload unique-container-test ./README.md\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#show-containers","title":"Show containers","text":"<p>Then type the following command to get list of your containers:</p> <pre><code>swift list\n</code></pre> <p>This will output your existing container on your project, for e.g. <code>unique-container-test</code></p> <p>Show objects inside your container:</p> <pre><code>swift list &lt;container_name&gt;.\n</code></pre> <p>For example:</p> <pre><code>swift list unique-container-test\nREADME.md\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#show-statistics-of-your-containers-and-objects","title":"Show statistics of your containers and objects","text":"<p>You can see statistics, ranging from specific objects to the entire account. Use the following command to se statistics of the specific container.</p> <pre><code>swift stat &lt;container_name&gt;\n</code></pre> <p>You can also use <code>swift stat &lt;container_name&gt; &lt;filename&gt;</code> to check stats of individual files.</p> <p>If you want to see stats from your whole account, you can type:</p> <pre><code>swift stat\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#download-objects","title":"Download objects","text":"<p>You can download single objects by using the following command:</p> <pre><code>swift download &lt;container_name&gt; &lt;your_object&gt; -o /path/to/local/&lt;your_object&gt;\n</code></pre> <p>For example:</p> <pre><code>swift download unique-container-test README.md -o ./README.md\nREADME.md [auth 2.763s, headers 2.907s, total 2.907s, 0.000 MB/s]\n</code></pre> <p>It's possible to test downloading an object/container without actually downloading, for testing purposes:</p> <pre><code>swift download &lt;container-name&gt; --no-download\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#download-all-objects-from-specific-container","title":"Download all objects from specific container","text":"<pre><code>swift download &lt;container_name&gt; -D &lt;/path/to/folder/&gt;\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#download-all-objects-from-your-account","title":"Download all objects from your account","text":"<pre><code>swift download --all -D &lt;/path/to/folder/&gt;\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#delete-objects","title":"Delete objects","text":"<p>Delete specific object by issuing the following command:</p> <pre><code>swift delete &lt;container_name&gt; &lt;object_name&gt;\n</code></pre> <p>For example:</p> <pre><code>swift delete unique-container-test README.md\nREADME.md\n</code></pre> <p>And finally delete specific container by typing the following:</p> <pre><code>swift delete &lt;container_name&gt;\n</code></pre> <p>For example:</p> <pre><code>swift delete unique-container-test\n</code></pre> <p>Other helpful Swift commands:</p> <pre><code>delete               Delete a container or objects within a container.\ndownload             Download objects from containers.\nlist                 Lists the containers for the account or the objects\n                    for a container.\npost                 Updates meta information for the account, container,\n                    or object; creates containers if not present.\ncopy                 Copies object, optionally adds meta\nstat                 Displays information for the account, container,\n                    or object.\nupload               Uploads files or directories to the given container.\ncapabilities         List cluster capabilities.\ntempurl              Create a temporary URL.\nauth                 Display auth related environment variables.\nbash_completion      Outputs option and flag cli data ready for\n                    bash_completion.\n</code></pre> <p>Helpful Tip</p> <p>Type <code>swift -h</code> to learn more about using the swift commands. The client has a <code>--debug</code>flag, which can be useful if you are facing any issues.</p>"},{"location":"openstack/persistent-storage/object-storage/#iii-using-aws-cli","title":"iii. Using AWS CLI","text":"<p>The Ceph Object Gateway supports basic operations through the Amazon S3 interface.</p> <p>You can use both high-level (s3) commands with the AWS CLI and API-Level (s3api) commands with the AWS CLI to access object storage on your NERC project.</p> <p>Prerequisites:</p> <p>To run the <code>s3</code> or <code>s3api</code> commands, you need to have:</p> <ul> <li> <p>AWS CLI installed, see     Installing or updating the latest version of the AWS CLI     for more information.</p> </li> <li> <p>The NERC's Swift Endpoint URL: <code>https://stack.nerc.mghpcc.org:13808</code>.</p> </li> </ul> <p>Very Important: Endpoint for S3 API for the NERC Object Storage/ Swift</p> <p>The default endpoint for S3 API for the NERC Object Storage/ Swift is <code>https://stack.nerc.mghpcc.org:13808</code>.</p> <p>Understand these Amazon S3 terms</p> <p>i. Bucket \u2013 A top-level Amazon S3 folder.</p> <p>ii. Prefix \u2013 An Amazon S3 folder in a bucket.</p> <p>iii. Object \u2013 Any item that's hosted in an Amazon S3 bucket.</p>"},{"location":"openstack/persistent-storage/object-storage/#configuring-the-aws-cli","title":"Configuring the AWS CLI","text":"<p>To access this interface, you must login through the OpenStack Dashboard and navigate to \"Projects &gt; API Access\" where you can download the \"Download OpenStack RC File\" as well as the \"EC2 Credentials\".</p> <p></p> <p>While clicking on \"EC2 Credentials\", this will download a file zip file including <code>ec2rc.sh</code> file that has content similar to shown below. The important parts are <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code>, keep them noted.</p> <pre><code>#!/bin/bash\n\nNOVARC=$(readlink -f \"${BASH_SOURCE:-${0}}\" 2&gt;/dev/null) || NOVARC=$(python -c 'import os,sys; print os.path.abspath(os.path.realpath(sys.argv[1]))' \"${BASH_SOURCE:-${0}}\")\nNOVA_KEY_DIR=${NOVARC%/*}\nexport EC2_ACCESS_KEY=...\nexport EC2_SECRET_KEY=...\nexport EC2_URL=https://localhost/notimplemented\nexport EC2_USER_ID=42 # nova does not use user id, but bundling requires it\nexport EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem\nexport EC2_CERT=${NOVA_KEY_DIR}/cert.pem\nexport NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem\nexport EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set\n\nalias ec2-bundle-image=\"ec2-bundle-image --cert ${EC2_CERT} --privatekey ${EC2_PRIVATE_KEY} --user 42 --ec2cert ${NOVA_CERT}\"\nalias ec2-upload-bundle=\"ec2-upload-bundle -a ${EC2_ACCESS_KEY} -s ${EC2_SECRET_KEY} --url ${S3_URL} --ec2cert ${NOVA_CERT}\"\n</code></pre> <p>Alternatively, you can obtain your EC2 access keys using the openstack client:</p> <pre><code>sudo apt install python3-openstackclient\n\nopenstack ec2 credentials list\n+------------------+------------------+--------------+-----------+\n| Access           | Secret           | Project ID   | User ID   |\n+------------------+------------------+--------------+-----------+\n| &lt;EC2_ACCESS_KEY&gt; | &lt;EC2_SECRET_KEY&gt; | &lt;Project_ID&gt; | &lt;User_ID&gt; |\n+------------------+------------------+--------------+-----------+\n</code></pre> <p>OR, you can even create a new one by running:</p> <pre><code>openstack ec2 credentials create\n</code></pre> <ul> <li>Source the downloaded OpenStack RC File from Projects &gt; API Access by using:     <code>source *-openrc.sh</code> command. Sourcing the RC File will set the required environment     variables.</li> </ul> <p>Then run aws configuration command which requires the <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file (during the \"Configuring the AWS CLI\" step):</p> <pre><code>  $&gt; aws configure --profile \"'${OS_PROJECT_NAME}'\"\n  AWS Access Key ID [None]: &lt;EC2_ACCESS_KEY&gt;\n  AWS Secret Access Key [None]: &lt;EC2_SECRET_KEY&gt;\n  Default region name [None]:\n  Default output format [None]:\n</code></pre> <p>This will create the configuration file for AWS cli in your home directory <code>~/.aws/config</code> with the EC2 profile based on your <code>${OS_PROJECT_NAME}</code> and <code>~/.aws/credentials</code> credentials with Access and Secret keys that you provided above.</p> <p>The EC2 profile is stored here:</p> <pre><code>  cat ~/.aws/config\n\n  [profile ''\"'\"'${OS_PROJECT_NAME}'\"'\"'']\n</code></pre> <p>Where as Credentials are store here:</p> <pre><code>  cat ~/.aws/credentials\n\n  ['${OS_PROJECT_NAME}']\n  aws_access_key_id = &lt;EC2_ACCESS_KEY&gt;\n  aws_secret_access_key = &lt;EC2_SECRET_KEY&gt;\n</code></pre> <p>Then you can manually create the configuration file for AWS cli in your home directory <code>~/.aws/config</code> with the ec2 profile and credentials as shown below:</p> <pre><code>  cat ~/.aws/config\n\n  ['${OS_PROJECT_NAME}']\n  aws_access_key_id = &lt;EC2_ACCESS_KEY&gt;\n  aws_secret_access_key = &lt;EC2_SECRET_KEY&gt;\n</code></pre> <p>Information</p> <p>We need to have a profile that you use must have permissions to allow   the AWS operations can be performed.</p>"},{"location":"openstack/persistent-storage/object-storage/#listing-buckets-using-aws-cli","title":"Listing buckets using aws-cli","text":"<p>i. Using <code>s3api</code>:</p> <pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3api list-buckets\n\n{\n    \"Buckets\": [\n        {\n            \"Name\": \"unique-container-test\",\n            \"CreationDate\": \"2009-02-03T16:45:09+00:00\"\n        }\n    ],\n    \"Owner\": {\n        \"DisplayName\": \"Test Project-f69dcff:mmunakami@fas.harvard.edu\",\n        \"ID\": \"Test Project-f69dcff:mmunakami@fas.harvard.edu\"\n    }\n}\n</code></pre> <p>ii. Alternatively, you can do the same using <code>s3</code>:</p> <pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 ls\n</code></pre> <p>Output:</p> <pre><code>2009-02-03 11:45:09 unique-container-test\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-list-contents-inside-bucket","title":"To list contents inside bucket","text":"<pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 ls s3://&lt;your-bucket&gt;\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-make-a-bucket","title":"To make a bucket","text":"<pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 mb s3://&lt;your-bucket&gt;\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#adding-copying-files-from-one-container-to-another-container","title":"Adding/ Copying files from one container to another container","text":"<ol> <li> <p>Single file copy using <code>cp</code> command:</p> <p>The aws tool provides a <code>cp</code> command to move files to your <code>s3</code> bucket:</p> <pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 cp &lt;Your-file&gt; s3://&lt;your-bucket&gt;/\n</code></pre> <p>Output:</p> <pre><code>upload: .\\&lt;Your-file&gt; to s3://&lt;your-bucket&gt;/&lt;Your-file&gt;\n</code></pre> </li> <li> <p>Whole directory copy using the <code>--recursive</code> flag:</p> <pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 cp &lt;Your-directory&gt; s3://&lt;your-bucket&gt;/ --recursive\n</code></pre> <p>Output:</p> <pre><code>upload: &lt;your-directory&gt;/&lt;file0&gt; to s3://&lt;your-bucket&gt;/&lt;file0&gt;\nupload: &lt;your-directory&gt;/&lt;file1&gt; to s3://&lt;your-bucket&gt;/&lt;file1&gt;\n...\nupload: &lt;your-directory&gt;/&lt;fileN&gt; to s3://&lt;your-bucket&gt;/&lt;fileN&gt;\n</code></pre> </li> </ol> <p>You can then use aws <code>s3 ls</code> to check that your files have been properly uploaded:</p> <pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 ls s3://&lt;your-bucket&gt;/\n</code></pre> <p>Output:</p> <pre><code>2022-04-04 16:32:38          &lt;size&gt; &lt;file0&gt;\n2022-04-04 16:32:38          &lt;size&gt; &lt;file1&gt;\n...\n2022-04-04 16:25:50          &lt;size&gt; &lt;fileN&gt;\n</code></pre> <p>Other Useful Flags</p> <p>Additionally, <code>aws cp</code> provides an <code>--exclude</code> flag to filter files not to be transferred, the syntax is: <code>--exclude \"&lt;regex&gt;\"</code></p>"},{"location":"openstack/persistent-storage/object-storage/#to-delete-an-object-from-a-bucket","title":"To delete an object from a bucket","text":"<pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 rm s3://&lt;your-bucket&gt;/argparse-1.2.1.tar.gz\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-remove-a-bucket","title":"To remove a bucket","text":"<pre><code>aws --profile \"'${OS_PROJECT_NAME}'\" --endpoint-url=https://stack.nerc.mghpcc.org:13808 \\\n    s3 rb s3://&lt;your-bucket&gt;\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#iv-using-s3cmd","title":"iv. Using s3cmd","text":"<p><code>S3cmd</code> is a free command-line tool and client for uploading, retrieving and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol.</p> <p>Prerequisites:</p> <ul> <li>S3cmd installed, see Download and Install the latest version of the S3cmd     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/object-storage/#configuring-s3cmd","title":"Configuring s3cmd","text":"<p>The <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file can then be plugged into <code>s3cfg</code> config file.</p> <p>The <code>.s3cfg</code> file requires the following configuration to work with our Object storage service:</p> <pre><code># Setup endpoint\nhost_base = stack.nerc.mghpcc.org:13808\nhost_bucket = stack.nerc.mghpcc.org:13808\nuse_https = True\n\n# Setup access keys\naccess_key = &lt;YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE&gt;\nsecret_key = &lt;YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE&gt;\n\n# Enable S3 v4 signature APIs\nsignature_v2 = False\n</code></pre> <p>We are assuming that the configuration file is placed in default location i.e. <code>$HOME/.s3cfg</code>. If it is not the case you need to add the parameter <code>--config=FILE</code> with the location of your configuration file to override the config location.</p>"},{"location":"openstack/persistent-storage/object-storage/#using-s3cmd","title":"Using s3cmd","text":""},{"location":"openstack/persistent-storage/object-storage/#to-list-buckets","title":"To list buckets","text":"<p>Use the following command to list all s3 buckets</p> <pre><code>s3cmd ls\n</code></pre> <p>Or,</p> <pre><code>s3cmd ls s3://\n\n2009-02-03 16:45  s3://nerc-test-container\n2009-02-03 16:45  s3://second-mycontainer\n2009-02-03 16:45  s3://unique-container-test\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#create-a-new-bucket","title":"Create a new bucket","text":"<p>In order to create a bucket, you can use <code>s3cmd</code> with the following command</p> <pre><code>s3cmd mb s3://mybucket\n\nBucket 's3://mybucket/' created\n\ns3cmd ls\n2009-02-03 16:45  s3://mybucket\n\n2009-02-03 16:45  s3://nerc-test-container\n2009-02-03 16:45  s3://second-mycontainer\n2009-02-03 16:45  s3://unique-container-test\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-copy-an-object-to-bucket","title":"To copy an object to bucket","text":"<p>Below command will upload file <code>file.txt</code> to the bucket using <code>s3cmd</code> command.</p> <pre><code>s3cmd put ~/file.txt s3://mybucket/\n\nupload: 'file.txt' -&gt; 's3://mybucket/file.txt'  [1 of 1]\n0 of 0     0% in    0s     0.00 B/s  done\n</code></pre> <p><code>s3cmd</code> also allows to set additional properties to the objects stored. In the example below, we set the content type with the <code>--mime-type</code> option and the cache-control parameter to 1 hour with <code>--add-header</code>.</p> <pre><code>s3cmd put --mime-type='application/json' --add-header='Cache-Control: max-age=3600' ~/file.txt s3://mybucket\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#uploading-directory-in-bucket","title":"Uploading Directory in bucket","text":"<p>If we need to upload entire directory use <code>-r</code> to upload it recursively as below.</p> <pre><code>s3cmd put -r &lt;your-directory&gt; s3://mybucket/\n\nupload: 'backup/hello.txt' -&gt; 's3://mybucket/backup/hello.txt'  [1 of 1]\n0 of 0     0% in    0s     0.00 B/s  done\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#list-the-objects-of-bucket","title":"List the objects of bucket","text":"<p>List the objects of the bucket using <code>ls</code> switch with s3cmd.</p> <pre><code>s3cmd ls s3://mybucket/\n\n                       DIR   s3://mybucket/backup/\n2022-04-05 03:10         0   s3://mybucket/file.txt\n2022-04-05 03:14         0   s3://mybucket/hello.txt\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-copy-download-an-object-to-local-system","title":"To copy/ download an object to local system","text":"<p>Use the following command to download files from the bucket:</p> <pre><code>s3cmd get s3://mybucket/file.txt\n\ndownload: 's3://mybucket/file.txt' -&gt; './file.txt'  [1 of 1]\n0 of 0     0% in    0s     0.00 B/s  done\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-sync-local-filedirectory-to-a-bucket","title":"To sync local file/directory to a bucket","text":"<pre><code>s3cmd sync newdemo s3://mybucket\n\nupload: 'newdemo/newdemo_file.txt' -&gt; 's3://mybucket/newdemo/newdemo_file.txt'  [1 of 1]\n0 of 0     0% in    0s     0.00 B/s  done\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-sync-bucket-or-object-with-local-filesystem","title":"To sync bucket or object with local filesystem","text":"<pre><code>s3cmd sync  s3://unique-container-test otherlocalbucket\n\ndownload: 's3://unique-container-test/README.md' -&gt; 'otherlocalbucket/README.md'  [1 of 3]\n653 of 653   100% in    0s     4.54 kB/s  done\ndownload: 's3://unique-container-test/image.png' -&gt; 'otherlocalbucket/image.png'  [2 of 3]\n0 of 0     0% in    0s     0.00 B/s  done\ndownload: 's3://unique-container-test/test-file' -&gt; 'otherlocalbucket/test-file'  [3 of 3]\n12 of 12   100% in    0s    83.83 B/s  done\nDone. Downloaded 665 bytes in 1.0 seconds, 665.00 B/s.\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-delete-an-object-from-bucket","title":"To delete an object from bucket","text":"<p>You can delete files from the bucket with the following <code>s3cmd</code> command</p> <pre><code>s3cmd del s3://unique-container-test/README.md\n\ndelete: 's3://unique-container-test/README.md'\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-delete-directory-from-bucket","title":"To delete directory from bucket","text":"<pre><code>s3cmd del s3://mybucket/newdemo\n\ndelete: 's3://mybucket/newdemo'\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#to-delete-a-bucket","title":"To delete a bucket","text":"<pre><code>s3cmd rb s3://mybucket\n\nERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not empty\n</code></pre> <p>Important Information</p> <p>The above command failed because of the bucket was not empty! You can remove all objects inside the bucket and then use the command again. Or, you can run the following command with <code>-r</code> or <code>--recursive</code> flag i.e. <code>s3cmd rb s3://mybucket -r</code> or <code>s3cmd rb s3://mybucket --recursive</code>.</p>"},{"location":"openstack/persistent-storage/object-storage/#v-using-rclone","title":"v. Using Rclone","text":"<p><code>rclone</code> is a convenient and performant command-line tool for transferring files and synchronizing directories directly between your local file systems and the NERC's containers.</p> <p>Prerequisites:</p> <p>To run the <code>rclone</code> commands, you need to have:</p> <ul> <li><code>rclone</code> installed, see     Downloading and Installing the latest version of the Rclone     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/object-storage/#configuring-rclone","title":"Configuring Rclone","text":"<p>First, you'll need to configure <code>rclone</code>. As the object storage systems have quite complicated authentication these are kept in a config file.</p> <p>If you run <code>rclone config file</code> you will see where the default location is for you.</p> <p>Note</p> <p>For Windows users, you many need to specify the full path to the Rclone executable file, if its not included in your systems PATH variable.</p> <p>The <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file can then be plugged into <code>rclone</code> config file.</p> <p>Edit the config file's content on the path location described by <code>rclone config file</code> command and add the following entry with the name [nerc]:</p> <pre><code>[nerc]\ntype = s3\nenv_auth = false\nprovider = Other\nendpoint = https://stack.nerc.mghpcc.org:13808\nacl = public-read\naccess_key_id = &lt;YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE&gt;\nsecret_access_key = &lt;YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE&gt;\nlocation_constraint =\nserver_side_encryption =\n</code></pre> <p>More about the config for AWS S3 compatible API can be seen here.</p> <p>Important Information</p> <p>Mind that if set <code>env_auth = true</code> then it will take variables from environment, so you shouldn't insert it in this case.</p> <p>OR, You can locally copy this content to a new config file and then use this flag to override the config location, e.g. <code>rclone --config=FILE</code></p> <p>Interactive Configuration</p> <p>Run <code>rclone config</code> to setup. See rclone config docs for more details.</p>"},{"location":"openstack/persistent-storage/object-storage/#using-rclone","title":"Using Rclone","text":"<p><code>rclone</code> supports many subcommands (see the complete list of Rclone subcommands). A few commonly used subcommands (assuming you configured the NERC Object Storage as <code>nerc</code>):</p>"},{"location":"openstack/persistent-storage/object-storage/#listing-the-containers-and-contains-of-a-container","title":"Listing the Containers and Contains of a Container","text":"<p>Once your Object Storage has been configured in Rclone, you can then use the Rclone interface to List all the Containers with the \"lsd\" command</p> <pre><code>rclone lsd \"nerc:\"\n</code></pre> <p>Or,</p> <pre><code>rclone lsd \"nerc:\" --config=rclone.conf\n</code></pre> <p>For e.g.,</p> <pre><code>rclone lsd \"nerc:\" --config=rclone.conf\n        -1 2009-02-03 11:45:09        -1 second-mycontainer\n        -1 2009-02-03 11:45:09        -1 unique-container-test\n</code></pre> <p>To list the files and folders available within a container i.e. \"unique-container-test\" in this case, within a container we can use the \"ls\" command:</p> <pre><code>rclone ls \"nerc:unique-container-test/\"\n  653 README.md\n    0 image.png\n   12 test-file\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#uploading-and-downloading-files-and-folders","title":"Uploading and Downloading Files and Folders","text":"<p><code>rclone</code> support a variety of options to allow you to Copy, Sync and Move files from one destination to another.</p> <p>A simple example of this can be seen below, where we copy (Upload) the file \"upload.me\" to the <code>&lt;your-bucket&gt;</code> container:</p> <pre><code>rclone copy \"./upload.me\" \"nerc:&lt;your-bucket&gt;/\"\n</code></pre> <p>Another example, to copy (Download) the file \"upload.me\" from the <code>&lt;your-bucket&gt;</code> container to your local:</p> <pre><code>rclone -P copy \"nerc:&lt;your-bucket&gt;/upload.me\" \"./\"\n</code></pre> <p>Also, to Sync files into to the <code>&lt;your-bucket&gt;</code> container - try with <code>--dry-run</code> first</p> <pre><code>rclone --dry-run sync /path/to/files nerc:&lt;your-bucket&gt;\n</code></pre> <p>Then sync for real</p> <pre><code>rclone sync /path/to/files nerc:&lt;your-bucket&gt;\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#mounting-object-storage-on-local-filesystem","title":"Mounting object storage on local filesystem","text":"<p>Linux:</p> <p>First, you need to create a directory on which you will mount your filesystem:</p> <p><code>mkdir ~/mnt-rclone</code></p> <p>Then you can simply mount your object storage with:</p> <p><code>rclone -vv --vfs-cache-mode writes mount nerc: ~/mnt-rclone</code></p> <p>More about using Rclone</p> <p>You can read more about Rclone Mounting here.</p> <p>Windows:</p> <p>First you have to download Winfsp:</p> <p>WinFsp is an open source Windows File System Proxy which provides a FUSE emulation layer.</p> <p>Then you can simply mount your object storage with (no need to create the directory in advance):</p> <p><code>rclone -vv --vfs-cache-mode writes mount nerc: C:/mnt-rclone</code></p> <p><code>vfs-cache-mode</code> flag enable file caching, you can use either <code>writes</code> or <code>full</code> option. For further explanation you can see official documentation.</p> <p>Now that your object storage is mounted, you can list, create and delete files in it.</p>"},{"location":"openstack/persistent-storage/object-storage/#unmount-object-storage","title":"Unmount object storage","text":"<p>To unmount, simply press <code>CTRL-C</code> and the mount will be interrupted.</p>"},{"location":"openstack/persistent-storage/object-storage/#vi-using-client-python-libraries","title":"vi. Using client (Python) libraries","text":"<p>a. The <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file can then be plugged into your application. See below example using the Python Boto3 library, which connects through the S3 API interface through EC2 credentials, and perform some basic operations on available buckets and file that the user has access to.</p> <pre><code>import boto3\n\n# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#bucket\ns3 = boto3.resource('s3',\n    aws_access_key_id='YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE',\n    aws_secret_access_key='YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret\n    endpoint_url='https://stack.nerc.mghpcc.org:13808',\n)\n\n# List all containers\nfor bucket in s3.buckets.all():\n    print(' -&gt;', bucket)\n\n# List all objects in a container i.e. unique-container-test is your current Container\nbucket = s3.Bucket('unique-container-test')\nfor obj in bucket.objects.all():\n    print(' -&gt;', obj)\n\n# Download an S3 object i.e. test-file a file available in your unique-container-test Container\ns3.Bucket('unique-container-test').download_file('test-file', './test-file.txt')\n\n# Add an image to the bucket\n# bucket.put_object(Body=open('image.png', mode='rb'), Key='image.png')\n</code></pre> <p>We can configure the Python Boto3 library, to work with the saved aws profile.</p> <pre><code>import boto3\n\n# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html\nsession = boto3.Session(profile_name='&lt;YOUR_CONFIGURED_AWS_PROFILE_NAME&gt;')\n\n# List all containers\ns3 = boto3.client('s3', endpoint_url='https://stack.nerc.mghpcc.org:13808',)\nresponse = s3.list_buckets()\n\nfor bucket in response['Buckets']:\n    print(' -&gt;', bucket)\n</code></pre> <p>b. The <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file can then be plugged into your application. See below example using the Python Minio library, which connects through the S3 API interface through EC2 credentials, and perform some basic operations on available buckets and file that the user has access to.</p> <pre><code>from minio import Minio\n\n# Create client with access key and secret key.\n# https://docs.min.io/docs/python-client-api-reference.html\nclient = Minio(\n    \"stack.nerc.mghpcc.org:13808\",\n    access_key='YOUR_EC2_ACCESS_KEY_FROM_ec2rc_FILE',\n    secret_key='YOUR_EC2_SECRET_KEY_FROM_ec2rc_FILE', #pragma: allowlist secret\n)\n\n# List all containers\nbuckets = client.list_buckets()\nfor bucket in buckets:\n    # print(bucket.name, bucket.creation_date)\n    print(' -&gt;', bucket)\n\n# Make 'nerc-test-container' container if not exist.\nfound = client.bucket_exists(\"nerc-test-container\")\nif not found:\n    client.make_bucket(\"nerc-test-container\")\nelse:\n    print(\"Bucket 'nerc-test-container' already exists\")\n\n# Upload './nerc-backup.zip' as object name 'nerc-backup-2022.zip'\n# to bucket 'nerc-test-container'.\nclient.fput_object(\n    \"nerc-test-container\", \"nerc-backup-2022.zip\", \"./nerc-backup.zip\",\n)\n</code></pre>"},{"location":"openstack/persistent-storage/object-storage/#3-using-graphical-user-interface-gui-tools","title":"3. Using Graphical User Interface (GUI) Tools","text":""},{"location":"openstack/persistent-storage/object-storage/#i-using-winscp","title":"i. Using WinSCP","text":"<p>WinSCP is a popular and free open-source SFTP client, SCP client, and FTP client for Windows. Its main function is file transfer between a local and a remote computer, with some basic file management functionality using FTP, FTPS, SCP, SFTP, WebDAV or S3 file transfer protocols.</p> <p>Prerequisites:</p> <ul> <li> <p>WinSCP installed, see Download and Install the latest version of the WinSCP     for more information.</p> </li> <li> <p>Go to WinSCP menu and open \"Options &gt; Preferences\".</p> </li> <li> <p>When the \"Preferences\" dialog window appears, select \"Transfer\" in the options     on the left pane.</p> </li> <li> <p>Click on \"Edit\" button.</p> </li> <li> <p>Then, on shown popup dialog box review the \"Common options\" group, uncheck the     \"Preserve timestamp\" option as shown below:</p> </li> </ul> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#configuring-winscp","title":"Configuring WinSCP","text":"<ul> <li>Click on \"New Session\" tab button, as shown below:</li> </ul> <ul> <li>Select \"Amazon S3\" from the \"File protocol\" dropdown options as shown below:</li> </ul> <ul> <li>Provide the following required endpoint information:</li> </ul> <p>\"Host name\": \"stack.nerc.mghpcc.org\"</p> <p>\"Port number\": \"13808\"</p> <p>The <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file can then be plugged into \"Access key ID\" and \"Secret access key\" respectively.</p> <p></p> <p>Helpful Tips</p> <p>You can save your above configured session with a preferred name by clicking the \"Save\" button and then giving a proper name to your session. So that next time you don't need to again manually enter all your configuration.</p>"},{"location":"openstack/persistent-storage/object-storage/#using-winscp","title":"Using WinSCP","text":"<p>You can follow above step to manually add a new session next time you open WinSCP or, you can connect to your previously saved session (as listed on popup dialog will show your all saved session name list) that will show up by just clicking on the session name.</p> <p>Then click \"Login\" button to connect to your NERC project's Object Storage as shown below:</p> <p></p> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#ii-using-cyberduck","title":"ii. Using Cyberduck","text":"<p>Cyberduck is a libre server and cloud storage browser for Mac and Windows. With an easy-to-use interface, connect to servers, enterprise file sharing, and cloud storage.</p> <p>Prerequisites:</p> <ul> <li>Cyberduck installed, see Download and Install the latest version of the Cyberduck     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/object-storage/#configuring-cyberduck","title":"Configuring Cyberduck","text":"<ul> <li>Click on \"Open Connection\" tab button, as shown below:</li> </ul> <ul> <li>Select \"Amazon S3\" from the dropdown options as shown below:</li> </ul> <ul> <li>Provide the following required endpoint information:</li> </ul> <p>\"Server\": \"stack.nerc.mghpcc.org\"</p> <p>\"Port\": \"13808\"</p> <p>The <code>EC2_ACCESS_KEY</code> and <code>EC2_SECRET_KEY</code> keys that you noted from <code>ec2rc.sh</code> file can then be plugged into \"Access key ID\" and \"Secret Access Key\" respectively</p> <p></p>"},{"location":"openstack/persistent-storage/object-storage/#using-cyberduck","title":"Using Cyberduck","text":"<p>Then click \"Connect\" button to connect to your NERC project's Object Storage as shown below:</p> <p></p>"},{"location":"openstack/persistent-storage/transfer-a-volume/","title":"Transfer a Volume","text":""},{"location":"openstack/persistent-storage/transfer-a-volume/#transfer-a-volume","title":"Transfer A Volume","text":"<p>You may wish to transfer a volume to a different project. Volumes are specific to a project and can only be attached to one virtual machine at a time.</p> <p>Important</p> <p>The volume to be transferred must not be attached to an instance. This can be examined by looking into \"Status\" column of the volume i.e. it need to be \"Available\" instead of \"In-use\" and \"Attached To\" column need to be empty.</p>"},{"location":"openstack/persistent-storage/transfer-a-volume/#using-horizon-dashboard","title":"Using Horizon dashboard","text":"<p>Once you're logged in to NERC's Horizon dashboard.</p> <p>Navigate to Project -&gt; Volumes -&gt; Volumes.</p> <p>Select the volume that you want to transfer and then click the dropdown next to the \"Edit volume\" and choose \"Create Transfer\".</p> <p></p> <p>Give the transfer a name.</p> <p></p> <p>You will see a screen like shown below. Be sure to capture the Transfer ID and the Authorization Key.</p> <p></p> <p>Important Note</p> <p>You can always get the transfer ID later if needed, but there is no way to retrieve the key.</p> <p>If the key is lost before the transfer is completed, you will have to cancel the pending transfer and create a new one.</p> <p>Then the volume will show the status like below:</p> <p></p> <p>Assuming you have access to the receiving project, switch to it using the Project dropdown at the top right.</p> <p>If you don't have access to the receiving project, give the transfer ID and Authorization Key to a collaborator who does, and have them complete the next steps.</p> <p>In the receiving project, go to the Volumes tab, and click \"Accept Transfer\" button, as shown below:</p> <p></p> <p>Enter the \"Transfer ID\" and the \"Authorization Key\" that were captured when the transfer was created in the previous project.</p> <p></p> <p>The volume should now appear in the Volumes list of the receiving project as shown below:</p> <p></p> <p>Important Note</p> <p>Any pending transfers can be cancelled if they are not yet accepted, but there is no way to \"undo\" a transfer once it is complete. To send the volume back to the original project, a new transfer would be required.</p>"},{"location":"openstack/persistent-storage/transfer-a-volume/#using-the-cli","title":"Using the CLI","text":"<p>Prerequisites:</p> <p>To run the OpenStack CLI commands, you need to have:</p> <ul> <li>OpenStack CLI setup, see OpenStack Command Line setup     for more information.</li> </ul>"},{"location":"openstack/persistent-storage/transfer-a-volume/#using-the-openstack-client","title":"Using the openstack client","text":"<ul> <li>Identifying volume to transfer in your source project</li> </ul> <p>openstack volume list +---------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +---------------------------+-----------+-----------+------+-------------+ | d8a5da4c-...-8b6678ce4936 | my-volume | available | 100 | | +---------------------------+-----------+-----------+------+-------------+</p> <ul> <li>Create the transfer request</li> </ul> <p>openstack volume transfer request create my-volume +------------+--------------------------------------+ | Field | Value | +------------+--------------------------------------+ | auth_key | b92d98fec2766582 | | created_at | 2024-02-04T14:30:08.362907 | | id | a16494cf-cfa0-47f6-b606-62573357922a | | name | None | | volume_id | d8a5da4c-41c8-4c2d-b57a-8b6678ce4936 | +------------+--------------------------------------+</p> <p>Pro Tip</p> <p>If your volume name includes spaces, you need to enclose them in quotes, i.e. <code>\"&lt;VOLUME_NAME_OR_ID&gt;\"</code>. For example: <code>openstack volume transfer request create \"My Volume\"</code></p> <ul> <li>The volume can be checked as in the transfer status using     <code>openstack volume transfer request list</code> as follows and the volume is in status     <code>awaiting-transfer</code> while running <code>openstack volume show &lt;VOLUME_NAME_OR_ID&gt;</code>     as shown below:</li> </ul> <p>openstack volume transfer request list +---------------------------+------+--------------------------------------+ | ID | Name | Volume | +---------------------------+------+--------------------------------------+ | a16494cf-...-62573357922a | None | d8a5da4c-41c8-4c2d-b57a-8b6678ce4936 | +---------------------------+------+--------------------------------------+</p> <p>openstack volume show my-volume +------------------------------+--------------------------------------+ | Field | Value | +------------------------------+--------------------------------------+ ... | name | my-volume | ... | status | awaiting-transfer | +------------------------------+--------------------------------------+</p> <ul> <li>The user of the destination project can authenticate and receive the authentication     key reported above. The transfer can then be initiated.</li> </ul> <p>openstack volume transfer request accept --auth-key b92d98fec2766582 a16494cf-cfa0-47f6-b606-62573357922a +-----------+--------------------------------------+ | Field | Value | +-----------+--------------------------------------+ | id | a16494cf-cfa0-47f6-b606-62573357922a | | name | None | | volume_id | d8a5da4c-41c8-4c2d-b57a-8b6678ce4936 | +-----------+--------------------------------------+</p> <ul> <li>And the results confirmed in the volume list for the destination project.</li> </ul> <p>openstack volume list +---------------------------+-----------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +---------------------------+-----------+-----------+------+-------------+ | d8a5da4c-...-8b6678ce4936 | my-volume | available | 100 | | +---------------------------+-----------+-----------+------+-------------+</p>"},{"location":"openstack/persistent-storage/volumes/","title":"Block Storage/ Volumes/ Cinder","text":""},{"location":"openstack/persistent-storage/volumes/#persistent-storage","title":"Persistent Storage","text":""},{"location":"openstack/persistent-storage/volumes/#ephemeral-disk","title":"Ephemeral disk","text":"<p>OpenStack offers two types of block storage: ephemeral storage and persistent volumes. Ephemeral storage is available only during the instance's lifespan, persisting across guest operating system reboots. However, once the instance is deleted, its associated storage is also removed. The size of ephemeral storage is determined by the virtual machine's flavor and remains constant for all virtual machines of that flavor. The service level for ephemeral storage relies on the underlying hardware.</p> <p>In its default configuration, when the instance is launched from an Image or an Instance Snapshot, the choice for utilizing persistent storage is configured by selecting the Yes option for \"Create New Volume\". Additionally, the \"Delete Volume on Instance Delete\" setting is pre-set to No as shown below:</p> <p></p> <p>If you set the \"Create New Volume\" option to No, the instance will boot from either an image or a snapshot, with the instance only being attached to an ephemeral disk. It's crucial to note that this configuration does NOT create persistent block storage in the form of a Volume, which can pose risks. Consequently, the disk of the instance won't appear in the \"Volumes\" list. To mitigate potential data loss, we strongly recommend regularly taking a snapshot of such a running ephemeral instance, referred to as an \"instance snapshot\", especially if you want to safeguard or recover important states of your instance.</p> <p>Very Important Note</p> <p>Never use Ephemeral disk if you're setting up a production-level environment. When the instance is deleted, its associated ephemeral storage is also removed.</p>"},{"location":"openstack/persistent-storage/volumes/#volumes","title":"Volumes","text":"<p>A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance.</p> <p>Unlike Ephemeral disk, Volumes are the Block Storage devices that you attach to instances to enable persistent storage. Users can attach a volume to a running instance or detach a volume and attach it to another instance at any time.</p> <p>Ownership of volumes can be transferred to another project by transferring it to another project as described here.</p> <p>Some uses for volumes:</p> <ul> <li> <p>Persistent data storage for ephemeral instances.</p> </li> <li> <p>Transfer of data between projects</p> </li> <li> <p>Bootable image where disk changes persist</p> </li> <li> <p>Mounting the disk of one instance to another for troubleshooting</p> </li> </ul>"},{"location":"openstack/persistent-storage/volumes/#how-do-you-make-your-vm-setup-and-data-persistent","title":"How do you make your VM setup and data persistent?","text":"<ul> <li>By default, when the instance is launched from an Image or an     Instance Snapshot, the choice for utilizing persistent storage is configured     by selecting the Yes option for \"Create New Volume\". It's crucial to     note that this configuration automatically creates persistent block storage     in the form of a Volume instead of using Ephemeral disk, which appears in     the \"Volumes\" list in the Horizon dashboard: Project -&gt; Volumes -&gt; Volumes.</li> </ul> <ul> <li>By default, the setting for \"Delete Volume on Instance Delete\" is configured     to use No. This setting ensures that the volume created during the launch     of a virtual machine remains persistent and won't be deleted alongside the     instance unless explicitly chosen as \"Yes\". Such instances boot from a     bootable volume, utilizing an existing volume listed in the     Project -&gt; Volumes -&gt; Volumes menu.</li> </ul> <p>To minimize the risk of potential data loss, we highly recommend consistently creating backups through snapshots. You can opt for a \"volume snapshot\" if you only need to capture the volume's data. However, if your VM involves extended running processes and vital in-memory data, preserving the precise VM state is essential. In such cases, we recommend regularly taking a snapshot of the entire instance, known as an \"instance snapshot\", provided you have sufficient Volume Storage quotas, specifically the \"OpenStack Volume Quota (GiB)\" allocated for your resource allocation. Please ensure that your allocation includes sufficient quota for the \"OpenStack Number of Volumes Quota\" to allow for the creation of additional volumes based on your quota attributes. Utilizing snapshots for backups is of utmost importance, particularly when safeguarding or recovering critical states and data from your instance.</p> <p>Very Important: Requested/Approved Allocated Storage Quota and Cost</p> <p>When you delete virtual machines backed by persistent volumes, the disk data is retained, continuing to consume approved storage resources for which you will still be billed. It's important to note that the Storage quotas for NERC (OpenStack) Resource Allocations, are specified by the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes. Storage cost is determined by your requested and approved allocation values to reserve storage from the total NESE storage pool.</p> <p>If you request additional storage by specifying a changed quota value for the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota (GiB)\" allocation attributes through NERC's ColdFront interface, invoicing for the extra storage will take place upon fulfillment or approval of your request, as explained in our Billing FAQs.</p> <p>Conversely, if you request a reduction in the Storage quotas by specifying a reduced quota value for the \"OpenStack Volume Quota (GiB)\" and \"OpenStack Swift Quota in Gigabytes\" allocation attributes through a change request using ColdFront, your invoicing will be adjusted accordingly when the request is submitted.</p> <p>In both scenarios, 'invoicing' refers to the accumulation of hours corresponding to the added or removed storage quantity.</p> <p>Help Regarding Billing</p> <p>Please send your questions or concerns regarding Storage and Cost by emailing us at help@nerc.mghpcc.org or, by submitting a new ticket at the NERC's Support Ticketing System.</p>"},{"location":"other-tools/","title":"Other Useful Tools","text":""},{"location":"other-tools/#kubernetes","title":"Kubernetes","text":"<ul> <li> <p>Kubernetes Overview</p> </li> <li> <p>K8s Flavors Comparision</p> </li> </ul>"},{"location":"other-tools/#i-kubernetes-development-environment","title":"i. Kubernetes Development environment","text":"<ol> <li> <p>Minikube</p> </li> <li> <p>Kind</p> </li> <li> <p>MicroK8s</p> </li> <li> <p>K3s</p> <p>4.a. K3s with High Availibility(HA) setup</p> <p>4.b. Multi-master HA K3s cluster using k3sup</p> <p>4.c. Single-Node K3s Cluster using k3d</p> <p>4.d. Multi-master K3s cluster setup using k3d</p> </li> <li> <p>k0s</p> </li> </ol>"},{"location":"other-tools/#ii-kubernetes-production-environment","title":"ii. Kubernetes Production environment","text":"<ol> <li> <p>Kubeadm</p> <p>1.a. Bootstrapping cluster with kubeadm</p> <p>1.b. Creating a HA cluster with kubeadm</p> </li> <li> <p>Kubespray</p> </li> </ol>"},{"location":"other-tools/#ci-cd-tools","title":"CI/ CD Tools","text":"<ul> <li> <p>CI/CD Overview</p> </li> <li> <p>Using Jenkins</p> <ul> <li> <p>Setup Jenkins CI/CD Pipeline</p> </li> <li> <p>GitHub to Jenkins Pipeline</p> </li> </ul> </li> <li> <p>Using Github Actions</p> <ul> <li>GitHub Actions CI/CD Pipeline</li> </ul> </li> </ul>"},{"location":"other-tools/#r","title":"R","text":"<ul> <li> <p>RStudio</p> </li> <li> <p>R Shiny</p> </li> </ul>"},{"location":"other-tools/#apache-spark","title":"Apache Spark","text":"<ul> <li>Apache Spark</li> </ul>"},{"location":"other-tools/#mlflow","title":"MLflow","text":"<ul> <li> <p>MLflow Overview</p> </li> <li> <p>MLflow Tracking Server Setup</p> </li> </ul>"},{"location":"other-tools/#setup-nfs-server-and-client","title":"Setup NFS Server and Client","text":"<ul> <li>Setup NFS Server and Client</li> </ul>"},{"location":"other-tools/CI-CD/CI-CD-pipeline/","title":"CI/CD Overview","text":""},{"location":"other-tools/CI-CD/CI-CD-pipeline/#what-is-continuous-integrationcontinuous-delivery-cicd-pipeline","title":"What is Continuous Integration/Continuous Delivery (CI/CD) Pipeline?","text":"<p>A Continuous Integration/Continuous Delivery (CI/CD) pipeline involves a series of steps that is performed in order to deliver a new version of application. CI/CD pipelines are a practice focused on improving software delivery using automation.</p>"},{"location":"other-tools/CI-CD/CI-CD-pipeline/#components-of-a-cicd-pipeline","title":"Components of a CI/CD pipeline","text":"<p>The steps that form a CI/CD pipeline are distinct subsets of tasks that are grouped into a pipeline stage. Typical pipeline stages include:</p> <ul> <li> <p>Build - The stage where the application is compiled.</p> </li> <li> <p>Test - The stage where code is tested. Automation here can save both time     and effort.</p> </li> <li> <p>Release - The stage where the application is delivered to the central repository.</p> </li> <li> <p>Deploy - In this stage code is deployed to production environment.</p> </li> <li> <p>Validation and compliance - The steps to validate a build are determined     by the needs of your organization. Image security scanning, security scanning     and code analysis of applications ensure the quality of images and written application's     code.</p> </li> </ul> <p> Figure: CI/CD Pipeline Stages</p>"},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/","title":"GitHub Actions CI/CD Pipeline","text":""},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#how-to-setup-github-actions-pipeline","title":"How to setup GitHub Actions Pipeline","text":"<p>GitHub Actions gives you the ability to create workflows to automate the deployment process to OpenShift. GitHub Actions makes it easy to automate all your CI/CD workflows.</p>"},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#terminiology","title":"Terminiology","text":""},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#workflow","title":"Workflow","text":"<p>Automation-as-code that you can set up in your repository.</p>"},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#events","title":"Events","text":"<p>30+ workflow triggers, including on schedule and from external systems.</p>"},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#actions","title":"Actions","text":"<p>Community-powered units of work that you can use as steps to create a job in a workflow.</p>"},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#deploy-an-application-to-your-nerc-openshift-project","title":"Deploy an Application to your NERC OpenShift Project","text":"<ul> <li> <p>Prerequisites:</p> <p>You must have at least one active NERC-OCP (OpenShift) type resource allocation. You can refer to this documentation on how to get allocation and request \"NERC-OCP (OpenShift)\" type resource allocations.</p> </li> </ul>"},{"location":"other-tools/CI-CD/github-actions/setup-github-actions-pipeline/#steps","title":"Steps","text":"<ol> <li> <p>Access to the NERC's OpenShift Container Platform at https://console.apps.shift.nerc.mghpcc.org     as described here.     To get access to NERC's OCP web console you need to be part of ColdFront's active     allocation.</p> </li> <li> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI     to enable <code>oc</code> commands. Refer to this user guide.</p> </li> <li> <p>Setup Github CLI on your local machine as described here     and verify you are able to run <code>gh</code> commands as shown below:</p> <p></p> </li> <li> <p>Fork the <code>simple-node-app</code> App in your own Github account:</p> <p>This application runs a simple node.js server and serves up some static routes with some static responses. This demo shows a simple container based app can easily be bootstrapped onto your NERC OpenShift project space.</p> <p>Very Important Information</p> <p>As you won't have full access to this repository, we recommend first forking the repository on your own GitHub account. So, you'll need to update all references to <code>https://github.com/nerc-project/simple-node-app.git</code> to point to your own forked repository.</p> <p>To create a fork of the example <code>simple-node-app</code> repository:</p> <ul> <li> <p>Go to https://github.com/nerc-project/simple-node-app.</p> </li> <li> <p>Click the \"Fork\" button to create a fork in your own GitHub account, e.g. \"<code>https://github.com/&lt;github_username&gt;/simple-node-app</code>\".</p> <p></p> </li> </ul> </li> <li> <p>Clone your forked simple-node-app git repository:</p> <pre><code>git clone &lt;https://github.com/&gt;&lt;github_username&gt;/simple-node-app.git\ncd simple-node-app\n</code></pre> </li> <li> <p>Run either the <code>setsecret.cmd</code> file if you are using Windows or the <code>setsecret.sh</code>     file if you are on a Linux-based machine. Once executed, verify that the GitHub     Secrets are set properly under your repository's     settings &gt;&gt; secrets and variables &gt;&gt; Actions as shown here:</p> <p></p> <p>Important Note</p> <p>If you are using the GitHub Container Registry (GHCR), you do not need to set other registry-related secrets, such as <code>IMAGE_REGISTRY_USER</code> and <code>MY_REGISTRY_PASSWORD</code>, as they are obtained directly from your repository.</p> <p>These two additional secrets are required only if you plan to use Quay.io or Dockerhub registries.</p> </li> <li> <p>Enable and Update GitHub Actions Pipeline on your own forked repo:</p> <ul> <li> <p>Enable the OpenShift Workflow in the Actions tab of in your GitHub repository.</p> <p></p> </li> <li> <p>Update the provided sample OpenShift workflow YAML file i.e. <code>openshift.yml</code>,     which is located at \"<code>https://github.com/&lt;github_username&gt;/simple-node-app/actions/workflows/openshift.yml</code>\".</p> <p>Very Important Information</p> <p>Workflow execution on OpenShift pipelines follows these steps:</p> <ol> <li>Checkout your repository</li> <li>Perform a container image build</li> <li>Push the built image to the GitHub Container Registry (GHCR) or your preferred Registry</li> <li>Log in to your NERC OpenShift cluster's project space</li> <li>Create an OpenShift app from the image and expose it to the internet</li> </ol> </li> </ul> </li> <li> <p>Edit the top-level <code>env</code> section as marked with '\ud83d\udd8a\ufe0f' in the <code>openshift.yml</code>     file if the default values are not suitable for your project.  </p> <p>Very Important Note</p> <p>In the provided sample OpenShift workflow YAML file (<code>openshift.yml</code>), uncomment the lines for your chosen registry: <code>GitHub Container Registry (GHCR)</code>, <code>quay.io</code>, or <code>docker.io</code>. By default, <code>GHCR</code> is used as the appropriate lines are already uncommented.</p> </li> <li> <p>(Optional) Edit the build-image step to build your project:</p> <p>The default build type uses a Dockerfile at the root of the repository, but can be replaced with a different file, a source-to-image build, or a step-by-step buildah build.</p> </li> <li> <p>Commit and push the workflow file to your default branch to trigger a workflow     run as shown below:</p> <p></p> <p>Troubleshooting</p> <p>Repositories on <code>GitHub Container Registry (GHCR)</code>, <code>quay.io</code>, and <code>docker.io</code> are private by default. This means that when you push an image for the first time, you must change its visibility to Public for the pipeline to work successfully.</p> </li> <li> <p>Verify that you can see the newly deployed application on the NERC's OpenShift     Container Platform at https://console.apps.shift.nerc.mghpcc.org     as described here,     and ensure that it can be browsed properly.</p> <p></p> </li> </ol> <p>That's it! Every time you commit changes to your GitHub repo, GitHub Actions will trigger your configured Pipeline, which will ultimately deploy your application to your own NERC OpenShift Project.</p> <p></p>"},{"location":"other-tools/CI-CD/jenkins/integrate-your-GitHub-repository/","title":"GitHub to Jenkins Pipeline","text":""},{"location":"other-tools/CI-CD/jenkins/integrate-your-GitHub-repository/#how-to-integrate-your-github-repository-to-your-jenkins-project","title":"How to Integrate Your GitHub Repository to Your Jenkins Project","text":"<p>This explains how to add a GitHub Webhook in your Jenkins Pipeline that saves your time and keeps your project updated all the time.</p> <p>Prerequisite</p> <p>You need to have setup CI/CD Pipelines on NERC's OpenStack by following this document.</p>"},{"location":"other-tools/CI-CD/jenkins/integrate-your-GitHub-repository/#what-is-a-webhook","title":"What is a webhook?","text":"<p>A webhook is an HTTP callback, an HTTP POST that occurs when something happens through a simple event-notification via HTTP POST. Github provides its own webhooks options for such tasks.</p>"},{"location":"other-tools/CI-CD/jenkins/integrate-your-GitHub-repository/#configuring-github","title":"Configuring GitHub","text":"<p>Let's see how to configure and add a webhook in GitHub:</p> <ol> <li> <p>Go to your forked GitHub project repository.</p> </li> <li> <p>Click on \"Settings\". in the right corner as shown below:</p> <p></p> </li> <li> <p>Click on \"Webhooks\" and then \"Click \"Add webhooks\".</p> <p></p> </li> <li> <p>In the \"Payload URL\" field paste your Jenkins environment URL. At the end of this    URL add /github-webhook/ using <code>http://&lt;Floating-IP&gt;:8080/github-webhook/</code>    i.e. <code>http://199.94.60.4:8080/github-webhook/</code>.    Select \"Content type\" as \"application/json\" and leave the \"Secret\" field empty.</p> <p></p> </li> <li> <p>In the page \"Which events would you like to trigger this webhook?\" select the    option \"Let me select individual events\". Then, check \"Pull Requests\" and \"Pushes\".    At the end of this option, make sure that the \"Active\" option is checked and    then click on \"Add webhook\" button.</p> <p></p> </li> </ol> <p>We're done with the configuration on GitHub's side! Now let's config on Jenkins side to use this webhook.</p> <p>That's it! in this way we can add a webhook to our job and ensure that everytime you commits your changes to your Github repo, GitHub will trigger your new Jenkins job. As we already had setup \"Github hook tirgger for GITScm polling\" for our Jenkins pipeline setup previously.</p>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/","title":"Setup Jenkins CI/CD Pipeline","text":""},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#how-to-set-up-jenkins-pipeline-on-a-vm","title":"How to Set Up Jenkins Pipeline on a VM","text":"<p>This document will walk you through how to setup a minimal \"CI/CD Pipeline To Deploy To Kubernetes Cluster Using a CI/CD tool called Jenkins\" on your NERC's OpenStack environment. Jenkins uses the Kubernetes control plane on K8s Cluster to run pipeline tasks that enable DevOps to spend more time coding and testing and less time troubleshooting.</p> <p>Prerequisite</p> <p>You need Kubernetes cluster running in your OpenStack environment. To setup your K8s cluster please Read this.</p> <p> Figure: CI/CD Pipeline To Deploy To Kubernetes Cluster Using Jenkins on NERC</p>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#setup-a-jenkins-server-vm","title":"Setup a Jenkins Server VM","text":"<ul> <li> <p>Launch 1 Linux machine based on <code>ubuntu-20.04-x86_64</code> and <code>cpu-su.2</code> flavor with     2vCPU, 8GB RAM, and 20GB storage.</p> </li> <li> <p>Make sure you have added rules in the     Security Groups     to allow ssh using Port 22 access to the instance.</p> </li> <li> <p>Setup a new Security Group with the following rules exposing port 8080 and     attach it to your new instance.</p> <p></p> </li> <li> <p>Assign a Floating IP     to your new instance so that you will be able to ssh into this machine:</p> <pre><code>ssh ubuntu@&lt;Floating-IP&gt; -A -i &lt;Path_To_Your_Private_Key&gt;\n</code></pre> <p>For example:</p> <pre><code>ssh ubuntu@199.94.60.4 -A -i cloud.key\n</code></pre> </li> </ul> <p>Upon successfully SSH accessing the machine, execute the following dependencies:</p> <p>Very Important</p> <p>Run the following steps as non-root user i.e. ubuntu.</p> <ul> <li> <p>Update the repositories and packages:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>Turn off <code>swap</code></p> <pre><code>swapoff -a\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> </li> <li> <p>Install <code>curl</code> and <code>apt-transport-https</code></p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\n</code></pre> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#download-and-install-the-latest-version-of-docker-ce","title":"Download and install the latest version of Docker CE","text":"<ul> <li> <p>Download and install Docker CE:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> </li> <li> <p>Configure the Docker daemon:</p> <pre><code>sudo usermod -aG docker $USER &amp;&amp; newgrp docker\n</code></pre> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#install-kubectl","title":"Install kubectl","text":"<p>kubectl: the command line util to talk to your cluster.</p> <ul> <li> <p>Download the Google Cloud public signing key and add key to verify releases</p> <pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo \\\n  apt-key add -\n</code></pre> </li> <li> <p>add kubernetes apt repo</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n</code></pre> </li> <li> <p>Install kubectl</p> <pre><code>sudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p><code>apt-mark hold</code> is used so that these packages will not be updated/removed automatically</p> <pre><code>sudo apt-mark hold kubectl\n</code></pre> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#install-a-jenkins-server-using-docker","title":"Install a Jenkins Server using Docker","text":"<p>To install a Jenkins server using Docker run the following command:</p> <pre><code>docker run -u 0 --privileged --name jenkins -it -d -p 8080:8080 -p 50000:50000 \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v $(which docker):/usr/bin/docker \\\n    -v $(which kubectl):/usr/bin/kubectl \\\n    -v /home/jenkins_home:/var/jenkins_home \\\n    jenkins/jenkins:latest\n</code></pre> <p>Once successfully docker run, browse to <code>http://&lt;Floating-IP&gt;:8080</code> this will show you where to get the initial Administrator password to get started i.e. <code>/var/jenkins_home/secrets/initialAdminPassword</code> as shown below:</p> <p></p> <p>The <code>/var/jenkins_home</code> in Jenkins docker container is a mounted volume to the host's <code>/home/jenkins_home</code> so you can just browse to <code>/home/jenkins_home/secrets/initialAdminPassword</code> on your ssh'ed host machine to copy the same content from <code>/var/jenkins_home/secrets/initialAdminPassword</code>.</p> <p>Initial Admin Password</p> <p>If you can't find the Admin password at <code>/var/jenkins_home/secrets/initialAdminPassword</code>, then try to locate it at its original location, i.e. <code>/home/jenkins_home/secrets/initialAdminPassword</code>.</p> <p>OR, you can run <code>docker ps</code> on worker node where you run the Jenkins server. Note the Name of the docker container and then run: <code>docker logs -f &lt;jenkins_docker_container_name&gt;</code>. This will show the initial Administrator password on the terminal which you can copy and paste on the web GUI on the browser.</p> <p>Initial Admin Password</p> <p>When you run <code>docker logs -f &lt;jenkins_docker_container_name&gt;</code>, the initial password for the \"Admin\" user can be found between the rows of asterisks as shown below: </p> <ul> <li> <p>Once you login to the Jenkins Web UI by entering the admin password shown on     CLI terminal, click on the \"Install suggested plugins\" button, as shown below:</p> <p></p> <p></p> <p>Continue by selecting 'Skip and continue as admin' first as shown below:</p> <p></p> <p>Then click the 'Save and Finish' button as shown below and then, Jenkins is ready to use.</p> <p></p> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#install-the-required-plugins","title":"Install the required Plugins","text":"<ul> <li> <p>Jenkins has a wide range of plugin options. From your Jenkins dashboard navigate     to \"Manage Jenkins &gt; Manage Plugins\" as shown below:</p> <p></p> <p>Select the \"Available\" tab and then locate Docker pipeline by searching and then click \"Install without restart\" button, as shown below:</p> <p></p> <p>Also, install the Kubernetes CLI plugin that allows you to configure <code>kubectl</code> commands on Jenkinsfile to interact with Kubernetes clusters as shown below:</p> <p></p> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#create-the-required-credentials","title":"Create the required Credentials","text":"<ul> <li> <p>Create a global credential for your Docker Hub Registry by providing the username     and password that will be used by the Jenkins pipelines:</p> <ol> <li> <p>Click on the \"Manage Jenkins\" menu and then click on the \"Manage Credentials\"    link as shown below:</p> <p></p> </li> <li> <p>Click on Jenkins Store as shown below:</p> <p></p> </li> <li> <p>The credentials can be added by clicking the 'Add Credentials' button in    the left pane.</p> <p></p> </li> </ol> </li> <li> <p>First, add the 'DockerHub' credentials as 'Username with password' with the     ID <code>dockerhublogin</code>.</p> <p>a. Select the Kind \"Username with password\" from the dropdown options.</p> <p>b. Provide your Docker Hub Registry's username and password.</p> <p>c. Give its ID and short description. ID is very important is that will need to be specify as used on your Jenkinsfile i.e. <code>dockerhublogin</code>.</p> <p></p> </li> <li> <p>Config the 'Kubeconfig' credentials as 'Secret file' that holds Kubeconfig     file from K8s master i.e. located at <code>/etc/kubernetes/admin.conf</code> with the ID     'kubernetes'</p> <p>a. Click on the \"Add Credentials\" button in the left pane.</p> <p>b. Select the Kind \"Secret file\" from the dropdown options.</p> <p>c. On File section choose the config file that contains the EXACT content from your K8s master's kubeconfig file located at: <code>/etc/kubernetes/admin.conf</code></p> <p>d. Give a ID and description that you will need to use on your Jenkinsfile i.e. <code>kubernetes</code>.</p> <p></p> <p>e. Once both credentials are successfully added the following credentials are shown:</p> <p></p> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#fork-the-nodeapp-app-in-your-own-github","title":"Fork the <code>nodeapp</code> App in your own Github","text":"<p>Very Important Information</p> <p>As you won't have full access to this repository, we recommend first forking the repository on your own GitHub account. So, you'll need to update all references to <code>https://github.com/nerc-project/nodeapp.git</code> to point to your own forked repository.</p> <p>To create a fork of the example <code>nodeapp</code> repository:</p> <ol> <li> <p>Go to https://github.com/nerc-project/nodeapp.</p> </li> <li> <p>Click the \"Fork\" button to create a fork in your own GitHub account, e.g. \"<code>https://github.com/&lt;github_username&gt;/nodeapp</code>\".</p> <p></p> </li> <li> <p>Review the \"Jenkinsfile\" that is included at the root of the forked git repo.</p> <p>Very Important Information</p> <p>A sample Jenkinsfile is available at the root of our demo application's Git repository, which we can reference in our Jenkins pipeline steps. For example, in this case, we are using this repository where our demo Node.js application resides.</p> </li> </ol>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#modify-the-jenkins-declarative-pipeline-script-file","title":"Modify the Jenkins Declarative Pipeline Script file","text":"<ul> <li> <p>Modify the provided 'Jenkinsfile' to specify your own Docker Hub account     and github repository as specified in \"<code>&lt;dockerhub_username&gt;</code>\" and \"<code>&lt;github_username&gt;</code>\".</p> <p>Very Important Information</p> <p>You need to replace \"<code>&lt;dockerhub_username&gt;</code>\" and \"<code>&lt;github_username&gt;</code>\" with your actual DockerHub and GitHub usernames, respectively. Also, ensure that the global credentials IDs mentioned above match those used during the credential saving steps mentioned earlier. For instance, <code>dockerhublogin</code> corresponds to the DockerHub ID saved during the credential saving process for your Docker Hub Registry's username and password. Similarly, <code>kubernetes</code> corresponds to the 'Kubeconfig' ID assigned for the Kubeconfig credential file.</p> </li> <li> <p>Below is an example of a Jenkins declarative Pipeline Script file:</p> <p>pipeline {</p> <pre><code>environment {\n  dockerimagename = \"&lt;dockerhub_username&gt;/nodeapp:${env.BUILD_NUMBER}\"\n  dockerImage = \"\"\n}\n\nagent any\n\nstages {\n\n  stage('Checkout Source') {\n    steps {\n      git branch: 'main', url: 'https://github.com/&lt;github_username&gt;/nodeapp.git'\n    }\n  }\n\n  stage('Build image') {\n    steps{\n      script {\n        dockerImage = docker.build dockerimagename\n      }\n    }\n  }\n\n  stage('Pushing Image') {\n    environment {\n      registryCredential = 'dockerhublogin'\n    }\n    steps{\n      script {\n        docker.withRegistry('https://registry.hub.docker.com', registryCredential){\n          dockerImage.push()\n        }\n      }\n    }\n  }\n\n  stage('Docker Remove Image') {\n    steps {\n      sh \"docker rmi -f ${dockerimagename}\"\n      sh \"docker rmi -f registry.hub.docker.com/${dockerimagename}\"\n    }\n  }\n\n  stage('Deploying App to Kubernetes') {\n    steps {\n      sh \"sed -i 's/nodeapp:latest/nodeapp:${env.BUILD_NUMBER}/g' deploymentservice.yml\"\n      withKubeConfig([credentialsId: 'kubernetes']) {\n        sh 'kubectl apply -f deploymentservice.yml'\n      }\n    }\n  }\n}\n</code></pre> <p>}</p> <p>Other way to Generate Pipeline Jenkinsfile</p> <p>You can generate your custom Jenkinsfile by clicking on \"Pipeline Syntax\" link shown when you create a new Pipeline when clicking the \"New Item\" menu link.</p> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#setup-a-pipeline","title":"Setup a Pipeline","text":"<ul> <li> <p>Once you review the provided Jenkinsfile and understand the stages,     you can now create a pipeline to trigger it on your newly setup Jenkins server:</p> <p>a. Click on the \"New Item\" link.</p> <p>b. Select the \"Pipeline\" link.</p> <p>c. Give name to your Pipeline i.e. \"jenkins-k8s-pipeline\"</p> <p></p> <p>d. Select \"Build Triggers\" tab and then select Github hook tirgger for GITScm polling as shown below:</p> <p></p> <p>e. Select \"Pipeline\" tab and then select the \"Pipeline script from SCM\" from the dropdown options. Then you need to specify the Git as SCM and also \"Repository URL\" for your public git repo and also specify your branch and Jenkinsfile's name as shown below:</p> <p></p> <p>OR, You can copy/paste the contents of your Jenkinsfile on the given textbox. Please make sure you are selecting the \"Pipeline script\" from the dropdown options.</p> <p></p> <p>f. Click on \"Save\" button.</p> </li> </ul>"},{"location":"other-tools/CI-CD/jenkins/setup-jenkins-CI-CD-pipeline/#how-to-manually-trigger-the-pipeline","title":"How to manually Trigger the Pipeline","text":"<ul> <li> <p>Finally, click on the \"Build Now\" menu link on right side navigation that     will triggers the Pipeline process i.e. Build docker image, Push Image to your     Docker Hub Registry and Pull the image from Docker Registry, Remove local Docker     images and then Deploy to K8s Cluster as shown below:</p> <p></p> <p>You can see the deployment to your K8s Cluster is successful then you can browse the output using <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> as shown below:</p> <p></p> <p>You can see the Console Output logs of this pipeline process by clicking the icon before the id of the started Pipeline on the right bottom corner.</p> <p></p> <p>The pipeline stages after successful completion looks like below:</p> <p></p> </li> </ul> <p>We will continue on next documentation on how to setup GitHub Webhook in your Jenkins Pipeline so that Jenkins will trigger the build when a devops commits code to your GitHub repository's specific branch.</p>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/","title":"R Shiny","text":""},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#running-r-shiny-server-on-nerc","title":"Running R Shiny Server on NERC","text":""},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#running-r-shiny-server-on-nerc-openstack","title":"Running R Shiny Server on NERC OpenStack","text":""},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#setup-r-shiny-server-using-terraform","title":"Setup R Shiny Server using Terraform","text":"<p>Refer to this documentation on using Terraform to set up an R Shiny Server on your NERC OpenStack.</p> <p>To get started, you will need to clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/terraform-nerc-r-shiny.git\n</code></pre>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#setup-r-shiny-server-while-launching-a-vm","title":"Setup R Shiny Server while launching a VM","text":"<p>Within the terraform-nerc-r-shiny repo, it includes the bash script file i.e. <code>install-R-Shiny-&lt;OS&gt;.sh</code> required to setup the R Shiny Server. You can use a custom user-defined bash script based on your selected Image OS while launching an instance.</p> <p>During the launch, in the \"Configuration\" tab, you can enter the script in the \"CustomizationScript\" text area or upload the script file directly.</p> <p>Which R Shiny Script i.e. <code>install-R-Shiny-&lt;OS&gt;.sh</code> to Choose?</p> <p>Please use the appropriate bash script file i.e. <code>install-R-Shiny-&lt;OS&gt;.sh</code> based on your operating system (OS):</p> <ul> <li> <p>AlmaLinux -&gt; <code>install-R-Shiny-AlmaLinux.sh</code></p> </li> <li> <p>CentOS -&gt; <code>install-R-Shiny-Centos.sh</code></p> </li> <li> <p>Ubuntu -&gt; <code>install-R-Shiny-Ubuntu.sh</code></p> </li> </ul>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#running-r-shiny-server-on-nerc-openshift","title":"Running R Shiny Server on NERC OpenShift","text":"<p>We have used the Base Docker image compatible with OpenShift to create an OpenShift Template. Here, we walk through the process of creating a simple R Shiny Server template that bundles all the necessary resources - ConfigMap, Pod, Route, Service, etc. - and then deploy a Shiny application using that template.</p>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#steps-to-prepare-your-git-repo-with-application-source-code","title":"Steps to Prepare Your Git Repo with Application Source Code","text":"<p>To get started, fork the <code>rshiny-testapp</code> App in your own Github account.</p> <p>Very Important Information</p> <p>As you won't have full access to this repository, we recommend first forking the repository on your own GitHub account. So, you'll need to update all references to <code>https://github.com/nerc-project/rshiny-testapp.git</code> to point to your own forked repository.</p> <p>To create a fork of the example <code>rshiny-testapp</code> repository:</p> <ul> <li> <p>Go to https://github.com/nerc-project/rshiny-testapp.</p> </li> <li> <p>Click the \"Fork\" button to create a fork in your own GitHub account, e.g. \"<code>https://github.com/&lt;github_username&gt;/rshiny-testapp</code>\".</p> <p></p> </li> </ul> <p>Clone your forked rshiny-testapp git repository:</p> <pre><code>git clone &lt;https://github.com/&gt;&lt;github_username&gt;/rshiny-testapp.git\ncd rshiny-testapp\n</code></pre> <p>Shiny Application Source</p> <p>A ready-to-use sample Shiny application is available within the Git Repository.</p> <p>The repository includes:</p> <ul> <li> <p>A <code>Dockerfile</code> for containerization</p> </li> <li> <p>A <code>src</code> directory containing <code>app.R</code>, the main application script for a     sample shiny app.</p> </li> <li> <p>A <code>rshiny-server-template.yaml</code> file is available under the <code>openshift</code>     directory, providing a ready-to-use OpenShift Template for deploying     Shiny applications.</p> </li> </ul> <p>Always remember the <code>Dockerfile</code> needs to specify the base image i.e. <code>dukegcb/openshift-shiny-verse:4.1.2</code>, additional package requirements, and the location of your Shiny application code under <code>src</code> directory.</p> <p>You can update the contents of your forked repository with your own <code>Dockerfile</code> and <code>src</code> directory containing your Shiny app files.</p> <p>For example, if your app is located in the <code>src</code> directory and requires the <code>here</code> package (you can comment it out if no additional packages are needed), your <code>Dockerfile</code> should look like this:</p> <pre><code>FROM dukegcb/openshift-shiny-verse:4.1.2\nRUN install2.r here\nADD ./src /srv/code\n</code></pre> <p>Important Note</p> <p>The <code>install2.r</code> script is a simple utility to install R packages that is provided by the <code>rocker</code> images.</p>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#deploy-your-shiny-app-using-the-openshift-web-console","title":"Deploy your shiny app using the OpenShift Web console","text":"<p>Here, we walk through the process of creating a simple R Shiny Web Server using an OpenShift Template, which bundles all the necessary resources required to run it, such as ImageStream, BuildConfig, Deployment, Route, Service, etc., and then initiates and deploys a Shiny application from that template.</p> <p>An OpenShift template file, <code>rshiny-server-template.yaml</code>, is available within the Git Repository under the <code>openshift</code> directory.</p> <p>More about Writing Templates</p> <p>For more options and customization please read this.</p> <ol> <li> <p>Click the \"Import YAML\" button, represented by the \"+\" icon in the top navigation    bar, or navigate to the From Local Machine section and select Import YAML,    as shown below:</p> <p></p> <p>Next, the Import YAML editor box will open, as shown below:</p> <p></p> </li> <li> <p>Either drag and drop the locally downloaded rshiny-server-template.yaml    file or copy and paste its contents into the opened Import YAML editor box, as    shown below:</p> <p></p> </li> <li> <p>You need to go to the Home -&gt; Software Catalog menu as shown below:</p> <p></p> </li> <li> <p>Then, you will be able to use the created Developer Catalog template by searching    for \"rshiny\" on catalog as shown below:</p> <p></p> </li> <li> <p>Once selected by clicking the template, you will see Instantiate Template web    interface as shown below:</p> <p></p> </li> <li> <p>Based on our template definition, we request that users input some variables    to initate the R Shiny Server.</p> <p>Variables</p> <p>All variables are mandatory for the application to be created.</p> <ul> <li> <p>APP_NAME - Name used for the app</p> </li> <li> <p>APP_LABEL - Label used for the app</p> </li> <li> <p>APP_GIT_URI - GitHub repository URL</p> </li> <li> <p>APP_GIT_BRANCH - Git branch to build from</p> </li> <li> <p>REPO_DOCKERFILE_PATH - Location of the shiny app Dockerfile</p> </li> <li> <p>IMAGE_TAG - Tag for the built image</p> </li> <li> <p>NAMESPACE - Namespace where the application will be deployed</p> </li> <li> <p>PROBE_INITIAL_DELAY - Initial delay in seconds for liveness and readiness probes</p> </li> <li> <p>PROBE_TIMEOUT - Timeout in seconds for liveness and readiness probes</p> </li> </ul> <p></p> <p>Important: Update the Variable Values According to Your Own Configuration!</p> <p>It is essential to update the variable values in the OpenShift template to align with your specific configuration. Ensure that these values are adjusted according to your environment and requirements before deployment. If variable values are not explicitly provided, the template will automatically use its default values.</p> </li> <li> <p>Once successfully initiated, you can either open the application URL using the    Open URL icon as shown below or you can naviate to the route URL by    navigating to the \"Routes\" section under the Location path as shown below:</p> <p></p> </li> <li> <p>Finally, you will be able to see the R Shiny app!</p> <p></p> </li> </ol> <p>Modifying uploaded templates</p> <p>You can edit a template that has already been uploaded to your project: <code>oc edit template &lt;template&gt;</code>.</p>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#deploy-your-shiny-app-using-the-openshift-cli-oc","title":"Deploy your shiny app using the OpenShift CLI (<code>oc</code>)","text":"<ul> <li> <p>Prerequisites:</p> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI to enable <code>oc</code> commands. Refer to this user guide.</p> <p>Information</p> <p>Some users may have access to multiple projects. Run the following command to switch to a specific project space: <code>oc project &lt;your-project-namespace&gt;</code>.</p> <p>Please confirm the correct project is being selected by running <code>oc project</code>, as shown below:</p> <pre><code>oc project\nUsing project \"&lt;your_openshift_project_to_add_r_shiny_app&gt;\" on server \"https://api.shift.nerc.mghpcc.org:6443\".\n</code></pre> </li> </ul>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#deploy-your-r-shiny-application","title":"Deploy your R Shiny application","text":"<p>Process and apply template using default values from the template and passing your application specific parameters.</p> <pre><code>oc process -f ./openshift/rshiny-server-template.yaml \\\n   -p APP_GIT_URI=&lt;YOUR_GIT_REPO&gt; \\\n   -p APP_GIT_BRANCH=&lt;YOUR_GIT_BRANCH&gt; \\\n   -p REPO_DOCKERFILE_PATH=&lt;PATH_TO_DOCKERFILE_IN_YOUR_REPO&gt; \\\n   -p NAMESPACE=$(oc project --short) \\\n   | oc create -f -\n</code></pre> <p>For example, the command will look like this:</p> <pre><code>oc process -f ./openshift/rshiny-server-template.yaml \\\n    -p APP_GIT_URI=https://github.com/nerc-project/rshiny-testapp \\\n    -p APP_GIT_BRANCH=main \\\n    -p REPO_DOCKERFILE_PATH=\"Dockerfile\" \\\n    -p NAMESPACE=$(oc project --short) \\\n    | oc create -f -\n</code></pre> <p>Important: Default Template Values Will Be Used If Not Overridden!</p> <p>If variable values are not explicitly provided, the template will automatically use its default values.</p>"},{"location":"other-tools/R/r-shiny-server/running-rshiny-on-NERC/#deleting-your-application-and-all-its-resources","title":"Deleting your application and all its resources","text":"<p>Either using the OpenShift Web console, you can Right click on the application box and then confirm to delete it as shown below:</p> <p></p> <p>Alternatively, you can use the OpenShift CLI i.e. <code>oc</code> commands to delete all application resources:</p> <pre><code>oc delete all -l app=&lt;APP_LABEL&gt;\noc delete svc &lt;APP_NAME&gt;-service\noc delete route &lt;APP_NAME&gt;-route\noc delete bc &lt;APP_NAME&gt;-build\noc delete deploy &lt;APP_NAME&gt;-deployment\noc delete imagestream &lt;APP_NAME&gt;-imagestream\noc delete template shiny-server-template\n</code></pre> <p>For example, the command will look like this:</p> <pre><code>oc delete all -l app=shiny\noc delete svc shiny-app-service\noc delete route shiny-app-route\noc delete bc shiny-app-build\noc delete deploy shiny-app-deployment\noc delete imagestream shiny-app-imagestream\noc delete template shiny-server-template\n</code></pre>"},{"location":"other-tools/R/rstudio-server/running-rstudio-on-NERC/","title":"RStudio","text":""},{"location":"other-tools/R/rstudio-server/running-rstudio-on-NERC/#running-rstudio-server-on-nerc","title":"Running RStudio Server on NERC","text":""},{"location":"other-tools/R/rstudio-server/running-rstudio-on-NERC/#deploying-as-a-workbench-using-a-data-science-project-dsp-on-nerc-rhoai","title":"Deploying as a Workbench Using a Data Science Project (DSP) on NERC RHOAI","text":"<ol> <li> <p>Navigating to the OpenShift AI dashboard.</p> <p>Please follow these steps to access the NERC OpenShift AI dashboard.</p> </li> <li> <p>Please ensure that you start your RStudio Web server with options as depicted     in the following configuration screen. This screen provides you with the opportunity     to select a notebook image and configure its options, including the Accelerator     and Number of accelerators (GPUs).</p> <p></p> <p>For our example project, let's name it \"RStudio Workbench\". We'll select the RStudio image, choose a Deployment size of Small, choose Accelerator of None (no GPU is needed for this setup), and allocate a Cluster storage space of 20GB (Selected By Default).</p> <p>Tip</p> <p>The dashboard currently enforces a minimum storage volume size of 20GB. Please ensure that you modify this based on your need in Cluster Storage.</p> </li> <li> <p>If this procedure is successful, you have started your RStudio Web Server. When     your workbench is ready and the status changes to Running, click the open     icon () next to your workbench's name, or     click the workbench name directly to access your environment:</p> <p></p> </li> <li> <p>Once you have successfully authenticated by clicking \"mss-keycloak\" when     prompted, as shown below:</p> <p></p> <p>Next, you should see the RStudio Web Server, as shown below:</p> <p></p> </li> </ol>"},{"location":"other-tools/R/rstudio-server/running-rstudio-on-NERC/#running-rstudio-server-on-nerc-openshift","title":"Running RStudio Server on NERC OpenShift","text":"<p>In this guide, we walk through the process of creating a simple RStudio Web Server using an OpenShift Template, which bundles all the necessary resources required to run it, such as ConfigMap, Pod, Route, Service, etc., and then initiate and deploy the RStudio server from that template.</p> <p>To get started, clone the repository by running:</p> <pre><code>git clone https://github.com/nerc-project/rstudio-testapp.git\n</code></pre> <p>After that, follow the steps outlined in that guide. At the end, you will be able to view the RStudio Web Server interface!</p> <p></p>"},{"location":"other-tools/apache-spark/spark/","title":"Apache Spark","text":""},{"location":"other-tools/apache-spark/spark/#apache-spark-cluster-setup-on-nerc-openstack","title":"Apache Spark Cluster Setup on NERC OpenStack","text":""},{"location":"other-tools/apache-spark/spark/#apache-spark-overview","title":"Apache Spark Overview","text":"<p>Apache Spark is increasingly recognized as the primary analysis suite for big data, particularly among Python users. Spark offers a robust Python API and includes several valuable built-in libraries such as MLlib for machine learning and Spark Streaming for real-time analysis. In contrast to Apache Hadoop, Spark performs most computations in main memory boosting the performance.</p> <p>Many modern computational tasks utilize the MapReduce parallel paradigm. This computational process comprises two stages: Map and Reduce. Before task execution, all data is distributed across the nodes of the cluster. During the \"Map\" stage, the master node dispatches the executable task to the other nodes, and each worker processes its respective data. The subsequent step is \"Reduce\" that involves the master node collecting all results from the workers and generating final results based on the workers' outcomes. Apache Spark also implements this model of computations so it provides Big Data Processing abilities.</p>"},{"location":"other-tools/apache-spark/spark/#apache-spark-cluster-setup","title":"Apache Spark Cluster Setup","text":"<p>To get a Spark standalone cluster up and running manually, all you need to do is spawn some VMs and start Spark as master on one of them and worker on the others. They will automatically form a cluster that you can connect to/from Python, Java, and Scala applications using the IP address of the master VM.</p>"},{"location":"other-tools/apache-spark/spark/#setup-a-master-vm","title":"Setup a Master VM","text":"<ul> <li> <p>To create a master VM for the first time, ensure that the \"Image\" dropdown option     is selected. In this example, we selected ubuntu-22.04-x86_64 and the <code>cpu-su.2</code>     flavor is being used.</p> </li> <li> <p>Make sure you have added rules in the     Security Groups     to allow ssh using Port 22 access to the instance.</p> </li> <li> <p>Assign a Floating IP     to your new instance so that you will be able to ssh into this machine:</p> <pre><code>ssh ubuntu@&lt;Floating-IP&gt; -A -i &lt;Path_To_Your_Private_Key&gt;\n</code></pre> <p>For example:</p> <pre><code>ssh ubuntu@199.94.61.4 -A -i cloud.key\n</code></pre> </li> <li> <p>Upon successfully accessing the machine, execute the following dependencies:</p> <pre><code>sudo apt-get -y update\nsudo apt install default-jre -y\n</code></pre> </li> <li> <p>Download and install Scala:</p> <pre><code>wget https://downloads.lightbend.com/scala/2.13.10/scala-2.13.10.deb\nsudo dpkg -i scala-2.13.10.deb\nsudo apt-get install scala\n</code></pre> <p>Note</p> <p>Installing Scala means installing various command-line tools such as the Scala compiler and build tools.</p> </li> <li> <p>Download and unpack Apache Spark:</p> <pre><code>SPARK_VERSION=\"3.4.2\"\nAPACHE_MIRROR=\"dlcdn.apache.org\"\n\nwget https://$APACHE_MIRROR/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3-scala2.13.tgz\nsudo tar -zxvf spark-$SPARK_VERSION-bin-hadoop3-scala2.13.tgz\nsudo cp -far spark-$SPARK_VERSION-bin-hadoop3-scala2.13 /usr/local/spark\n</code></pre> <p>Very Important Note</p> <p>Please ensure you are using the latest Spark version by modifying the <code>SPARK_VERSION</code> in the above script. Additionally, verify that the version exists on the <code>APACHE_MIRROR</code> website. Please note the value of <code>SPARK_VERSION</code> as you will need it during Preparing Jobs for Execution and Examination.</p> </li> <li> <p>Create an SSH/RSA Key by running <code>ssh-keygen -t rsa</code> without using any passphrase:</p> <pre><code>ssh-keygen -t rsa\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/ubuntu/.ssh/id_rsa\nYour public key has been saved in /home/ubuntu/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:8i/TVSCfrkdV4+Jyqc00RoZZFSHNj8C0QugmBa7RX7U ubuntu@spark-master\nThe key's randomart image is:\n+---[RSA 3072]----+\n|      .. ..o..++o|\n|     o  o.. +o.+.|\n|    . +o  .o=+.oo|\n|     +.oo  +o++..|\n|    o EoS  .+oo  |\n|     . o   .+B   |\n|        .. +O .  |\n|        o.o..o   |\n|         o..     |\n+----[SHA256]-----+\n</code></pre> </li> <li> <p>Copy and append the contents of SSH public key i.e. <code>~/.ssh/id_rsa.pub</code> to     the <code>~/.ssh/authorized_keys</code> file.</p> </li> </ul>"},{"location":"other-tools/apache-spark/spark/#create-a-volume-snapshot-of-the-master-vm","title":"Create a Volume Snapshot of the master VM","text":"<ul> <li> <p>Once you're logged in to NERC's Horizon dashboard.     You need to Shut Off the master vm before creating a volume snapshot.</p> <p>Click Action -&gt; Shut Off Instance.</p> <p>Status will change to <code>Shutoff</code>.</p> </li> <li> <p>Then, create a snapshot of its attached volume by clicking on the \"Create snapshot\"     from the Project -&gt; Volumes -&gt; Volumes as described here.</p> </li> </ul>"},{"location":"other-tools/apache-spark/spark/#create-two-worker-instances-from-the-volume-snapshot","title":"Create Two Worker Instances from the Volume Snapshot","text":"<ul> <li> <p>Once a snapshot is created and is in \"Available\" status, you can view and manage     it under the Volumes menu in the Horizon dashboard under Volume Snapshots.</p> <p>Navigate to Project -&gt; Volumes -&gt; Snapshots.</p> </li> <li> <p>You have the option to directly launch this volume as an instance by clicking     on the arrow next to \"Create Volume\" and selecting \"Launch as Instance\".</p> <p>NOTE: Specify Count: 2 to launch 2 instances using the volume snapshot as shown below:</p> <p></p> <p>Naming, Security Group and Flavor for Worker Nodes</p> <p>You can specify the \"Instance Name\" as \"spark-worker\", and for each instance, it will automatically append incremental values at the end, such as <code>spark-worker-1</code> and <code>spark-worker-2</code>. Also, make sure you have attached the Security Groups to allow ssh using Port 22 access to the worker instances.</p> </li> </ul> <p>Additionally, during launch, you will have the option to choose your preferred flavor for the worker nodes, which can differ from the master VM based on your computational requirements.</p> <ul> <li> <p>Navigate to Project -&gt; Compute -&gt; Instances.</p> </li> <li> <p>Restart the shutdown master VM, click Action -&gt; Start Instance.</p> </li> <li> <p>The final set up for our Spark cluster looks like this, with 1 master node and     2 worker nodes:</p> <p></p> </li> </ul>"},{"location":"other-tools/apache-spark/spark/#configure-spark-on-the-master-vm","title":"Configure Spark on the Master VM","text":"<ul> <li> <p>SSH login into the master VM again.</p> </li> <li> <p>Update the <code>/etc/hosts</code> file to specify all three hostnames with their corresponding     internal IP addresses.</p> <pre><code>sudo nano /etc/hosts\n</code></pre> <p>Ensure all hosts are resolvable by adding them to <code>/etc/hosts</code>. You can modify the following content specifying each VM's internal IP addresses and paste the updated content at the end of the <code>/etc/hosts</code> file. Alternatively, you can use <code>sudo cat &gt;&gt; /etc/hosts</code> to append the content directly to the end of the <code>/etc/hosts</code> file.</p> <pre><code>&lt;Master-Internal-IP&gt; master\n&lt;Worker1-Internal-IP&gt; worker1\n&lt;Worker2-Internal-IP&gt; worker2\n</code></pre> <p>Very Important Note</p> <p>Make sure to use <code>&gt;&gt;</code> instead of <code>&gt;</code> to avoid overwriting the existing content and append the new content at the end of the file.</p> <p>For example, the end of the <code>/etc/hosts</code> file looks like this:</p> <pre><code>sudo cat /etc/hosts\n...\n192.168.0.46 master\n192.168.0.26 worker1\n192.168.0.136 worker2\n</code></pre> </li> <li> <p>Verify that you can SSH into both worker nodes by using <code>ssh worker1</code> and     <code>ssh worker2</code> from the Spark master node's terminal.</p> </li> <li> <p>Copy the sample configuration file for the Spark:</p> <pre><code>cd /usr/local/spark/conf/\ncp spark-env.sh.template spark-env.sh\n</code></pre> </li> <li> <p>Update the environment variables file i.e. <code>spark-env.sh</code> to include the following     information:</p> <pre><code>export SPARK_MASTER_HOST='&lt;Master-Internal-IP&gt;'\nexport JAVA_HOME=&lt;Path_of_JAVA_installation&gt;\n</code></pre> <p>Environment Variables</p> <p>Executing this command: <code>readlink -f $(which java)</code> will display the path to the current Java setup in your VM. For example: <code>/usr/lib/jvm/java-11-openjdk-amd64/bin/java</code>, you need to remove the last <code>bin/java</code> part, i.e. <code>/usr/lib/jvm/java-11-openjdk-amd64</code>, to set it as the <code>JAVA_HOME</code> environment variable. Learn more about other Spark settings that can be configured through environment variables here.</p> <p>For example:</p> <pre><code>echo \"export SPARK_MASTER_HOST='192.168.0.46'\" &gt;&gt; spark-env.sh\necho \"export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\" &gt;&gt; spark-env.sh\n</code></pre> </li> <li> <p>Source the changed environment variables file i.e. <code>spark-env.sh</code>:</p> <pre><code>source spark-env.sh\n</code></pre> </li> <li> <p>Create a file named <code>slaves</code> in the Spark configuration directory (i.e.,     <code>/usr/local/spark/conf/</code>) that specifies all 3 hostnames (nodes) as specified     in <code>/etc/hosts</code>:</p> <pre><code>sudo cat slaves\nmaster\nworker1\nworker2\n</code></pre> </li> </ul>"},{"location":"other-tools/apache-spark/spark/#run-the-spark-cluster-from-the-master-vm","title":"Run the Spark cluster from the Master VM","text":"<ul> <li> <p>SSH into the master VM again if you are not already logged in.</p> </li> <li> <p>You need to run the Spark cluster from <code>/usr/local/spark</code>:</p> <pre><code>cd /usr/local/spark\n\n# Start all hosts (nodes) including master and workers\n./sbin/start-all.sh\n</code></pre> <p>How to Stop All Spark Cluster</p> <p>To stop all of the Spark cluster nodes, execute <code>./sbin/stop-all.sh</code> command from <code>/usr/local/spark</code>.</p> </li> </ul>"},{"location":"other-tools/apache-spark/spark/#connect-to-the-spark-webui","title":"Connect to the Spark WebUI","text":"<p>Apache Spark provides a suite of web user interfaces (WebUIs) that you can use to monitor the status and resource consumption of your Spark cluster.</p> <p>Different types of Spark Web UI</p> <p>Apache Spark provides different web UIs: Master web UI, Worker web UI, and Application web UI.</p> <ul> <li> <p>You can connect to the Master web UI using     SSH Port Forwarding, aka SSH Tunneling     i.e. Local Port Forwarding from your local machine's terminal by running:</p> <pre><code>ssh -N -L &lt;Your_Preferred_Port&gt;:localhost:8080 &lt;User&gt;@&lt;Floating-IP&gt; -i &lt;Path_To_Your_Private_Key&gt;\n</code></pre> <p>Here, you can choose any port that is available on your machine as <code>&lt;Your_Preferred_Port&gt;</code> and then master VM's assigned Floating IP as <code>&lt;Floating-IP&gt;</code> and associated Private Key pair attached to the VM as <code>&lt;Path_To_Your_Private_Key&gt;</code>.</p> <p>For example:</p> <pre><code>ssh -N -L 8080:localhost:8080 ubuntu@199.94.61.4 -i ~/.ssh/cloud.key\n</code></pre> </li> <li> <p>Once the SSH Tunneling is successful, please do not close or stop the terminal     where you are running the SSH Tunneling. Instead, log in to the Master web UI     using your web browser: <code>http://localhost:&lt;Your_Preferred_Port&gt;</code> i.e. <code>http://localhost:8080</code>.</p> </li> </ul> <p>The Master web UI offers an overview of the Spark cluster, showcasing the following details:</p> <ul> <li>Master URL and REST URL</li> <li>Available CPUs and memory for the Spark cluster</li> <li>Status and allocated resources for each worker</li> <li>Details on active and completed applications, including their status, resources,     and duration</li> <li>Details on active and completed drivers, including their status and resources</li> </ul> <p>The Master web UI appears as shown below when you navigate to <code>http://localhost:&lt;Your_Preferred_Port&gt;</code> i.e. <code>http://localhost:8080</code> from your web browser:</p> <p></p> <p>The Master web UI also provides an overview of the applications. Through the Master web UI, you can easily identify the allocated vCPU (Core) and memory resources for both the Spark cluster and individual applications.</p>"},{"location":"other-tools/apache-spark/spark/#preparing-jobs-for-execution-and-examination","title":"Preparing Jobs for Execution and Examination","text":"<ul> <li> <p>To run jobs from <code>/usr/local/spark</code>, execute the following commands:</p> <pre><code>cd /usr/local/spark\nSPARK_VERSION=\"3.4.2\"\n</code></pre> <p>Very Important Note</p> <p>Please ensure you are using the same Spark version that you have downloaded and installed previously as the value of <code>SPARK_VERSION</code> in the above script.</p> </li> <li> <p>Single Node Job:</p> <p>Let's quickly start to run a simple job:</p> <pre><code>./bin/spark-submit --driver-memory 2g --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.13-$SPARK_VERSION.jar 50\n</code></pre> </li> <li> <p>Cluster Mode Job:</p> <p>Let's submit a longer and more complex job with many tasks that will be distributed among the multi-node cluster, and then view the Master web UI:</p> <pre><code>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 examples/jars/spark-examples_2.13-$SPARK_VERSION.jar 1000\n</code></pre> <p>While the job is running, you will see a similar view on the Master web UI under the \"Running Applications\" section:</p> <p></p> <p>Once the job is completed, it will show up under the \"Completed Applications\" section on the Master web UI as shown below:</p> <p></p> </li> </ul>"},{"location":"other-tools/kubernetes/comparisons/","title":"K8s Flavors Comparision","text":""},{"location":"other-tools/kubernetes/comparisons/#comparison","title":"Comparison","text":""},{"location":"other-tools/kubernetes/comparisons/#kubespray-vs-kubeadm","title":"Kubespray vs Kubeadm","text":"<p>Kubeadm provides domain Knowledge of Kubernetes clusters' life cycle management, including self-hosted layouts, dynamic discovery services and so on. Had it belonged to the new operators world, it may have been named a \"Kubernetes cluster operator\". Kubespray however, does generic configuration management tasks from the \"OS operators\" ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping.</p> <p>Kubespray has started using <code>kubeadm</code> internally for cluster creation since v2.3 in order to consume life cycle management domain knowledge from it and offload generic OS configuration things from it, which hopefully benefits both sides.</p>"},{"location":"other-tools/kubernetes/k0s/","title":"k0s","text":""},{"location":"other-tools/kubernetes/k0s/#k0s","title":"k0s","text":""},{"location":"other-tools/kubernetes/k0s/#key-features","title":"Key Features","text":"<ul> <li>Available as a single static binary</li> <li>Offers a self-hosted, isolated control plane</li> <li>Supports a variety of storage backends, including etcd, SQLite, MySQL (or any     compatible), and PostgreSQL.</li> <li>Offers an Elastic control plane</li> <li>Vanilla upstream Kubernetes</li> <li>Supports custom container runtimes (containerd is the default)</li> <li>Supports custom Container Network Interface (CNI) plugins (calico is the default)</li> <li>Supports x86_64 and arm64</li> </ul>"},{"location":"other-tools/kubernetes/k0s/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 VM to create a single node kubernetes cluster using <code>k0s</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu OS image,     <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage - also assign Floating IP     to this VM.</p> </li> <li> <p>setup Unique hostname to the machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.252 k0s\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname k0s\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k0s/#install-k0s-on-ubuntu","title":"Install k0s on Ubuntu","text":"<p>Run the below command on the Ubuntu VM:</p> <ul> <li> <p>SSH into k0s machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>apt-get update &amp;&amp; apt-get upgrade -y\n</code></pre> </li> <li> <p>Download k0s:</p> <pre><code>curl -sSLf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>Install k0s as a service:</p> <pre><code>k0s install controller --single\n\nINFO[2021-10-12 01:45:52] no config file given, using defaults\nINFO[2021-10-12 01:45:52] creating user: etcd\nINFO[2021-10-12 01:46:00] creating user: kube-apiserver\nINFO[2021-10-12 01:46:00] creating user: konnectivity-server\nINFO[2021-10-12 01:46:00] creating user: kube-scheduler\nINFO[2021-10-12 01:46:01] Installing k0s service\n</code></pre> </li> <li> <p>Start <code>k0s</code> as a service:</p> <pre><code>k0s start\n</code></pre> </li> <li> <p>Check service, logs and <code>k0s</code> status:</p> <pre><code>k0s status\n\nVersion: v1.22.2+k0s.1\nProcess ID: 16625\nRole: controller\nWorkloads: true\n</code></pre> </li> <li> <p>Access your cluster using <code>kubectl</code>:</p> <pre><code>k0s kubectl get nodes\n\nNAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   8m3s   v1.22.2+k0s\n</code></pre> <pre><code>alias kubectl='k0s kubectl'\nkubectl get nodes -o wide\n</code></pre> <pre><code>kubectl get all\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   38s\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k0s/#uninstall-k0s","title":"Uninstall k0s","text":"<ul> <li> <p>Stop the service:</p> <pre><code>sudo k0s stop\n</code></pre> </li> <li> <p>Execute the <code>k0s reset</code> command - cleans up the installed system service, data     directories, containers, mounts and network namespaces.</p> <pre><code>sudo k0s reset\n</code></pre> </li> <li> <p>Reboot the system</p> </li> </ul>"},{"location":"other-tools/kubernetes/kind/","title":"Kind","text":""},{"location":"other-tools/kubernetes/kind/#kind","title":"Kind","text":""},{"location":"other-tools/kubernetes/kind/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 VM to create a single node kubernetes cluster using <code>kind</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine, <code>almalinux-9-x86_64</code>, <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM,     20GB storage - also assign Floating IP     to this VM.</p> </li> <li> <p>setup Unique hostname to the machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.167 kind\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname kind\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kind/#install-docker-on-almalinux","title":"Install docker on AlmaLinux","text":"<p>Run the below command on the AlmaLinux VM:</p> <ul> <li> <p>SSH into kind machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Execute the below command to initialize the cluster:</p> <p>Please remove <code>container-tools</code> module that includes stable versions of podman, buildah, skopeo, runc, conmon, etc as well as dependencies and will be removed with the module. If this module is not removed then it will conflict with Docker. Red Hat does recommend Podman on RHEL 8.</p> <pre><code>dnf module remove container-tools\n\ndnf update -y\n\ndnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo\n\ndnf install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n\nsystemctl start docker\nsystemctl enable --now docker\nsystemctl status docker\n\ndocker -v\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kind/#install-kubectl-on-almalinux","title":"Install kubectl on AlmaLinux","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl\nchmod +x /usr/bin/kubectl\n</code></pre> <ul> <li> <p>Test to ensure that the <code>kubectl</code> is installed:</p> <pre><code>kubectl version --client\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kind/#install-kind","title":"Install kind","text":"<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64\nchmod +x ./kind\nmv ./kind /usr/bin\n</code></pre> <pre><code>which kind\n\n/bin/kind\n</code></pre> <pre><code>kind version\n\nkind v0.11.1 go1.16.4 linux/amd64\n</code></pre> <ul> <li> <p>To communicate with cluster, just give the cluster name as a context in kubectl:</p> <pre><code>kind create cluster --name k8s-kind-cluster1\n\nCreating cluster \"k8s-kind-cluster1\" ...\n\u2713 Ensuring node image (kindest/node:v1.21.1) \ud83d\uddbc\n\u2713 Preparing nodes \ud83d\udce6\n\u2713 Writing configuration \ud83d\udcdc\n\u2713 Starting control-plane \ud83d\udd79\ufe0f\n\u2713 Installing CNI \ud83d\udd0c\n\u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-k8s-kind-cluster1\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-k8s-kind-cluster1\n\nHave a nice day! \ud83d\udc4b\n</code></pre> </li> <li> <p>Get the cluster details:</p> <pre><code>kubectl cluster-info --context kind-k8s-kind-cluster1\n\nKubernetes control plane is running at https://127.0.0.1:38646\nCoreDNS is running at https://127.0.0.1:38646/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <pre><code>kubectl get all\n\nNAME                TYPE       CLUSTER-IP  EXTERNAL-IP  PORT(S)  AGE\nservice/kubernetes  ClusterIP  10.96.0.1   &lt;none&gt;       443/TCP  5m25s\n</code></pre> <pre><code>kubectl get nodes\n\nNAME                             STATUS  ROLES                AGE    VERSION\nk8s-kind-cluster1-control-plane  Ready  control-plane,master  5m26s  v1.21.1\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kind/#deleting-a-cluster","title":"Deleting a Cluster","text":"<p>If you created a cluster with kind create cluster then deleting is equally simple:</p> <pre><code>kind delete cluster\n</code></pre>"},{"location":"other-tools/kubernetes/kubernetes/","title":"Kubernetes Overview","text":""},{"location":"other-tools/kubernetes/kubernetes/#kubernetes-overview","title":"Kubernetes Overview","text":"<p>Kubernetes, commonly known as K8s is an open sourced container orchestration tool for managing containerized cloud-native workloads and services in computing, networking, and storage infrastructure. K8s can help to deploy and manage containerized applications like platforms as a service(PaaS), batch processing workers, and microservices in the cloud at scale. It reduces cloud computing costs while simplifying the operation of resilient and scalable applications. While it is possible to install and manage Kubernetes on infrastructure that you manage, it is a time-consuming and complicated process. To make provisioning and deploying clusters much easier, we have listed a number of popular platforms and tools to setup your K8s on your NERC's OpenStack Project space.</p>"},{"location":"other-tools/kubernetes/kubernetes/#kubernetes-components-architecture","title":"Kubernetes Components &amp; Architecture","text":"<p>A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload.</p> <p>The control plane or master manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance, redundancy, and high availability.</p> <p>Here's the diagram of a Kubernetes cluster with all the components tied together. </p>"},{"location":"other-tools/kubernetes/kubernetes/#kubernetes-basics-workflow","title":"Kubernetes Basics workflow","text":"<ol> <li> <p>Create a Kubernetes cluster    </p> </li> <li> <p>Deploy an app    </p> </li> <li> <p>Explore your app    </p> </li> <li> <p>Expose your app publicly    </p> </li> <li> <p>Scale up your app    </p> </li> <li> <p>Update your app    </p> </li> </ol>"},{"location":"other-tools/kubernetes/kubernetes/#development-environment","title":"Development environment","text":"<ol> <li> <p>Minikube is a local Kubernetes    cluster that focuses on making Kubernetes development and learning simple.    Kubernetes may be started with just a single command if you have a Docker    (or similarly comparable) container or a Virtual Machine environment.    For more read this.</p> </li> <li> <p>Kind is a tool for running    local Kubernetes clusters utilizing Docker container \"nodes\". It was built for    Kubernetes testing, but it may also be used for local development and continuous    integration. For more read this.</p> </li> <li> <p>MicroK8s is the smallest, fastest, and most conformant    Kubernetes that tracks upstream releases and simplifies clustering. MicroK8s    is ideal for prototyping, testing, and offline development.    For more read this.</p> </li> <li> <p>K3s is a single &lt;40MB binary, certified Kubernetes distribution    developed by Rancher Labs and now a CNCF sandbox project that fully implements    the Kubernetes API and is less than 40MB in size. To do so, they got rid of    a lot of additional drivers that didn't need to be in the core and could    easily be replaced with add-ons. For more read this.</p> <p>To setup a Multi-master HA K3s cluster using k3sup(pronounced ketchup) read this.</p> <p>To setup a Single-Node K3s Cluster using k3d read this and if you would like to setup Multi-master K3s cluster setup using k3d read this.</p> </li> <li> <p>k0s is an all-inclusive Kubernetes distribution,    configured with all of the features needed to build a Kubernetes cluster simply    by copying and running an executable file on each target host.    For more read this.</p> </li> </ol>"},{"location":"other-tools/kubernetes/kubernetes/#production-environment","title":"Production environment","text":"<p>If your Kubernetes cluster has to run critical workloads, it must be configured to be resilient and higly available(HA) production-ready Kubernetes cluster. To setup production-quality cluster, you can use the following deployment tools.</p> <ol> <li> <p>Kubeadm    performs the actions necessary to get a minimum viable, secure cluster up and    running in a user friendly way.    Bootstrapping cluster with kubeadm read this    and if you would like to setup Multi-master cluster setup using Kubeadm    read this.</p> </li> <li> <p>Kubespray    helps to install a Kubernetes cluster on NERC OpenStack. Kubespray is a    composition of Ansible playbooks, inventory, provisioning tools, and domain    knowledge for generic OS/Kubernetes clusters configuration management tasks.    Installing Kubernetes with Kubespray read this.</p> </li> </ol> <p>To choose a tool which best fits your use case, read this comparison.</p>"},{"location":"other-tools/kubernetes/kubespray/","title":"Kubespray","text":""},{"location":"other-tools/kubernetes/kubespray/#kubespray","title":"Kubespray","text":""},{"location":"other-tools/kubernetes/kubespray/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 control-plane(master) and 1 worker node to create a single control-plane kubernetes cluster using <code>Kubespray</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine for Ansible master, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu     OS image, <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage.</p> </li> <li> <p>1 Linux machine for master, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu     OS image, <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage -     also assign Floating IP     to the master node.</p> </li> <li> <p>1 Linux machines for worker, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu     OS image, <code>cpu-su.1</code> flavor with 1vCPU, 4GB RAM, 20GB storage.</p> </li> <li> <p>ssh access to all machines: Read more here     on how to set up SSH on your remote VMs.</p> </li> <li> <p>To allow SSH from Ansible master to all other nodes: Read more here     Generate SSH key for Ansible master node using:</p> <pre><code>ssh-keygen -t rsa\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /root/.ssh/id_rsa\nYour public key has been saved in /root/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:OMsKP7EmhT400AJA/KN1smKt6eTaa3QFQUiepmj8dxroot@ansible-master\nThe key's randomart image is:\n+---[RSA 3072]----+\n|=o.oo.           |\n|.o...            |\n|..=  .           |\n|=o.= ...         |\n|o=+.=.o SE       |\n|.+*o+. o. .      |\n|.=== +o. .       |\n|o+=o=..          |\n|++o=o.           |\n+----[SHA256]-----+\n</code></pre> <p>Copy and append the contents of SSH public key i.e. <code>~/.ssh/id_rsa.pub</code> to other nodes's <code>~/.ssh/authorized_keys</code> file. Please make sure you are logged in as <code>root</code> user by doing <code>sudo su</code> before you copy this public key to the end of <code>~/.ssh/authorized_keys</code> file of the other master and worker nodes. This will allow <code>ssh &lt;other_nodes_internal_ip&gt;</code> from the Ansible master node's terminal.</p> </li> <li> <p>Create 2 security groups with appropriate ports and protocols:</p> <p>i. To be used by the master nodes: </p> <p>ii. To be used by the worker nodes: </p> </li> <li> <p>setup Unique hostname to each machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.224 ansible_master\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname ansible_master\n</code></pre> </li> </ul> <p>In this step, you will update packages and disable <code>swap</code> on the all 3 nodes:</p> <ul> <li> <p>1 Ansible Master Node - ansible_master</p> </li> <li> <p>1 Kubernetes Master Node - kubspray_master</p> </li> <li> <p>1 Kubernetes Worker Node - kubspray_worker1</p> </li> </ul> <p>The below steps will be performed on all the above mentioned nodes:</p> <ul> <li> <p>SSH into all the 3 machines</p> </li> <li> <p>Switch as root: <code>sudo su</code></p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>apt-get update &amp;&amp; apt-get upgrade -y\n</code></pre> </li> <li> <p>Turn off <code>swap</code></p> <pre><code>swapoff -a\nsed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubespray/#configure-kubespray-on-ansible_master-node-using-ansible-playbook","title":"Configure Kubespray on <code>ansible_master</code> node using Ansible Playbook","text":"<p>Run the below command on the master node i.e. <code>master</code> that you want to setup as control plane.</p> <ul> <li> <p>SSH into ansible_master machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Execute the below command to initialize the cluster:</p> </li> <li> <p>Install Python3 and upgrade pip to pip3:</p> <pre><code>apt install python3-pip -y\npip3 install --upgrade pip\npython3 -V &amp;&amp; pip3 -V\npip -V\n</code></pre> </li> <li> <p>Clone the Kubespray git repository:</p> <pre><code>git clone https://github.com/kubernetes-sigs/kubespray.git\ncd kubespray\n</code></pre> </li> <li> <p>Install dependencies from <code>requirements.txt</code>:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Copy <code>inventory/sample</code> as <code>inventory/mycluster</code></p> <pre><code>cp -rfp inventory/sample inventory/mycluster\n</code></pre> </li> <li> <p>Update Ansible inventory file with inventory builder:</p> <p>This step is little trivial because we need to update <code>hosts.yml</code> with the nodes IP.</p> <p>Now we are going to declare a variable \"IPS\" for storing the IP address of other K8s nodes .i.e. kubspray_master(192.168.0.130), kubspray_worker1(192.168.0.32)</p> <pre><code>declare -a IPS=(192.168.0.130 192.168.0.32)\nCONFIG_FILE=inventory/mycluster/hosts.yml python3 \\\n    contrib/inventory_builder/inventory.py ${IPS[@]}\n</code></pre> <p>This outputs:</p> <pre><code>DEBUG: Adding group all\nDEBUG: Adding group kube_control_plane\nDEBUG: Adding group kube_node\nDEBUG: Adding group etcd\nDEBUG: Adding group k8s_cluster\nDEBUG: Adding group calico_rr\nDEBUG: adding host node1 to group all\nDEBUG: adding host node2 to group all\nDEBUG: adding host node1 to group etcd\nDEBUG: adding host node1 to group kube_control_plane\nDEBUG: adding host node2 to group kube_control_plane\nDEBUG: adding host node1 to group kube_node\nDEBUG: adding host node2 to group kube_node\n</code></pre> </li> <li> <p>After running the above commands do verify the <code>hosts.yml</code> and its content:</p> <pre><code>cat inventory/mycluster/hosts.yml\n</code></pre> <p>The contents of the <code>hosts.yml</code> file should looks like:</p> <pre><code>all:\n    hosts:\n        node1:\n            ansible_host: 192.168.0.130\n            ip: 192.168.0.130\n            access_ip: 192.168.0.130\n        node2:\n            ansible_host: 192.168.0.32\n            ip: 192.168.0.32\n            access_ip: 192.168.0.32\n    children:\n        kube_control_plane:\n            hosts:\n                node1:\n                node2:\n        kube_node:\n            hosts:\n                node1:\n                node2:\n        etcd:\n            hosts:\n                node1:\n        k8s_cluster:\n            children:\n                kube_control_plane:\n                kube_node:\n        calico_rr:\n            hosts: {}\n</code></pre> </li> <li> <p>Review and change parameters under <code>inventory/mycluster/group_vars</code></p> <pre><code>cat inventory/mycluster/group_vars/all/all.yml\ncat inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml\n</code></pre> </li> <li> <p>It can be useful to set the following two variables to true in     <code>inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml</code>: <code>kubeconfig_localhost</code>     (to make a copy of <code>kubeconfig</code> on the host that runs Ansible in     <code>{ inventory_dir }/artifacts</code>) and <code>kubectl_localhost</code>     (to download <code>kubectl</code> onto the host that runs Ansible in <code>{ bin_dir }</code>).</p> <p>Very Important</p> <p>As Ubuntu 20 kvm kernel doesn't have dummy module we need to modify the following two variables in <code>inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml</code>: <code>enable_nodelocaldns: false</code> and <code>kube_proxy_mode: iptables</code> which will Disable nodelocal dns cache and Kube-proxy proxyMode to iptables respectively.</p> </li> <li> <p>Deploy Kubespray with Ansible Playbook - run the playbook as <code>root</code> user.     The option <code>--become</code> is required, as for example writing SSL keys in <code>/etc/</code>,     installing packages and interacting with various <code>systemd</code> daemons. Without     <code>--become</code> the playbook will fail to run!</p> <pre><code>ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml\n</code></pre> <p>Note</p> <p>Running ansible playbook takes little time because it depends on the network bandwidth also.</p> </li> </ul>"},{"location":"other-tools/kubernetes/kubespray/#install-kubectl-on-kubernetes-master-node-ie-kubspray_master","title":"Install kubectl on Kubernetes master node .i.e. <code>kubspray_master</code>","text":"<ul> <li> <p>Install kubectl binary</p> <pre><code>snap install kubectl --classic\n</code></pre> <p>This outputs: <code>kubectl 1.26.1 from Canonical\u2713 installed</code></p> </li> <li> <p>Now verify the kubectl version:</p> <pre><code>kubectl version -o yaml\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubespray/#validate-all-cluster-components-and-nodes-are-visible-on-all-nodes","title":"Validate all cluster components and nodes are visible on all nodes","text":"<ul> <li> <p>Verify the cluster</p> <pre><code>kubectl get nodes\n\nNAME    STATUS   ROLES                  AGE     VERSION\nnode1   Ready    control-plane,master   6m7s    v1.26.1\nnode2   Ready    control-plane,master   5m32s   v1.26.1\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubespray/#deploy-a-hello-minikube-application","title":"Deploy A Hello Minikube Application","text":"<ul> <li> <p>Use the kubectl create command to create a Deployment that manages a Pod. The     Pod runs a Container based on the provided Docker image.</p> <pre><code>kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4\n</code></pre> <pre><code>kubectl expose deployment hello-minikube --type=LoadBalancer --port=8080\n\nservice/hello-minikube exposed\n</code></pre> </li> <li> <p>View the deployments information:</p> <pre><code>kubectl get deployments\n\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nhello-minikube   1/1     1            1           50s\n</code></pre> </li> <li> <p>View the port information:</p> <pre><code>kubectl get svc hello-minikube\n\nNAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nhello-minikube   LoadBalancer   10.233.35.126   &lt;pending&gt;     8080:30723/TCP   40s\n</code></pre> </li> <li> <p>Expose the service locally</p> <pre><code>kubectl port-forward svc/hello-minikube 30723:8080\n\nForwarding from [::1]:30723 -&gt; 8080\nForwarding from 127.0.0.1:30723 -&gt; 8080\nHandling connection for 30723\nHandling connection for 30723\n</code></pre> </li> </ul> <p>Go to browser, visit <code>http://&lt;Master-Floating-IP&gt;:8080</code> i.e. http://140.247.152.235:8080/ to check the hello minikube default page.</p>"},{"location":"other-tools/kubernetes/kubespray/#clean-up","title":"Clean up","text":"<p>Now you can clean up the app resources you created in your cluster:</p> <pre><code>kubectl delete service hello-minikube\nkubectl delete deployment hello-minikube\n</code></pre>"},{"location":"other-tools/kubernetes/microk8s/","title":"MicroK8s","text":""},{"location":"other-tools/kubernetes/microk8s/#microk8s","title":"Microk8s","text":""},{"location":"other-tools/kubernetes/microk8s/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 VM to create a single node kubernetes cluster using <code>microk8s</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu OS image,     <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage - also assign Floating IP     to this VM.</p> </li> <li> <p>setup Unique hostname to the machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.62 microk8s\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname microk8s\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/microk8s/#install-microk8s-on-ubuntu","title":"Install MicroK8s on Ubuntu","text":"<p>Run the below command on the Ubuntu VM:</p> <ul> <li> <p>SSH into microk8s machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>apt-get update &amp;&amp; apt-get upgrade -y\n</code></pre> </li> <li> <p>Install MicroK8s:</p> <pre><code>sudo snap install microk8s --classic\n</code></pre> </li> <li> <p>Check the status while Kubernetes starts</p> <pre><code>microk8s status --wait-ready\n</code></pre> </li> <li> <p>Turn on the services you want:</p> <pre><code>microk8s enable dns dashboard\n</code></pre> <p>Try <code>microk8s enable --help</code> for a list of available services and optional features. <code>microk8s disable &lt;name&gt;</code> turns off a service. For example other useful services are: <code>microk8s enable registry istio storage</code></p> </li> <li> <p>Start using Kubernetes</p> <pre><code>microk8s kubectl get all --all-namespaces\n</code></pre> <p>If you mainly use MicroK8s you can make our kubectl the default one on your command-line with <code>alias mkctl=\"microk8s kubectl\"</code>. Since it is a standard upstream kubectl, you can also drive other Kubernetes clusters with it by pointing to the respective kubeconfig file via the <code>--kubeconfig</code> argument.</p> </li> <li> <p>Access the Kubernetes dashboard     UI:</p> <p></p> <p>As we see above the kubernetes-dashboard service in the kube-system namespace has a ClusterIP of 10.152.183.73 and listens on TCP port 443. The ClusterIP is randomly assigned, so if you follow these steps on your host, make sure you check the IP adress you got.</p> <p>Note</p> <p>Another way to access the default token to be used for the dashboard access can be retrieved with:</p> <pre><code>token=$(microk8s kubectl -n kube-system get secret | grep default-token | cut -d \"\" -f1)\nmicrok8s kubectl -n kube-system describe secret $token\n</code></pre> </li> <li> <p>Keep running the kubernetes-dashboad on Proxy to access it via web browser:</p> <pre><code>microk8s dashboard-proxy\n\nChecking if Dashboard is running.\nDashboard will be available at https://127.0.0.1:10443\nUse the following token to login:\neyJhbGc....\n</code></pre> <p>Important</p> <p>This tells us the IP address of the Dashboard and the port. The values assigned to your Dashboard will differ. Please note the displayed PORT and the TOKEN that are required to access the kubernetes-dashboard. Make sure, the exposed PORT is opened in Security Groups for the instance following this guide.</p> <p>This will show the token to login to the Dashbord shown on the url with NodePort.</p> <p>You'll need to wait a few minutes before the dashboard becomes available. If you open a web browser on the same desktop you deployed Microk8s and point it to <code>https://&lt;Floating-IP&gt;:&lt;PORT&gt;</code> (where PORT is the PORT assigned to the Dashboard noted while running the above command), you'll need to accept the risk (because the Dashboard uses a self-signed certificate). And, we can enter the previously noted TOKEN to access the kubernetes-dashboard.</p> <p></p> <p>Once you enter the correct TOKEN the kubernetes-dashboard is accessed and looks like below:</p> <p></p> </li> </ul> <p>Information</p> <ul> <li> <p>Start and stop Kubernetes: Kubernetes is a collection of system services that talk to each other all the time. If you don't need them running in the background then you will save battery by stopping them. <code>microk8s start</code> and <code>microk8s stop</code> will those tasks for you.</p> </li> <li> <p>To Reset the infrastructure to a clean state: <code>microk8s reset</code></p> </li> </ul>"},{"location":"other-tools/kubernetes/microk8s/#deploy-a-container-using-the-kubernetes-dashboard","title":"Deploy a Container using the Kubernetes-Dashboard","text":"<p>Click on the + button in the top left corner of the main window. On the resulting page, click Create from form and then fill out the necessary information as shown below:</p> <p></p> <p>You should immediately be directed to a page that lists your new deployment as shown below:</p> <p></p> <p>Go back to the terminal window and issue the command:</p> <pre><code>microk8s kubectl get svc tns -n kube-system\n\nNAME   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ntns    LoadBalancer   10.152.183.90   &lt;pending&gt;     8080:30012/TCP   14m\n</code></pre> <p>Go to browser, visit <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.26.4:30012/ to check the nginx default page.</p>"},{"location":"other-tools/kubernetes/microk8s/#deploy-a-sample-nginx-application","title":"Deploy A Sample Nginx Application","text":"<ul> <li> <p>Create an alias:</p> <pre><code>alias mkctl=\"microk8s kubectl\"\n</code></pre> </li> <li> <p>Create a deployment, in this case Nginx:</p> <pre><code>mkctl create deployment --image nginx my-nginx\n</code></pre> </li> <li> <p>To access the deployment we will need to expose it:</p> <pre><code>mkctl expose deployment my-nginx --port=80 --type=NodePort\n</code></pre> <pre><code>mkctl get svc my-nginx\n\nNAME       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nmy-nginx   NodePort   10.152.183.41   &lt;none&gt;        80:31225/TCP   35h\n</code></pre> </li> </ul> <p>Go to browser, visit <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.26.4:31225/ to check the nginx default page.</p>"},{"location":"other-tools/kubernetes/microk8s/#deploy-another-application","title":"Deploy Another Application","text":"<p>You can start by creating a microbot deployment with two pods via the kubectl cli:</p> <pre><code>mkctl create deployment microbot --image=dontrebootme/microbot:v1\nmkctl scale deployment microbot --replicas=2\n</code></pre> <p>To expose the deployment to NodePort, you need to create a service:</p> <pre><code>mkctl expose deployment microbot --type=NodePort --port=80 --name=microbot-service\n</code></pre> <p>View the port information:</p> <pre><code>mkctl get svc microbot-service\n\nNAME               TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nmicrobot-service   NodePort   10.152.183.8   &lt;none&gt;        80:31442/TCP   35h\n</code></pre> <p>Go to browser, visit <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.26.4:31442/ to check the microbot default page.</p> <p></p>"},{"location":"other-tools/kubernetes/minikube/","title":"Minikube","text":""},{"location":"other-tools/kubernetes/minikube/#minikube","title":"Minikube","text":""},{"location":"other-tools/kubernetes/minikube/#minimum-system-requirements-for-minikube","title":"Minimum system requirements for minikube","text":"<ul> <li>2 GB RAM or more</li> <li>2 CPU / vCPUs or more</li> <li>20 GB free hard disk space or more</li> <li>Docker / Virtual Machine Manager \u2013 KVM &amp; VirtualBox. Docker, Hyperkit, Hyper-V,     KVM, Parallels, Podman, VirtualBox, or VMWare are examples of container or virtual     machine managers.</li> </ul>"},{"location":"other-tools/kubernetes/minikube/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 VM to create a single node kubernetes cluster using <code>minikube</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine for master, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu OS     image, <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage - also     assign Floating IP     to this VM.</p> </li> <li> <p>setup Unique hostname to the machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.62 minikube\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname minikube\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#install-minikube-on-ubuntu","title":"Install Minikube on Ubuntu","text":"<p>Run the below command on the Ubuntu VM:</p> <p>Very Important</p> <p>Run the following steps as non-root user i.e. ubuntu.</p> <ul> <li> <p>SSH into minikube machine</p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>Install <code>curl</code>, <code>wget</code>, and <code>apt-transport-https</code></p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y curl wget apt-transport-https\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#download-and-install-the-latest-version-of-docker-ce","title":"Download and install the latest version of Docker CE","text":"<ul> <li> <p>Download and install Docker CE:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> </li> <li> <p>Configure the Docker daemon:</p> <pre><code>sudo usermod -aG docker $USER &amp;&amp; newgrp docker\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#install-kubectl","title":"Install kubectl","text":"<ul> <li> <p>Install kubectl binary</p> <p>kubectl: the command line util to talk to your cluster.</p> <pre><code>sudo snap install kubectl --classic\n</code></pre> <p>This outputs:</p> <pre><code>kubectl 1.26.1 from Canonical\u2713 installed\n</code></pre> </li> <li> <p>Now verify the kubectl version:</p> <pre><code>sudo kubectl version -o yaml\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#install-the-container-runtime-ie-containerd-on-master-and-worker-nodes","title":"Install the container runtime i.e. containerd on master and worker nodes","text":"<p>To run containers in Pods, Kubernetes uses a container runtime.</p> <p>By default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.</p> <ul> <li> <p>Install container runtime - containerd</p> <p>The first thing to do is configure the persistent loading of the necessary <code>containerd</code> modules. This forwarding IPv4 and letting iptables see bridged trafficis is done with the following command:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre> </li> <li> <p>Ensure <code>net.bridge.bridge-nf-call-iptables</code> is set to <code>1</code> in your sysctl config:</p> <pre><code># sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n</code></pre> </li> <li> <p>Apply sysctl params without reboot:</p> <pre><code>sudo sysctl --system\n</code></pre> </li> <li> <p>Install the necessary dependencies with:</p> <pre><code>sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates\n</code></pre> </li> <li> <p>The <code>containerd.io</code> packages in DEB and RPM formats are distributed by Docker.     Add the required GPG key with:</p> <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n</code></pre> <p>It's now time to Install and configure containerd:</p> <pre><code>sudo apt update -y\nsudo apt install -y containerd.io\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# Reload the systemd daemon with\nsudo systemctl daemon-reload\n\n# Start containerd\nsudo systemctl restart containerd\nsudo systemctl enable --now containerd\n</code></pre> <p>You can verify <code>containerd</code> is running with the command:</p> <pre><code>sudo systemctl status containerd\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#installing-minikube","title":"Installing minikube","text":"<ul> <li> <p>Install minikube</p> <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb\nsudo dpkg -i minikube_latest_amd64.deb\n</code></pre> <p>OR, install minikube using <code>wget</code>:</p> <pre><code>wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\ncp minikube-linux-amd64 /usr/bin/minikube\nchmod +x /usr/bin/minikube\n</code></pre> </li> <li> <p>Verify the Minikube installation:</p> <pre><code>minikube version\n\nminikube version: v1.29.0\ncommit: ddac20b4b34a9c8c857fc602203b6ba2679794d3\n</code></pre> </li> <li> <p>Install conntrack:</p> <p>Kubernetes 1.26.1 requires conntrack to be installed in root's path:</p> <pre><code>sudo apt-get install -y conntrack\n</code></pre> </li> <li> <p>Start minikube:</p> <p>As we are already stated in the beginning that we would be using docker as base for minikue, so start the minikube with the docker driver,</p> <pre><code>minikube start --driver=docker --container-runtime=containerd\n</code></pre> <p>Note</p> <ul> <li> <p>To check the internal IP, run the <code>minikube ip</code> command.</p> </li> <li> <p>By default, Minikube uses the driver most relevant to the host OS. To use a different driver, set the <code>--driver</code> flag in <code>minikube start</code>. For example, to use others or none instead of Docker, run <code>minikube start --driver=none</code>. To persistent configuration so that you to run minikube start without explicitly passing i.e. in global scope the <code>--vm-driver docker</code> flag each time, run: <code>minikube config set vm-driver docker</code>.</p> </li> <li> <p>Other start options: <code>minikube start --force --driver=docker --network-plugin=cni --container-runtime=containerd</code></p> </li> <li> <p>In case you want to start minikube with customize resources and want installer to automatically select the driver then you can run following command, <code>minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g</code></p> </li> </ul> <p>Output would like below:</p> <p></p> <p>Perfect, above confirms that minikube cluster has been configured and started successfully.</p> </li> <li> <p>Run below minikube command to check status:</p> <pre><code>minikube status\n\nminikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n</code></pre> </li> <li> <p>Run following kubectl command to verify the cluster info and node status:</p> <pre><code>kubectl cluster-info\n\nKubernetes control plane is running at https://192.168.0.62:8443\nCoreDNS is running at https://192.168.0.62:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <pre><code>kubectl get nodes\n\nNAME       STATUS   ROLES                  AGE   VERSION\nminikube   Ready    control-plane,master   5m    v1.26.1\n</code></pre> </li> <li> <p>To see the kubectl configuration use the command:</p> <pre><code>kubectl config view\n</code></pre> <p>The output looks like:</p> <p></p> </li> <li> <p>Get minikube addon details:</p> <pre><code>minikube addons list\n</code></pre> <p>The output will display like below: </p> <p>If you wish to enable any addons run the below minikube command,</p> <pre><code>minikube addons enable &lt;addon-name&gt;\n</code></pre> </li> <li> <p>Enable minikube dashboard addon:</p> <pre><code>minikube dashboard\n\n\ud83d\udd0c  Enabling dashboard ...\n    \u25aa Using image kubernetesui/metrics-scraper:v1.0.7\n    \u25aa Using image kubernetesui/dashboard:v2.3.1\n\ud83e\udd14  Verifying dashboard health ...\n\ud83d\ude80  Launching proxy ...\n\ud83e\udd14  Verifying proxy health ...\nhttp://127.0.0.1:40783/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/\n</code></pre> </li> <li> <p>To view minikube dashboard url:</p> <pre><code>minikube dashboard --url\n\n\ud83e\udd14  Verifying dashboard health ...\n\ud83d\ude80  Launching proxy ...\n\ud83e\udd14  Verifying proxy health ...\nhttp://127.0.0.1:42669/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/\n</code></pre> </li> <li> <p>Expose Dashboard on NodePort instead of ClusterIP:</p> <p>-- Check the current port for <code>kubernetes-dashboard</code>:</p> <pre><code>kubectl get services -n kubernetes-dashboard\n</code></pre> <p>The output looks like below:</p> <p></p> <pre><code>kubectl edit service kubernetes-dashboard -n kubernetes-dashboard\n</code></pre> <p>-- Replace type: \"ClusterIP\" to \"NodePort\":</p> <p></p> <p>-- After saving the file: Test again: <code>kubectl get services -n kubernetes-dashboard</code></p> <p>Now the output should look like below:</p> <p></p> <p>So, now you can browser the K8s Dashboard, visit <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://140.247.152.235:31881 to view the Dashboard.</p> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#deploy-a-sample-nginx-application","title":"Deploy A Sample Nginx Application","text":"<ul> <li> <p>Create a deployment, in this case Nginx:</p> <p>A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods.</p> </li> <li> <p>Let's check if the Kubernetes cluster is up and running:</p> <pre><code>kubectl get all --all-namespaces\nkubectl get po -A\nkubectl get nodes\n</code></pre> <pre><code>kubectl create deployment --image nginx my-nginx\n</code></pre> </li> <li> <p>To access the deployment we will need to expose it:</p> <pre><code>kubectl expose deployment my-nginx --port=80 --type=NodePort\n</code></pre> <p>To check which NodePort is opened and running the Nginx run:</p> <pre><code>kubectl get svc\n</code></pre> <p>The output will show:</p> <p></p> <p>OR,</p> <pre><code>minikube service list\n\n|----------------------|---------------------------|--------------|-------------|\n|      NAMESPACE       |           NAME            | TARGET PORT  |       URL   |\n|----------------------|---------------------------|--------------|-------------|\n| default              | kubernetes                | No node port |\n| default              | my-nginx                  |           80 | http:.:31081|\n| kube-system          | kube-dns                  | No node port |\n| kubernetes-dashboard | dashboard-metrics-scraper | No node port |\n| kubernetes-dashboard | kubernetes-dashboard      |           80 | http:.:31929|\n|----------------------|---------------------------|--------------|-------------|\n</code></pre> <p>OR,</p> <pre><code>kubectl get svc my-nginx\nminikube service my-nginx --url\n</code></pre> <p>Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the node's Floating IP.</p> <p>Go to browser, visit <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://140.247.152.235:31081/ to check the nginx default page.</p> <p>For your example,</p> <p></p> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#deploy-a-hello-minikube-application","title":"Deploy A Hello Minikube Application","text":"<ul> <li> <p>Use the kubectl create command to create a Deployment that manages a Pod. The     Pod runs a Container based on the provided Docker image.</p> <pre><code>kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\n</code></pre> </li> <li> <p>View the port information:</p> <pre><code>kubectl get svc hello-minikube\nminikube service hello-minikube --url\n</code></pre> <p>Go to browser, visit <code>http://&lt;Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://140.247.152.235:31293/ to check the hello minikube default page.</p> <p>For your example,</p> <p></p> </li> </ul>"},{"location":"other-tools/kubernetes/minikube/#clean-up","title":"Clean up","text":"<p>Now you can clean up the app resources you created in your cluster:</p> <pre><code>kubectl delete service my-nginx\nkubectl delete deployment my-nginx\n\nkubectl delete service hello-minikube\nkubectl delete deployment hello-minikube\n</code></pre>"},{"location":"other-tools/kubernetes/minikube/#managing-minikube-cluster","title":"Managing Minikube Cluster","text":"<ul> <li> <p>To stop the minikube, run</p> <pre><code>minikube stop\n</code></pre> </li> <li> <p>To delete the single node cluster:</p> <pre><code>minikube delete\n</code></pre> </li> <li> <p>To Start the minikube, run</p> <pre><code>minikube start\n</code></pre> </li> <li> <p>Remove the Minikube configuration and data directories:</p> <pre><code>rm -rf ~/.minikube\nrm -rf ~/.kube\n</code></pre> </li> <li> <p>If you have installed any Minikube related packages, remove them:</p> <pre><code>sudo apt remove -y conntrack\n</code></pre> </li> <li> <p>In case you want to start the minikube with higher resource like 8 GB RM and     4 CPU then execute following commands one after the another.</p> <pre><code>minikube config set cpus 4\nminikube config set memory 8192\nminikube delete\nminikube start\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/","title":"Multi-master K3s cluster setup using k3d","text":""},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#set-up-k3s-in-high-availability-using-k3d","title":"Set up K3s in High Availability using k3d","text":"<p>First, Kubernetes HA has two possible setups: embedded or external database (DB). We'll use the embedded DB in this HA K3s cluster setup. For which <code>etcd</code> is the default embedded DB.</p> <p>There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#pre-requisite","title":"Pre-requisite","text":"<p>Make sure you have already installed k3d following this.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#ha-cluster-with-at-least-three-control-plane-nodes","title":"HA cluster with at least three control plane nodes","text":"<pre><code>k3d cluster create --servers 3 --image rancher/k3s:latest\n</code></pre> <p>Here, <code>--server 3</code>: specifies requests three nodes to be created with the role server and <code>--image rancher/k3s:latest</code>: specifies the K3s image to be used here we are using <code>latest</code></p> <ul> <li> <p>Switch context to the new cluster:</p> <pre><code>kubectl config use-context k3d-k3s-default\n</code></pre> <p>You can now check what has been created from the different points of view:</p> <pre><code>kubectl get nodes --output wide\n</code></pre> <p>The output will look like:</p> <p></p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> <p>The output will look like: </p> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#scale-up-the-cluster","title":"Scale up the cluster","text":"<p>You can quickly simulate the addition of another control plane node to the HA cluster:</p> <pre><code>k3d node create extraCPnode --role=server --image=rancher/k3s:latest\n\nINFO[0000] Adding 1 node(s) to the runtime local cluster 'k3s-default'...\nINFO[0000] Starting Node 'k3d-extraCPnode-0'\nINFO[0018] Updating loadbalancer config to include new server node(s)\nINFO[0018] Successfully configured loadbalancer k3d-k3s-default-serverlb!\nINFO[0019] Successfully created 1 node(s)!\n</code></pre> <p>Here, <code>extraCPnode</code>: specifies the name for the node, <code>--role=server</code> : sets the role for the node to be a control plane/server, <code>--image rancher/k3s:latest</code>: specifies the K3s image to be used here we are using <code>latest</code></p> <pre><code>kubectl get nodes\n\nNAME                       STATUS   ROLES         AGE   VERSION\nk3d-extracpnode-0          Ready    etcd,master   31m   v1.19.3+k3s2\nk3d-k3s-default-server-0   Ready    etcd,master   47m   v1.19.3+k3s2\nk3d-k3s-default-server-1   Ready    etcd,master   47m   v1.19.3+k3s2\nk3d-k3s-default-server-2   Ready    etcd,master   47m   v1.19.3+k3s2\n</code></pre> <p>OR,</p> <pre><code>kubectl get nodes --output wide\n</code></pre> <p>The output looks like below:</p> <p></p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#heavy-armored-against-crashes","title":"Heavy Armored against crashes","text":"<p>As we are working with containers, the best way to \"crash\" a node is to literally stop the container:</p> <pre><code>docker stop k3d-k3s-default-server-0\n</code></pre> <p>Note</p> <p>The Docker and k3d commands will show the state change immediately. However, the Kubernetes (read: K8s or K3s) cluster needs a short time to see the state change to NotReady.</p> <pre><code>kubectl get nodes\n\nNAME                       STATUS     ROLES         AGE   VERSION\nk3d-extracpnode-0          Ready      etcd,master   32m   v1.19.3+k3s2\nk3d-k3s-default-server-0   NotReady   etcd,master   48m   v1.19.3+k3s2\nk3d-k3s-default-server-1   Ready      etcd,master   48m   v1.19.3+k3s2\nk3d-k3s-default-server-2   Ready      etcd,master   48m   v1.19.3+k3s2\n</code></pre> <p>Now it is a good time to reference again the load balancer k3d uses and how it is critical in allowing us to continue accessing the K3s cluster.</p> <p>While the load balancer internally switched to the next available node, from an external connectivity point of view, we still use the same IP/host. This abstraction saves us quite some efforts and it's one of the most useful features of k3d.</p> <p>Let's look at the state of the cluster:</p> <pre><code>kubectl get all --all-namespaces\n</code></pre> <p>The output looks like below:</p> <p></p> <p>Everything looks right. If we look at the pods more specifically, then we will see that K3s automatically self-healed by recreating pods running on the failed node on other nodes:</p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>As the output can be seen:</p> <p></p> <p>Finally, to show the power of HA and how K3s manages it, let's restart the node0 and see it being re-included into the cluster as if nothing happened:</p> <pre><code>docker start k3d-k3s-default-server-0\n</code></pre> <p>Our cluster is stable, and all the nodes are fully operational again as shown below: </p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster-using-k3d/#cleaning-the-resources","title":"Cleaning the resources","text":"<pre><code>k3d cluster delete\n</code></pre>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/","title":"K3s with High Availibility(HA) setup","text":""},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#k3s-with-high-availability-setup","title":"K3s with High Availability setup","text":"<p>First, Kubernetes HA has two possible setups: embedded or external database (DB). We'll use the external DB in this HA K3s cluster setup. For which <code>MySQL</code> is the external DB as shown here: </p> <p>In the diagram above, both the user running <code>kubectl</code> and each of the two agents connect to the TCP Load Balancer. The Load Balancer uses a list of private IP addresses to balance the traffic between the three servers. If one of the servers crashes, it is be removed from the list of IP addresses.</p> <p>The servers use the SQL data store to synchronize the cluster's state.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#requirements","title":"Requirements","text":"<p>i. Managed TCP Load Balancer</p> <p>ii. Managed MySQL service</p> <p>iii. Three VMs to run as K3s servers</p> <p>iv. Two VMs to run as K3s agents</p> <p>There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#managed-tcp-load-balancer","title":"Managed TCP Load Balancer","text":"<p>Create a load balancer using <code>nginx</code>: The <code>nginx.conf</code> located at <code>etc/nginx/nginx.conf</code> contains upstream that is pointing to the 3 K3s Servers on port 6443 as shown below:</p> <pre><code>events {}\n...\n\nstream {\n  upstream k3s_servers {\n    server &lt;k3s_server1-Internal-IP&gt;:6443;\n    server &lt;k3s_server2-Internal-IP&gt;:6443;\n    server &lt;k3s_server3-Internal-IP&gt;:6443;\n  }\n\n  server {\n    listen 6443;\n    proxy_pass k3s_servers;\n  }\n}\n</code></pre>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#managed-mysql-service","title":"Managed MySQL service","text":"<p>Create a MySQL database server with a new database and create a new mysql user and password with granted permission to read/write the new database. In this example, you can create:</p> <p>database name: <code>&lt;YOUR_DB_NAME&gt;</code> database user: <code>&lt;YOUR_DB_USER_NAME&gt;</code> database password: <code>&lt;YOUR_DB_USER_PASSWORD&gt;</code></p>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#three-vms-to-run-as-k3s-servers","title":"Three VMs to run as K3s servers","text":"<p>Create 3 K3s Master VMs and perform the following steps on each of them:</p> <p>i. Export the datastore endpoint:</p> <pre><code>export K3S_DATASTORE_ENDPOINT='mysql://&lt;YOUR_DB_USER_NAME&gt;:&lt;YOUR_DB_USER_PASSWORD&gt;@tcp(&lt;MySQL-Server-Internal-IP&gt;:3306)/&lt;YOUR_DB_NAME&gt;'\n</code></pre> <p>ii. Install the K3s with setting not to deploy any pods on this server (opposite of affinity) unless critical addons and <code>tls-san</code> set <code>&lt;Loadbalancer-Internal-IP&gt;</code> as alternative name for that tls certificate.</p> <pre><code>curl -sfL https://get.k3s.io | sh -s - server \\\n    --node-taint CriticalAddonsOnly=true:NoExecute \\\n    --tls-san &lt;Loadbalancer-Internal-IP_or_Hostname&gt;\n</code></pre> <ul> <li> <p>Verify all master nodes are visible to one another:</p> <pre><code>sudo k3s kubectl get node\n</code></pre> </li> <li> <p>Generate token from one of the K3s Master VMs:     You need to extract a token from the master that will be used to join the nodes     to the control plane by running following command on one of the K3s master node:</p> <pre><code>sudo cat /var/lib/rancher/k3s/server/node-token\n</code></pre> <p>You will then obtain a token that looks like:</p> <pre><code>K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#two-vms-to-run-as-k3s-agents","title":"Two VMs to run as K3s agents","text":"<p>Set the <code>K3S_URL</code> to point to the Loadbalancer's internal IP and set the <code>K3S_TOKEN</code> from the clipboard on both of the agent nodes:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://&lt;Loadbalancer-Internal-IP_or_Hostname&gt;:6443\n    K3S_TOKEN=&lt;Token_From_Master&gt; sh -\n</code></pre> <p>Once both Agents are running, if you run the following command on Master Server, you can see all nodes:</p> <pre><code>sudo k3s kubectl get node\n</code></pre>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#simulate-a-failure","title":"Simulate a failure","text":"<p>To simulate a failure, stop the K3s service on one or more of the K3s servers manually, then run the <code>kubectl get nodes</code> command:</p> <pre><code>sudo systemctl stop k3s\n</code></pre> <p>The third server will take over at this point.</p> <ul> <li> <p>To restart servers manually:</p> <pre><code>sudo systemctl restart k3s\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#on-your-local-development-machine-to-access-kubernetes-cluster-remotely-optional","title":"On your local development machine to access Kubernetes Cluster Remotely (Optional)","text":"<p>Important Requirement</p> <p>Your local development machine must have installed <code>kubectl</code>.</p> <ul> <li> <p>Copy kubernetes config to your local machine:     Copy the <code>kubeconfig</code> file's content located at the K3s master node at <code>/etc/rancher/k3s/k3s.yaml</code>     to your local machine's <code>~/.kube/config</code> file. Before saving, please change the     cluster server path from 127.0.0.1 to <code>&lt;Loadbalancer-Internal-IP&gt;</code>. This     will allow your local machine to see the cluster nodes:</p> <pre><code>kubectl get nodes\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>The Kubernetes Dashboard is a GUI tool to help you work more efficiently with K8s cluster. This is only accessible from within the cluster (at least not without some serious tweaking).</p> <p>check releases for the command to use for Installation:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml\n</code></pre> <ul> <li> <p>Dashboard RBAC Configuration:</p> <p><code>dashboard.admin-user.yml</code></p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n</code></pre> <p><code>dashboard.admin-user-role.yml</code></p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n</code></pre> </li> <li> <p>Deploy the <code>admin-user</code> configuration:</p> <pre><code>sudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml\n</code></pre> <p>Important Note</p> <p>If you're doing this from your local development machine, remove <code>sudo k3s</code> and just use <code>kubectl</code>)</p> </li> <li> <p>Get bearer token</p> <pre><code>sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token \\\n  | grep ^token\n</code></pre> </li> <li> <p>Start dashboard locally:</p> <pre><code>sudo k3s kubectl proxy\n</code></pre> <p>Then you can sign in at this URL using your token we got in the previous step:</p> <pre><code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-ha-cluster/#deploying-nginx-using-deployment","title":"Deploying Nginx using deployment","text":"<ul> <li> <p>Create a deployment <code>nginx.yaml</code>:</p> <pre><code>vi nginx.yaml\n</code></pre> </li> <li> <p>Copy and paste the following content in <code>nginx.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysite\n  labels:\n    app: mysite\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysite\n  template:\n    metadata:\n      labels:\n        app : mysite\n    spec:\n      containers:\n        - name : mysite\n          image: nginx\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>sudo k3s kubectl apply -f nginx.yaml\n</code></pre> </li> <li> <p>Verify the nginx pod is in Running state:</p> <pre><code>sudo k3s kubectl get pods --all-namespaces\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> </li> <li> <p>Scale the pods to available agents:</p> <pre><code>sudo k3s kubectl scale --replicas=2 deploy/mysite\n</code></pre> </li> <li> <p>View all deployment status:</p> <pre><code>sudo k3s kubectl get deploy mysite\n\nNAME     READY   UP-TO-DATE   AVAILABLE   AGE\nmysite   2/2     2            2           85s\n</code></pre> </li> <li> <p>Delete the nginx deployment and pod:</p> <pre><code>sudo k3s kubectl delete -f nginx.yaml\n</code></pre> <p>OR,</p> <pre><code>sudo k3s kubectl delete deploy mysite\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/","title":"Single-Node K3s Cluster using k3d","text":""},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#setup-k3s-cluster-using-k3d","title":"Setup K3s cluster Using k3d","text":"<p>One of the most popular and second method of creating k3s cluster is by using <code>k3d</code>. By the name itself it suggests, <code>K3s-in-docker</code>, is a wrapper around K3s \u2013 Lightweight Kubernetes that runs it in docker. Please refer to this link to get brief insights of this wonderful tool. It provides a seamless experience working with K3s cluster management with some straight forward commands. k3d is efficient enough to create and manage K3s single node and well as K3s High Availability clusters just with few commands.</p> <p>Note</p> <p>For using <code>k3d</code> you must have docker installed in your system</p>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#install-docker","title":"Install Docker","text":"<ul> <li> <p>Install container runtime - docker</p> <pre><code>apt-get install docker.io -y\n</code></pre> </li> <li> <p>Configure the Docker daemon, in particular to use systemd for the management     of the container's cgroups</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json\n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\nEOF\n\nsystemctl enable --now docker\nusermod -aG docker ubuntu\nsystemctl daemon-reload\nsystemctl restart docker\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#install-kubectl","title":"Install kubectl","text":"<ul> <li> <p>Install kubectl binary</p> <p>kubectl: the command line util to talk to your cluster.</p> <pre><code>snap install kubectl --classic\n</code></pre> <p>This outputs:</p> <pre><code>kubectl 1.26.1 from Canonical\u2713 installed\n</code></pre> </li> <li> <p>Now verify the kubectl version:</p> <pre><code>kubectl version -o yaml\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#installing-k3d","title":"Installing k3d","text":""},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#k3d-installation","title":"k3d Installation","text":"<p>The below command will install the k3d, in your system using the installation script.</p> <pre><code>wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash\n</code></pre> <p>OR,</p> <pre><code>curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash\n</code></pre> <p>To verify the installation, please run the following command:</p> <pre><code>k3d version\n\nk3d version v5.0.0\nk3s version v1.21.5-k3s1 (default)\n</code></pre> <p>After the successful installation, you are ready to create your cluster using k3d and run K3s in docker within seconds.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#getting-started","title":"Getting Started","text":"<p>Now let's directly jump into creating our K3s cluster using <code>k3d</code>.</p> <ol> <li> <p>Create k3d Cluster:</p> <pre><code>k3d cluster create k3d-demo-cluster\n</code></pre> <p>This single command spawns a K3s cluster with two containers: A Kubernetes control-plane node(server) and a load balancer(serverlb) in front of it. It puts both of them in a dedicated Docker network and exposes the Kubernetes API on a randomly chosen free port on the Docker host. It also creates a named Docker volume in the background as a preparation for image imports.</p> <p>You can also look for advance syntax for cluster creation:</p> <pre><code>k3d cluster create mycluster --api-port 127.0.0.1:6445 --servers 3 \\\n    --agents 2 --volume '/home/me/mycode:/code@agent[*]' --port '8080:80@loadbalancer'\n</code></pre> <p>Here, the above single command spawns a K3s cluster with six containers:</p> <ul> <li> <p>load balancer</p> </li> <li> <p>3 servers (control-plane nodes)</p> </li> <li> <p>2 agents (formerly worker nodes)</p> </li> </ul> <p>With the <code>--api-port 127.0.0.1:6445</code>, you tell k3d to map the Kubernetes API Port (6443 internally) to <code>127.0.0.1/localhost</code>'s port 6445. That means that you will have this connection string in your Kubeconfig: <code>server: https://127.0.0.1:6445</code> to connect to this cluster.</p> <p>This port will be mapped from the load balancer to your host system. From there, requests will be proxied to your server nodes, effectively simulating a production setup, where server nodes also can go down and you would want to failover to another server.</p> <p>The <code>--volume /home/me/mycode:/code@agent[*]</code> bind mounts your local directory <code>/home/me/mycode</code> to the path <code>/code</code> inside all (<code>[*]</code> of your agent nodes). Replace * with an index (here: 0 or 1) to only mount it into one of them.</p> <p>The specification telling k3d which nodes it should mount the volume to is called \"node filter\" and it's also used for other flags, like the <code>--port</code> flag for port mappings.</p> <p>That said, <code>--port '8080:80@loadbalancer'</code> maps your local host's port 8080 to port 80 on the load balancer (serverlb), which can be used to forward HTTP ingress traffic to your cluster. For example, you can now deploy a web app into the cluster (Deployment), which is exposed (Service) externally via an Ingress such as <code>myapp.k3d.localhost</code>.</p> <p>Then (provided that everything is set up to resolve that domain to your localhost IP), you can point your browser to <code>http://myapp.k3d.localhost:8080</code> to access your app. Traffic then flows from your host through the Docker bridge interface to the load balancer. From there, it's proxied to the cluster, where it passes via Ingress and Service to your application Pod.</p> <p>Note</p> <p>You have to have some mechanism set up to route to resolve <code>myapp.k3d.localhost</code> to your local host IP (<code>127.0.0.1</code>). The most common way is using entries of the form <code>127.0.0.1</code> <code>myapp.k3d.localhost</code> in your <code>/etc/hosts</code> file (<code>C:\\Windows\\System32\\drivers\\etc\\hosts</code> on Windows). However, this does not allow for wildcard entries (<code>*.localhost</code>), so it may become a bit cumbersome after a while, so you may want to have a look at tools like <code>dnsmasq</code> (MacOS/UNIX) or <code>Acrylic</code> (Windows)  to ease the burden.</p> </li> <li> <p>Getting the cluster's kubeconfig:     Get the new cluster's connection details merged into your default kubeconfig     (usually specified using the <code>KUBECONFIG</code> environment variable or the default     path <code>$HOME/.kube/config</code>) and directly switch to the new context:</p> <pre><code>k3d kubeconfig merge k3d-demo-cluster --kubeconfig-switch-context\n</code></pre> <p>This outputs:</p> <pre><code>/root/.k3d/kubeconfig-k3d-demo-cluster.yaml\n</code></pre> </li> <li> <p>Checking the nodes running on k3d cluster:</p> <pre><code>k3d node list\n</code></pre> <p></p> <p>You can see here two nodes. The (very) smart implementation here is that while the cluster is running on its node k3d-k3s-default-server-0, there is another \"node\" that acts as the load balancer i.e. k3d-k3d-demo-cluster-serverlb.</p> </li> <li> <p>Firing Kubectl commands that allows you to run commands against Kubernetes:</p> <p>i. The below command will list the nodes available in our cluster:</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get nodes --output wide\n</code></pre> <p>The output will look like:</p> <p></p> <p>ii. To look at what's inside the K3s cluster (pods, services, deployments, etc.):</p> <pre><code>kubectl get all --all-namespaces\n</code></pre> <p>The output will look like:</p> <p></p> <p>We can see that, in addition to the Kubernetes service, K3s deploys DNS, metrics and ingress (traefik) services when you use the defaults.</p> <p>iii. List the active k3d clusters:</p> <pre><code>k3d cluster list\n</code></pre> <p></p> <p>iv. Check the cluster connectivity:</p> <pre><code>kubectl cluster-info\n</code></pre> <p></p> <p>To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</p> </li> <li> <p>Check the active containers:</p> <pre><code>docker ps\n</code></pre> </li> </ol> <p>Now as you can observe, the cluster is up and running and we can play around with the cluster, you can create and deploy your applications over the cluster.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3d/#deleting-cluster","title":"Deleting Cluster","text":"<pre><code>k3d cluster delete k3d-demo-cluster\n\nINFO[0000] Deleting cluster 'k3d-demo-cluster'\nINFO[0000] Deleted k3d-k3d-demo-cluster-serverlb\nINFO[0001] Deleted k3d-k3d-demo-cluster-server-0\nINFO[0001] Deleting cluster network 'k3d-k3d-demo-cluster'\nINFO[0001] Deleting image volume 'k3d-k3d-demo-cluster-images'\nINFO[0001] Removing cluster details from default kubeconfig...\nINFO[0001] Removing standalone kubeconfig file (if there is one)...\nINFO[0001] Successfully deleted cluster k3d-demo-cluster!\n</code></pre> <p>You can also create a k3d High Availability cluster and add as many nodes you want within seconds.</p>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/","title":"Multi-master HA K3s cluster using k3sup","text":""},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/#k3s-cluster-setup-using-k3sup","title":"K3s cluster setup using k3sup","text":"<p>k3sup (pronounced ketchup) is a popular open source tool to install K3s over SSH.</p> <ul> <li>Bootstrap the cluster     </li> </ul> <p>The two most important commands in k3sup are:</p> <p>i. install: install K3s to a new server and create a <code>join token</code> for the cluster</p> <p>ii. join: fetch the <code>join token</code> from a server, then use it to install K3s to an agent</p>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/#download-k3sup","title":"Download k3sup","text":"<pre><code>curl -sLS https://get.k3sup.dev | sh\nsudo install k3sup /usr/bin/\n\nk3sup --help\n</code></pre> <ul> <li> <p>Other options for <code>install</code>:</p> <p><code>--cluster</code> - start this server in clustering mode using embedded etcd (embedded HA)</p> <p><code>--skip-install</code> - if you already have k3s installed, you can just run this command to get the kubeconfig</p> <p><code>--ssh-key</code> - specify a specific path for the SSH key for remote login</p> <p><code>--local-path</code> - default is <code>./kubeconfig</code> - set the file where you want to save your cluster's <code>kubeconfig</code>. By default this file will be overwritten.</p> <p><code>--merge</code> - Merge config into existing file instead of overwriting (e.g. to add config to the default kubectl config, use <code>--local-path ~/.kube/config --merge</code>).</p> <p><code>--context</code> - default is default - set the name of the kubeconfig context.</p> <p><code>--ssh-port</code> - default is 22, but you can specify an alternative port i.e. <code>2222</code></p> <p><code>--k3s-extra-args</code> - Optional extra arguments to pass to k3s installer, wrapped in quotes, i.e. <code>--k3s-extra-args '--no-deploy traefik'</code> or <code>--k3s-extra-args '--docker'</code>. For multiple args combine then within single quotes <code>--k3s-extra-args</code></p> <p><code>--no-deploy traefik --docker</code>.</p> <p><code>--k3s-version</code> - set the specific version of k3s, i.e. v0.9.1</p> <p><code>--ipsec</code> - Enforces the optional extra argument for k3s: <code>--flannel-backend</code> option: <code>ipsec</code></p> <p><code>--print-command</code> - Prints out the command, sent over SSH to the remote computer</p> <p><code>--datastore</code> - used to pass a SQL connection-string to the <code>--datastore-endpoint</code> flag of k3s.</p> <p>See even more install options by running <code>k3sup install --help</code>.</p> </li> <li> <p>On Master Node:</p> <pre><code>export SERVER_IP=&lt;Master-Internal-IP&gt;\nexport USER=root\n\nk3sup install --ip $SERVER_IP --user $USER\n</code></pre> </li> <li> <p>On Agent Node:</p> <p>Next join one or more <code>agents</code> to the cluster:</p> <pre><code>export AGENT_IP=&lt;Agent-Internal-IP&gt;\n\nexport SERVER_IP=&lt;Master-Internal-IP&gt;\nexport USER=root\n\nk3sup join --ip $AGENT_IP --server-ip $SERVER_IP --user $USER\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s-using-k3sup/#create-a-multi-master-ha-setup-with-external-sql","title":"Create a multi-master (HA) setup with external SQL","text":"<pre><code>export LB_IP='&lt;Loadbalancer-Internal-IP_or_Hostname&gt;'\nexport DATASTORE='mysql://&lt;YOUR_DB_USER_NAME&gt;:&lt;YOUR_DB_USER_PASSWORD&gt;@tcp(&lt;MySQL-Server-Internal-IP&gt;:3306)/&lt;YOUR_DB_NAME&gt;'\nexport CHANNEL=latest\n</code></pre> <p>Before continuing, check that your environment variables are still populated from earlier, and if not, trace back and populate them.</p> <pre><code>echo $LB_IP\necho $DATASTORE\necho $CHANNEL\n</code></pre> <pre><code>k3sup install --user root --ip $SERVER1 \\\n--k3s-channel $CHANNEL \\\n--print-command \\\n--datastore='${DATASTORE}' \\\n--tls-san $LB_IP\n\nk3sup install --user root --ip $SERVER2 \\\n--k3s-channel $CHANNEL \\\n--print-command \\\n--datastore='${DATASTORE}' \\\n--tls-san $LB_IP\n\nk3sup install --user root --ip $SERVER3 \\\n--k3s-channel $CHANNEL \\\n--print-command \\\n--datastore='${DATASTORE}' \\\n--tls-san $LB_IP\n\nk3sup join --user root --server-ip $LB_IP --ip $AGENT1 \\\n--k3s-channel $CHANNEL \\\n--print-command\n\nk3sup join --user root --server-ip $LB_IP --ip $AGENT2 \\\n--k3s-channel $CHANNEL \\\n--print-command\n</code></pre> <p>There will be a kubeconfig file created in the current working directory with the IP address of the LoadBalancer set for kubectl to use.</p> <ul> <li> <p>Check the nodes have joined:</p> <pre><code>export KUBECONFIG=`pwd`/kubeconfig\nkubectl get node\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/","title":"K3s","text":""},{"location":"other-tools/kubernetes/k3s/k3s/#k3s","title":"K3s","text":""},{"location":"other-tools/kubernetes/k3s/k3s/#features","title":"Features","text":"<ul> <li> <p>Lightweight certified K8s distro</p> </li> <li> <p>Built for production operations</p> </li> <li> <p>40MB binary, 250MB memeory consumption</p> </li> <li> <p>Single process w/ integrated K8s master, Kubelet, and containerd</p> </li> <li> <p>Supports not only <code>etcd</code> to hold the cluster state, but also <code>SQLite</code>     (for single-node, simpler setups) or external DBs like <code>MySQL</code> and <code>PostgreSQL</code></p> </li> <li> <p>Open source project</p> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#components-and-architecure","title":"Components and architecure","text":"<ul> <li> <p>High-Availability K3s Server with an External DB:</p> <p> or, </p> <p>For this kind of high availability k3s setup read this.</p> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 control-plane(master) and 2 worker nodes to create a single control-plane kubernetes cluster using <code>k3s</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine for master, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu OS     image, <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage - also     assign Floating IP     to the master node.</p> </li> <li> <p>2 Linux machines for worker, <code>ubuntu-22.04-x86_64</code> or your choice of Ubuntu OS     image, <code>cpu-su.1</code> flavor with 1vCPU, 4GB RAM, 20GB storage.</p> </li> <li> <p>ssh access to all machines: Read more here     on how to set up SSH on your remote VMs.</p> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#networking","title":"Networking","text":"<p>The K3s server needs port 6443 to be accessible by all nodes.</p> <p>The nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN overlay networking is used. The node should not listen on any other port. K3s uses reverse tunneling such that the nodes make outbound connections to the server and all kubelet traffic runs through that tunnel. However, if you do not use Flannel and provide your own custom CNI, then port 8472 is not needed by K3s.</p> <p>If you wish to utilize the metrics server, you will need to open port 10250 on each node.</p> <p>If you plan on achieving high availability with embedded etcd, server nodes must be accessible to each other on ports 2379 and 2380.</p> <ul> <li> <p>Create 1 security group with appropriate Inbound Rules for K3s Server Nodes     that will be used by all 3 nodes:</p> <p></p> <p>Important Note</p> <p>The VXLAN overlay networking port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472.</p> </li> <li> <p>setup Unique hostname to each machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.235 k3s-master\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname k3s-master\n</code></pre> </li> </ul> <p>In this step, you will setup the following nodes:</p> <ul> <li> <p>k3s-master</p> </li> <li> <p>k3s-worker1</p> </li> <li> <p>k3s-worker2</p> </li> </ul> <p>The below steps will be performed on all the above mentioned nodes:</p> <ul> <li> <p>SSH into all the 3 machines</p> </li> <li> <p>Switch as root: <code>sudo su</code></p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>apt-get update &amp;&amp; apt-get upgrade -y\n</code></pre> </li> <li> <p>Install <code>curl</code> and <code>apt-transport-https</code></p> <pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https curl\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#install-docker","title":"Install Docker","text":"<ul> <li> <p>Install container runtime - docker</p> <pre><code>apt-get install docker.io -y\n</code></pre> </li> <li> <p>Configure the Docker daemon, in particular to use systemd for the management     of the container's cgroups</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json\n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\nEOF\n\nsystemctl enable --now docker\nusermod -aG docker ubuntu\nsystemctl daemon-reload\nsystemctl restart docker\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#configure-k3s-to-bootstrap-the-cluster-on-master-node","title":"Configure K3s to bootstrap the cluster on master node","text":"<p>Run the below command on the master node i.e. <code>k3s-master</code> that you want to setup as control plane.</p> <ul> <li> <p>SSH into k3s-master machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Execute the below command to initialize the cluster:</p> <pre><code>curl -sfL https://get.k3s.io | sh -s - --kubelet-arg 'cgroup-driver=systemd' \\\n--node-taint CriticalAddonsOnly=true:NoExecute --docker\n</code></pre> <p>OR, If you don't want to setup the K3s cluster without using docker as the container runtime, then just run without supplying the <code>--docker</code> argument.</p> <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre> </li> </ul> <p>After running this installation:</p> <ul> <li> <p>The K3s service will be configured to automatically restart after node reboots     or if the process crashes or is killed</p> </li> <li> <p>Additional utilities will be installed, including <code>kubectl</code>, <code>crictl</code>, <code>ctr</code>,     <code>k3s-killall.sh</code>, and <code>k3s-uninstall.sh</code></p> </li> <li> <p>A kubeconfig file will be written to <code>/etc/rancher/k3s/k3s.yaml</code> and the <code>kubectl</code>     installed by K3s will automatically use it.</p> </li> </ul> <p>To check if the service installed successfully, you can use:</p> <pre><code>systemctl status k3s\n</code></pre> <p>The output looks like:</p> <p></p> <p>OR,</p> <pre><code>k3s --version\nkubectl version\n</code></pre> <p>Note</p> <p>If you want to taint the node i.e. not to deploy pods on this node after installation then run: <code>kubectl taint nodes &lt;master_node_name&gt; k3s-controlplane=true:NoExecute</code> i.e. <code>kubectl taint nodes k3s-master k3s-controlplane=true:NoExecute</code></p> <p>You can check if the master node is working by:</p> <pre><code>k3s kubectl get nodes\n\nNAME         STATUS   ROLES                  AGE   VERSION\nk3s-master   Ready    control-plane,master   37s   v1.21.5+k3s2\n</code></pre> <pre><code>kubectl config get-clusters\n\nNAME\ndefault\n</code></pre> <pre><code>kubectl cluster-info\n\nKubernetes control plane is running at https://127.0.0.1:6443\nCoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <pre><code>kubectl get namespaces\n\nNAME              STATUS   AGE\ndefault           Active   27m\nkube-system       Active   27m\nkube-public       Active   27m\nkube-node-lease   Active   27m\n</code></pre> <pre><code>kubectl get endpoints -n kube-system\n\nNAME                    ENDPOINTS                                  AGE\nkube-dns                10.42.0.4:53,10.42.0.4:53,10.42.0.4:9153   27m\nmetrics-server          10.42.0.3:443                              27m\nrancher.io-local-path   &lt;none&gt;                                     27m\n</code></pre> <pre><code>kubectl get pods -n kube-system\n\nNAME                                      READY   STATUS    RESTARTS   AGE\nhelm-install-traefik-crd-ql7j2            0/1     Pending   0          32m\nhelm-install-traefik-mr65j                0/1     Pending   0          32m\ncoredns-7448499f4d-x57z7                  1/1     Running   0          32m\nmetrics-server-86cbb8457f-cg2fs           1/1     Running   0          32m\nlocal-path-provisioner-5ff76fc89d-kdfcl   1/1     Running   0          32m\n</code></pre> <p>You need to extract a token from the master that will be used to join the nodes to the master.</p> <p>On the master node:</p> <pre><code>sudo cat /var/lib/rancher/k3s/server/node-token\n</code></pre> <p>You will then obtain a token that looks like:</p> <pre><code>K1097aace305b0c1077fc854547f34a598d2::server:6cc9fbb6c5c9de96f37fb14b8\n</code></pre>"},{"location":"other-tools/kubernetes/k3s/k3s/#configure-k3s-on-worker-nodes-to-join-the-cluster","title":"Configure K3s on worker nodes to join the cluster","text":"<p>Run the below command on both of the worker nodes i.e. <code>k3s-worker1</code> and <code>k3s-worker2</code> that you want to join the cluster.</p> <ul> <li> <p>SSH into k3s-worker1 and k3s-worker1 machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Execute the below command to join the cluster using the token obtained from     the master node:</p> <p>To install K3s on worker nodes and add them to the cluster, run the installation script with the <code>K3S_URL</code> and <code>K3S_TOKEN</code> environment variables. Here is an example showing how to join a worker node:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://&lt;Master-Internal-IP&gt;:6443 \\\nK3S_TOKEN=&lt;Join_Token&gt; sh -\n</code></pre> <p>Where <code>&lt;Master-Internal-IP&gt;</code> is the Internal IP of the master node and <code>&lt;Join_Token&gt;</code> is the token obtained from the master node.</p> <p>For example:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.154:6443 \\\nK3S_TOKEN=K1019827f88b77cc5e1dce04d692d445c1015a578dafdc56aca829b2f\n501df9359a::server:1bf0d61c85c6dac6d5a0081da55f44ba sh -\n</code></pre> <p>You can verify if the <code>k3s-agent</code> on both of the worker nodes is running by:</p> <pre><code>systemctl status k3s-agent\n</code></pre> <p>The output looks like:</p> <p></p> </li> </ul> <p>To verify that our nodes have successfully been added to the cluster, run the following command on master node:</p> <pre><code>k3s kubectl get nodes\n</code></pre> <p>OR,</p> <pre><code>k3s kubectl get nodes -o wide\n</code></pre> <p>Your output should look like:</p> <pre><code>k3s kubectl get nodes\n\nNAME          STATUS   ROLES                  AGE     VERSION\nk3s-worker1   Ready    &lt;none&gt;                 5m16s   v1.21.5+k3s2\nk3s-worker2   Ready    &lt;none&gt;                 5m5s    v1.21.5+k3s2\nk3s-master    Ready    control-plane,master   9m33s   v1.21.5+k3s2\n</code></pre> <p>This shows that we have successfully setup our K3s cluster ready to deploy applications to it.</p>"},{"location":"other-tools/kubernetes/k3s/k3s/#deploying-nginx-using-deployment","title":"Deploying Nginx using deployment","text":"<ul> <li> <p>Create a deployment <code>nginx.yaml</code> on master node</p> <pre><code>vi nginx.yaml\n</code></pre> <p>The nginx.yaml looks like this:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysite\n  labels:\n    app: mysite\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mysite\n  template:\n    metadata:\n      labels:\n        app : mysite\n    spec:\n      containers:\n        - name : mysite\n          image: nginx\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f nginx.yaml\n</code></pre> </li> <li> <p>Verify the nginx pod is in Running state:</p> <pre><code>sudo k3s kubectl get pods --all-namespaces\n</code></pre> </li> <li> <p>Scale the pods to available agents:</p> <pre><code>sudo k3s kubectl scale --replicas=2 deploy/mysite\n</code></pre> </li> <li> <p>View all deployment status:</p> <pre><code>sudo k3s kubectl get deploy mysite\n\nNAME     READY   UP-TO-DATE   AVAILABLE   AGE\nmysite   2/2     2            2           85s\n</code></pre> </li> <li> <p>Delete the nginx deployment and pod:</p> <pre><code>sudo k3s kubectl delete -f nginx.yaml\n</code></pre> <p>OR,</p> <pre><code>sudo k3s kubectl delete deploy mysite\n</code></pre> <p>Note</p> <p>Instead of apply manually any new deployment yaml, you can just copy the yaml file to the /var/lib/rancher/k3s/server/manifests/ folder i.e. <code>sudo cp nginx.yaml /var/lib/rancher/k3s/server/manifests/.</code>. This will automatically deploy the newly copied deployment on your cluster.</p> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#deploy-addons-to-k3s","title":"Deploy Addons to K3s","text":"<p>K3s is a lightweight kubernetes tool that doesn't come packaged with all the tools but you can install them separately.</p> <ul> <li> <p>Install Helm Commandline tool on K3s:</p> <p>i. Download the latest version of Helm commandline tool using <code>wget</code> from this page.</p> <pre><code>wget https://get.helm.sh/helm-v3.7.0-linux-amd64.tar.gz\n</code></pre> <p>ii. Unpack it:</p> <pre><code>tar -zxvf helm-v3.7.0-linux-amd64.tar.gz\n</code></pre> <p>iii. Find the helm binary in the unpacked directory, and move it to its desired destination</p> <pre><code>mv linux-amd64/helm /usr/bin/helm\nchmod +x /usr/bin/helm\n</code></pre> <p>OR,</p> <p>Using Snap:</p> <pre><code>snap install helm --classic\n</code></pre> <p>OR,</p> <p>Using Apt (Debian/Ubuntu):</p> <pre><code>curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -\nsudo apt-get install apt-transport-https --yes\necho \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre> </li> <li> <p>Verify the <code>Helm</code> installation:</p> <pre><code>helm version\n\nversion.BuildInfo{Version:\"v3.7.0\", GitCommit:\"eeac83883cb4014fe60267ec63735\n70374ce770b\", GitTreeState:\"clean\", GoVersion:\"go1.16.8\"}\n</code></pre> </li> <li> <p>Add the helm chart repository to allow installation of applications using helm:</p> <pre><code>helm repo add stable https://charts.helm.sh/stable\nhelm repo update\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#deploy-a-sample-nginx-application-using-helm","title":"Deploy A Sample Nginx Application using Helm","text":"<p>Nginx can be used as a web proxy to expose ingress web traffic routes in and out of the cluster.</p> <ul> <li> <p>You can install \"nginx web-proxy\" using Helm:</p> <pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo list\nhelm repo update\nhelm install stable ingress-nginx/ingress-nginx --namespace kube-system \\\n    --set defaultBackend.enabled=false --set controller.publishService.enabled=true\n</code></pre> </li> <li> <p>We can test if the application has been installed by:</p> <pre><code>k3s kubectl get pods -n kube-system -l app=nginx-ingress -o wide\n\nNAME   READY STATUS  RESTARTS AGE  IP        NODE    NOMINATED NODE  READINESS GATES\nnginx.. 1/1  Running 0        19m  10.42.1.5 k3s-worker1   &lt;none&gt;      &lt;none&gt;\n</code></pre> </li> <li> <p>We have successfully deployed nginx web-proxy on k3s. Go to browser, visit <code>http://&lt;Master-Floating-IP&gt;</code>     i.e. http://128.31.25.246 to check the nginx default page.</p> </li> </ul>"},{"location":"other-tools/kubernetes/k3s/k3s/#upgrade-k3s-using-the-installation-script","title":"Upgrade K3s Using the Installation Script","text":"<p>To upgrade K3s from an older version you can re-run the installation script using the same flags, for example:</p> <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre> <p>This will upgrade to a newer version in the stable channel by default.</p> <p>If you want to upgrade to a newer version in a specific channel (such as latest) you can specify the channel:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -\n</code></pre> <p>If you want to upgrade to a specific version you can run the following command:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -\n</code></pre> <p>From non root user's terminal to install the latest version, you do not need to pass <code>INSTALL_K3S_VERSION</code> that by default loads the Latest version.</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--write-kubeconfig-mode 644\" \\\n    sh -\n</code></pre> <p>Note</p> <p>For more about on \"How to use flags and environment variables\" read this.</p>"},{"location":"other-tools/kubernetes/k3s/k3s/#restarting-k3s","title":"Restarting K3s","text":"<p>Restarting K3s is supported by the installation script for <code>systemd</code> and <code>OpenRC</code>.</p> <p>Using systemd:</p> <p>To restart servers manually:</p> <pre><code>sudo systemctl restart k3s\n</code></pre> <p>To restart agents manually:</p> <pre><code>sudo systemctl restart k3s-agent\n</code></pre> <p>Using OpenRC:</p> <p>To restart servers manually:</p> <pre><code>sudo service k3s restart\n</code></pre> <p>To restart agents manually:</p> <pre><code>sudo service k3s-agent restart\n</code></pre>"},{"location":"other-tools/kubernetes/k3s/k3s/#uninstalling","title":"Uninstalling","text":"<p>If you installed <code>K3s</code> with the help of the <code>install.sh</code> script, an uninstall script is generated during installation. The script is created on your master node at <code>/usr/bin/k3s-uninstall.sh</code> or as <code>k3s-agent-uninstall.sh</code> on your worker nodes.</p> <p>To remove K3s on the worker nodes, execute:</p> <pre><code>sudo /usr/bin/k3s-agent-uninstall.sh\nsudo rm -rf /var/lib/rancher\n</code></pre> <p>To remove k3s on the master node, execute:</p> <pre><code>sudo /usr/bin/k3s-uninstall.sh\nsudo rm -rf /var/lib/rancher\n</code></pre>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/","title":"Creating a HA cluster with kubeadm","text":""},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#highly-available-kubernetes-cluster-using-kubeadm","title":"Highly Available Kubernetes Cluster using kubeadm","text":""},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#objectives","title":"Objectives","text":"<ul> <li> <p>Install a multi control-plane(master) Kubernetes cluster</p> </li> <li> <p>Install a Pod network on the cluster so that your Pods can talk to each other</p> </li> <li> <p>Deploy and test a sample app</p> </li> <li> <p>Deploy K8s Dashboard to view all cluster's components</p> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#components-and-architecure","title":"Components and architecure","text":"<p>This shows components and architecture of a highly-available, production-grade Kubernetes cluster.</p> <p></p> <p>You can learn about each component from Kubernetes Componets.</p>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#pre-requisite","title":"Pre-requisite","text":"<p>You will need 2 control-plane(master node) and 2 worker nodes to create a multi-master kubernetes cluster using <code>kubeadm</code>. You are going to use the following set up for this purpose:</p> <ul> <li> <p>2 Linux machines for master, <code>ubuntu-20.04-x86_64</code> or your choice of Ubuntu OS     image, <code>cpu-su.2</code> flavor with 2vCPU, 8GB RAM, 20GB storage.</p> </li> <li> <p>2 Linux machines for worker, <code>ubuntu-20.04-x86_64</code> or your choice of Ubuntu OS     image, <code>cpu-su.1</code> flavor with 1vCPU, 4GB RAM, 20GB storage - also     assign Floating IPs     to both of the worker nodes.</p> </li> <li> <p>1 Linux machine for loadbalancer, <code>ubuntu-20.04-x86_64</code> or your choice of Ubuntu     OS image, <code>cpu-su.1</code> flavor with 1vCPU, 4GB RAM, 20GB storage.</p> </li> <li> <p>ssh access to all machines: Read more here     on how to setup SSH to your remote VMs.</p> </li> <li> <p>Create 2 security groups with appropriate ports and protocols:</p> </li> </ul> <p>i. To be used by the master nodes:</p> <p></p> <p>ii. To be used by the worker nodes:</p> <p></p> <ul> <li> <p>setup Unique hostname to each machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.167 loadbalancer\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname loadbalancer\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#steps","title":"Steps","text":"<ol> <li> <p>Prepare the Loadbalancer node to communicate with the two master nodes'    apiservers on their IPs via port 6443.</p> </li> <li> <p>Do following in all the nodes except the Loadbalancer node:</p> <ul> <li> <p>Disable swap.</p> </li> <li> <p>Install <code>kubelet</code> and <code>kubeadm</code>.</p> </li> <li> <p>Install container runtime - you will be using <code>containerd</code>.</p> </li> </ul> </li> <li> <p>Initiate <code>kubeadm</code> control plane configuration on one of the master nodes.</p> </li> <li> <p>Save the new master and worker node join commands with the token.</p> </li> <li> <p>Join the second master node to the control plane using the join command.</p> </li> <li> <p>Join the worker nodes to the control plane using the join command.</p> </li> <li> <p>Configure kubeconfig(<code>$HOME/.kube/config</code>) on loadbalancer node.</p> </li> <li> <p>Install <code>kubectl</code> on Loadbalancer node.</p> </li> <li> <p>Install CNI network plugin i.e. Flannel on Loadbalancer node.</p> </li> <li> <p>Validate all cluster components and nodes are visible on Loadbalancer node.</p> </li> <li> <p>Deploy a sample app and validate the app from Loadbalancer node.</p> </li> </ol>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#setting-up-loadbalancer","title":"Setting up loadbalancer","text":"<p>You will use HAPROXY as the primary loadbalancer, but you can use any other options as well. This node will be not part of the K8s cluster but will be outside of the cluster and interacts with the cluster using ports.</p> <p>You have 2 master nodes. Which means the user can connect to either of the 2 apiservers. The loadbalancer will be used to loadbalance between the 2 apiservers.</p> <ul> <li> <p>Login to the loadbalancer node</p> </li> <li> <p>Switch as root - <code>sudo su</code></p> </li> <li> <p>Update your repository and your system</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>Install haproxy</p> <pre><code>sudo apt-get install haproxy -y\n</code></pre> </li> <li> <p>Edit haproxy configuration</p> <pre><code>vi /etc/haproxy/haproxy.cfg\n</code></pre> <p>Add the below lines to create a frontend configuration for loadbalancer -</p> <pre><code>frontend fe-apiserver\nbind 0.0.0.0:6443\nmode tcp\noption tcplog\ndefault_backend be-apiserver\n</code></pre> <p>Add the below lines to create a backend configuration for master1 and master2 nodes at port 6443.</p> <p>Note</p> <p>6443 is the default port of kube-apiserver</p> <pre><code>backend be-apiserver\nmode tcp\noption tcplog\noption tcp-check\nbalance roundrobin\ndefault-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\n\n    server master1 10.138.0.15:6443 check\n    server master2 10.138.0.16:6443 check\n</code></pre> <p>Here - master1 and master2 are the hostnames of the master nodes and 10.138.0.15 and 10.138.0.16 are the corresponding internal IP addresses.</p> </li> <li> <p>Ensure haproxy config file is correctly formatted:</p> <pre><code>haproxy -c -q -V -f /etc/haproxy/haproxy.cfg\n</code></pre> </li> <li> <p>Restart and Verify haproxy</p> <pre><code>systemctl restart haproxy\nsystemctl status haproxy\n</code></pre> <p>Ensure haproxy is in running status.</p> <p>Run <code>nc</code> command as below:</p> <pre><code>nc -v localhost 6443\nConnection to localhost 6443 port [tcp/*] succeeded!\n</code></pre> <p>Note</p> <p>If you see failures for <code>master1</code> and <code>master2</code> connectivity, you can ignore them for time being as you have not yet installed anything on the servers.</p> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-kubeadm-kubelet-and-containerd-on-master-and-worker-nodes","title":"Install kubeadm, kubelet and containerd on master and worker nodes","text":"<p><code>kubeadm</code> will not install or manage <code>kubelet</code> or <code>kubectl</code> for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines:</p> <p>\u2022 kubeadm: the command to bootstrap the cluster.</p> <p>\u2022 kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.</p> <p>\u2022 kubectl: the command line util to talk to your cluster.</p> <p>In this step, you will install kubelet and kubeadm on the below nodes</p> <ul> <li> <p>master1</p> </li> <li> <p>master2</p> </li> <li> <p>worker1</p> </li> <li> <p>worker2</p> </li> </ul> <p>The below steps will be performed on all the above mentioned nodes:</p> <ul> <li> <p>SSH into all the 4 machines</p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>Turn off <code>swap</code></p> <pre><code>swapoff -a\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> </li> <li> <p>Install <code>curl</code> and <code>apt-transport-https</code></p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\n</code></pre> </li> <li> <p>Download the Google Cloud public signing key and add key to verify releases</p> <pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n</code></pre> </li> <li> <p>add kubernetes apt repo</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n</code></pre> </li> <li> <p>Install kubelet and kubeadm</p> <pre><code>sudo apt-get update\nsudo apt-get install -y kubelet kubeadm\n</code></pre> </li> <li> <p><code>apt-mark hold</code> is used so that these packages will not be updated/removed automatically</p> <pre><code>sudo apt-mark hold kubelet kubeadm\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-the-container-runtime-ie-containerd-on-master-and-worker-nodes","title":"Install the container runtime i.e. containerd on master and worker nodes","text":"<p>To run containers in Pods, Kubernetes uses a container runtime.</p> <p>By default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.</p> <ul> <li> <p>Install container runtime - containerd</p> <p>The first thing to do is configure the persistent loading of the necessary <code>containerd</code> modules. This forwarding IPv4 and letting iptables see bridged trafficis is done with the following command:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre> </li> <li> <p>Ensure <code>net.bridge.bridge-nf-call-iptables</code> is set to <code>1</code> in your sysctl config:</p> <pre><code># sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n</code></pre> </li> <li> <p>Apply sysctl params without reboot:</p> <pre><code>sudo sysctl --system\n</code></pre> </li> <li> <p>Install the necessary dependencies with:</p> <pre><code>sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates\n</code></pre> </li> <li> <p>The <code>containerd.io</code> packages in DEB and RPM formats are distributed by Docker.     Add the required GPG key with:</p> <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n</code></pre> <p>It's now time to Install and configure containerd:</p> <pre><code>sudo apt update -y\nsudo apt install -y containerd.io\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# Reload the systemd daemon with\nsudo systemctl daemon-reload\n\n# Start containerd\nsudo systemctl restart containerd\nsudo systemctl enable --now containerd\n</code></pre> <p>You can verify <code>containerd</code> is running with the command:</p> <pre><code>sudo systemctl status containerd\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#configure-kubeadm-to-bootstrap-the-cluster","title":"Configure kubeadm to bootstrap the cluster","text":"<p>You will start off by initializing only one master node. For this purpose, you choose <code>master1</code> to initialize our first control plane but you can also do the same in <code>master2</code>.</p> <ul> <li> <p>SSH into master1 machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> <p>Configuring the kubelet cgroup driver</p> <p>From 1.22 onwards, if you do not set the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code>, <code>kubeadm</code> will default it to <code>systemd</code>. So you do not need to do anything here by default but if you want you change it you can refer to this documentation.</p> </li> <li> <p>Execute the below command to initialize the cluster:</p> <pre><code>kubeadm config images pull\nkubeadm init --control-plane-endpoint\n\"LOAD_BALANCER_IP_OR_HOSTNAME:LOAD_BALANCER_PORT\" --upload-certs --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>Here, you can use either the IP address or the hostname of the loadbalancer in place of . You have not enabled the hostname of the server, i.e. <code>loadbalancer</code> as the LOAD_BALANCER_IP_OR_HOSTNAME that is visible from the master1 node. so instead of using not resolvable hostnames across your network, you will be using the IP address of the Loadbalancer server.</p> <p>The  is the front end configuration port defined in HAPROXY configuration. For this, you have kept the port as 6443 which is the default <code>apiserver</code> port.</p> <p>Important Note</p> <p><code>--pod-network-cidr</code> value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: <code>--pod-network-cidr=10.244.0.0/16</code>. If you are opted to use Calico CNI network plugin then you need to use: <code>--pod-network-cidr=192.168.0.0/16</code> and if you are opted to use Weave Net no need to pass this parameter.</p> <p>For example, our <code>Flannel</code> CNI network plugin based kubeadm init command with loadbalancer node with internal IP: <code>192.168.0.167</code> look like below:</p> <pre><code>kubeadm config images pull\nkubeadm init --control-plane-endpoint \"192.168.0.167:6443\" --upload-certs --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from <code>kubeadm init</code> should looks like below:</p> <pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following\ncommand on each worker nodes as root:\n\nkubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3\n    7ab9834567333b939458a5bfb5 \\\n    --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep\nit secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you\ncan use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as\nroot:\n\nkubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5\n</code></pre> <p>The output consists of 3 major tasks:</p> <p>A. Setup <code>kubeconfig</code> using on current master node: As you are running as <code>root</code> user so you need to run the following command:</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre> <p>We need to run the below commands as a normal user to use the kubectl from terminal.</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Now the machine is initialized as master.</p> <p>Warning</p> <p>Kubeadm signs the certificate in the admin.conf to have <code>Subject: O = system:masters, CN = kubernetes-admin. system:masters</code> is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC). Do not share the admin.conf file with anyone and instead grant users custom permissions by generating them a kubeconfig file using the <code>kubeadm kubeconfig user</code> command.</p> <p>B. Setup a new control plane (master) i.e. <code>master2</code> by running following command on master2 node:</p> <pre><code>kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e1\n        5ee37ab9834567333b939458a5bfb5 \\\n    --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146\n</code></pre> <p>C. Join worker nodes running following command on individual worker nodes:</p> <pre><code>kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5\n</code></pre> <p>Important Note</p> <p>Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here.</p> <p>If you do not have the token, you can get it by running the following command on the control-plane node:</p> <pre><code>kubeadm token list\n</code></pre> <p>The output is similar to this:</p> <pre><code>TOKEN     TTL  EXPIRES      USAGES           DESCRIPTION            EXTRA GROUPS\n8ewj1p... 23h  2018-06-12   authentication,  The default bootstrap  system:\n                            signing          token generated by     bootstrappers:\n                                            'kubeadm init'.         kubeadm:\n                                                                    default-node-token\n</code></pre> <p>If you missed the join command, execute the following command <code>kubeadm token create --print-join-command</code> in the master node to recreate the token with the join command.</p> <pre><code>root@master:~$ kubeadm token create --print-join-command\n\nkubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\\n--discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa\n</code></pre> <p>By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node:</p> <pre><code>kubeadm token create\n</code></pre> <p>The output is similar to this: <code>5didvk.d09sbcov8ph2amjw</code></p> <p>We can use this new token to join:</p> <pre><code>kubeadm join &lt;master-ip&gt;:&lt;master-port&gt; --token &lt;token&gt; \\\n    --discovery-token-ca-cert-hash sha256:&lt;hash&gt;\n</code></pre> </li> </ul> <ul> <li> <p>SSH into <code>master2</code></p> </li> <li> <p>Switch to root user:<code>sudo su</code></p> </li> <li> <p>Check the command provided by the output of <code>master1</code>:</p> <p>You can now use the below command to add another control-plane node(master) to the control plane:</p> <pre><code>kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3\n    7ab9834567333b939458a5bfb5 \\\n    --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146\n</code></pre> </li> <li> <p>Execute the kubeadm join command for control plane on <code>master2</code></p> <p>Your output should look like:</p> <pre><code>This node has joined the cluster and a new control plane instance was created:\n\n* Certificate signing request was sent to apiserver and approval was received.\n* The Kubelet was informed of the new secure connection details.\n* Control plane (master) label and taint were applied to the new node.\n* The Kubernetes control plane instances scaled up.\n* A new etcd member was added to the local/stacked etcd cluster.\n</code></pre> </li> </ul> <p>Now that you have initialized both the masters - you can now work on bootstrapping the worker nodes.</p> <ul> <li> <p>SSH into worker1 and worker2</p> </li> <li> <p>Switch to root user on both the machines: <code>sudo su</code></p> </li> <li> <p>Check the output given by the init command on master1 to join worker node:</p> <pre><code>kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5\n</code></pre> </li> <li> <p>Execute the above command on both the nodes:</p> </li> <li> <p>Your output should look like:</p> <pre><code>This node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#configure-kubeconfig-on-loadbalancer-node","title":"Configure kubeconfig on loadbalancer node","text":"<p>Now that you have configured the master and the worker nodes, its now time to configure Kubeconfig (<code>.kube</code>) on the loadbalancer node. It is completely up to you if you want to use the loadbalancer node to setup kubeconfig. kubeconfig can also be setup externally on a separate machine which has access to loadbalancer node. For the purpose of this demo you will use loadbalancer node to host kubeconfig and <code>kubectl</code>.</p> <ul> <li> <p>SSH into <code>loadbalancer</code> node</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Create a directory: .kube at $HOME of root user</p> <pre><code>mkdir -p $HOME/.kube\n</code></pre> </li> <li> <p>SCP configuration file from any one master node to loadbalancer node</p> <pre><code>scp master1:/etc/kubernetes/admin.conf $HOME/.kube/config\n</code></pre> <p>Important Note</p> <p>If you havent setup ssh connection between master node and loadbalancer, you can manually copy the contents of the file <code>/etc/kubernetes/admin.conf</code> from <code>master1</code> node and then paste it to <code>$HOME/.kube/config</code> file on the loadbalancer node. Ensure that the kubeconfig file path is <code>$HOME/.kube/config</code> on the loadbalancer node.</p> </li> <li> <p>Provide appropriate ownership to the copied file</p> <pre><code>chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-kubectl","title":"Install kubectl","text":"<ul> <li> <p>Install kubectl binary</p> <p>kubectl: the command line util to talk to your cluster.</p> <pre><code>snap install kubectl --classic\n</code></pre> <p>This outputs:</p> <pre><code>kubectl 1.26.1 from Canonical\u2713 installed\n</code></pre> </li> <li> <p>Verify the cluster</p> <pre><code>kubectl get nodes\n\nNAME STATUS ROLES AGE VERSION\nmaster1 NotReady control-plane,master 21m v1.26.1\nmaster2 NotReady control-plane,master 15m v1.26.1\nworker1 Ready &lt;none&gt; 9m17s v1.26.1\nworker2 Ready &lt;none&gt; 9m25s v1.26.1\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-cni-network-plugin","title":"Install CNI network plugin","text":""},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#cni-overview","title":"CNI overview","text":"<p>Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the <code>kubeadm</code> configuration file if applicable.</p> <p>You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: <code>kubectl get po -n kube-system</code>:</p> <p>You should see the following output. You will see the two <code>coredns-*</code> pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state.</p> <p>Output Example:</p> <pre><code>root@loadbalancer:~$ kubectl get po -n kube-system\n NAME                               READY  STATUS   RESTARTS  AGE\ncoredns-558bd4d5db-5jktc             0/1   Pending   0        10m\ncoredns-558bd4d5db-xdc5x             0/1   Pending   0        10m\netcd-master1                         1/1   Running   0        11m\nkube-apiserver-master1               1/1   Running   0        11m\nkube-controller-manager-master1      1/1   Running   0        11m\nkube-proxy-5jfh5                     1/1   Running   0        10m\nkube-scheduler-master1               1/1   Running   0        11m\n</code></pre>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#supported-cni-options","title":"Supported CNI options","text":"<p>To read more about the currently supported base CNI solutions for Kubernetes read here and also read this.</p> <p>The below command can be run on the Loadbalancer node to install the CNI plugin:</p> <pre><code>kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml\n</code></pre> <p>As you had passed <code>--pod-network-cidr=10.244.0.0/16</code> with <code>kubeadm init</code> so this should work for Flannel CNI.</p> <p>Using Other CNI Options</p> <p>For Calico CNI plugin to work correctly, you need to pass <code>--pod-network-cidr=192.168.0.0/16</code> with <code>kubeadm init</code> and then you can run: <code>kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code></p> <p>For Weave Net CNI plugin to work correctly, you don't need to pass <code>--pod-network-cidr</code> with <code>kubeadm init</code> and then you can run: <code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl     version | base64 | tr -d '\\n')\"</code></p> <p>Dual Network:</p> <p>It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram:</p> <p></p> <p>To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme:</p> <pre><code>external_interface: eth0\ninternal_interface: eth1\n</code></pre> <p>Also you can decide here what CIDR should your cluster use</p> <pre><code>cluster_cidr: 10.43.0.0/16\nservice_cidr: 10.44.0.0/16\n</code></pre> <p>Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running:</p> <pre><code>kubectl get nodes\n\nNAME      STATUS   ROLES                    AGE   VERSION\nmaster1   Ready    control-plane,master     22m   v1.26.1\nmaster2   Ready    control-plane,master     17m   v1.26.1\nworker1   Ready    &lt;none&gt;                   10m   v1.26.1\nworker2   Ready    &lt;none&gt;                   10m   v1.26.1\n</code></pre>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#deploy-a-sample-nginx-application-from-one-of-the-master-nodes","title":"Deploy A Sample Nginx Application From one of the master nodes","text":"<p>Now that we have all the components to make the cluster and applications work, let's deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767.</p> <p>The below command can be run on:</p> <pre><code>kubectl run nginx --image=nginx --port=80\nkubectl expose pod nginx --port=80 --type=NodePort\n</code></pre> <p>To check which NodePort is opened and running the Nginx run:</p> <pre><code>kubectl get svc\n</code></pre> <p>The output will show:</p> <p></p> <p>Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP.</p> <p>To check which worker node is serving <code>nginx</code>, you can check NODE column running the following command:</p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> <p>This will show like below:</p> <p></p> <p>Go to browser, visit <code>http://&lt;Worker-Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.25.246:32713 to check the nginx default page. Here <code>Worker_Floating-IP</code> corresponds to the Floating IP of the <code>nginx</code> pod running worker node i.e. <code>worker2</code>.</p> <p>For your example,</p> <p></p>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#deploy-a-k8s-dashboard","title":"Deploy A K8s Dashboard","text":"<p>You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components.</p> <ul> <li> <p>SSH into <code>loadbalancer</code> node</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Apply available deployment by running the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml\n</code></pre> <p>This will map Skooner port 4654 to a randomly selected port on the running node.</p> <p>The assigned NodePort can be found running:</p> <pre><code>kubectl get svc --namespace=kube-system\n</code></pre> <p>OR,</p> <pre><code>kubectl get po,svc -n kube-system\n</code></pre> <p></p> <p>To check which worker node is serving <code>skooner-*</code>, you can check NODE column running the following command:</p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> <p>This will show like below:</p> <p></p> <p>Go to browser, visit <code>http://&lt;Worker-Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Here <code>Worker_Floating-IP</code> corresponds to the Floating IP of the <code>skooner-*</code> pod running worker node i.e. <code>worker2</code>.</p> <p></p> </li> </ul> <p>Setup the Service Account Token to access the Skooner Dashboard:</p> <p>The first (and easiest) option is to create a dedicated service account. Run the following commands:</p> <ul> <li> <p>Create the service account in the current namespace (we assume default)</p> <pre><code>kubectl create serviceaccount skooner-sa\n</code></pre> </li> <li> <p>Give that service account root on the cluster</p> <pre><code>kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa\n</code></pre> </li> <li> <p>Create a secret that was created to hold the token for the SA:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n    name: skooner-sa-token\n    annotations:\n        kubernetes.io/service-account.name: skooner-sa\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre> <p>Information</p> <p>Since 1.22, this type of Secret is no longer used to mount credentials into Pods, and obtaining tokens via the TokenRequest API is recommended instead of using service account token Secret objects. Tokens obtained from the TokenRequest API are more secure than ones stored in Secret objects, because they have a bounded lifetime and are not readable by other API clients. You can use the <code>kubectl create token</code> command to obtain a token from the TokenRequest API. For example: <code>kubectl create token skooner-sa</code>, where <code>skooner-sa</code> is service account name.</p> </li> <li> <p>Find the secret that was created to hold the token for the SA</p> <pre><code>kubectl get secrets\n</code></pre> </li> <li> <p>Show the contents of the secret to extract the token</p> <pre><code>kubectl describe secret skooner-sa-token\n</code></pre> </li> </ul> <p>Copy the token value from the secret detail and enter it into the login screen to access the dashboard.</p>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#watch-demo-video-showing-how-to-setup-the-cluster","title":"Watch Demo Video showing how to setup the cluster","text":"<p>Here's a recorded demo video on how to setup HA K8s cluster using <code>kubeadm</code> as explained above.</p>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#very-important-certificates-renewal","title":"Very Important: Certificates Renewal","text":"<p>Client certificates generated by <code>kubeadm</code> expire after one year unless the Kubernetes version is upgraded or the certificates are manually renewed.</p> <p>To renew certificates manually, you can use the kubeadm certs renew command with the appropriate command line options. After running the command, you should restart the control plane Pods.</p> <p><code>kubeadm certs renew</code> can renew any specific certificate or, with the subcommand all, it can renew all of them, as shown below:</p> <pre><code>kubeadm certs renew all\n</code></pre> <p>Once renewing certificates is done. You must restart the <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code> and <code>etcd</code>, so that they can use the new certificates by running:</p> <pre><code>systemctl restart kubelet\n</code></pre> <p>Then, update the new kube config file:</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n</code></pre> <p>Don't Forget to Update the older kube config file</p> <p>Update wherever you are using the older kube config to connect with the cluster.</p>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#clean-up","title":"Clean Up","text":"<ul> <li> <p>To view the Cluster info:</p> <pre><code>kubectl cluster-info\n</code></pre> </li> <li> <p>To delete your local references to the cluster:</p> <pre><code>kubectl config delete-cluster\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/HA-clusters-with-kubeadm/#how-to-remove-the-node","title":"How to Remove the node?","text":"<p>Talking to the control-plane node with the appropriate credentials, run:</p> <pre><code>kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets\n</code></pre> <ul> <li> <p>Before removing the node, reset the state installed by kubeadm:</p> <pre><code>kubeadm reset\n</code></pre> <p>The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:</p> <pre><code>iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X\n</code></pre> <p>If you want to reset the IPVS tables, you must run the following command:</p> <pre><code>ipvsadm -C\n</code></pre> </li> <li> <p>Now remove the node:</p> <pre><code>kubectl delete node &lt;node name&gt;\n</code></pre> </li> </ul> <p>If you wish to start over, run <code>kubeadm init</code> or <code>kubeadm join</code> with the appropriate arguments.</p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/","title":"Bootstrapping cluster with kubeadm","text":""},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#creating-a-single-master-cluster-with-kubeadm","title":"Creating a Single Master cluster with kubeadm","text":""},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#objectives","title":"Objectives","text":"<ul> <li> <p>Install a single control-plane(master) Kubernetes cluster</p> </li> <li> <p>Install a Pod network on the cluster so that your Pods can talk to each other</p> </li> <li> <p>Deploy and test a sample app</p> </li> <li> <p>Deploy K8s Dashboard to view all cluster's components</p> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#components-and-architecure","title":"Components and architecure","text":"<p>You can learn about each component from Kubernetes Componets.</p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#pre-requisite","title":"Pre-requisite","text":"<p>We will need 1 control-plane(master) and 2 worker node to create a single control-plane kubernetes cluster using <code>kubeadm</code>. We are using following setting for this purpose:</p> <ul> <li> <p>1 Linux machine for master, <code>ubuntu-20.04-x86_64</code>, <code>cpu-su.2</code> flavor with 2vCPU,     8GB RAM, 20GB storage.</p> </li> <li> <p>2 Linux machines for worker, <code>ubuntu-20.04-x86_64</code>, <code>cpu-su.1</code> flavor with 1vCPU,     4GB RAM, 20GB storage - also assign Floating IPs     to both of the worker nodes.</p> </li> <li> <p>ssh access to all machines: Read more here     on how to set up SSH on your remote VMs.</p> </li> <li> <p>Create 2 security groups with appropriate ports and protocols:</p> </li> </ul> <p>i. To be used by the master nodes:</p> <p></p> <p>ii. To be used by the worker nodes:</p> <p></p> <ul> <li> <p>setup Unique hostname to each machine using the following command:</p> <pre><code>echo \"&lt;node_internal_IP&gt; &lt;host_name&gt;\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname &lt;host_name&gt;\n</code></pre> <p>For example:</p> <pre><code>echo \"192.168.0.167 master\" &gt;&gt; /etc/hosts\nhostnamectl set-hostname master\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#steps","title":"Steps","text":"<ol> <li> <p>Disable swap on all nodes.</p> </li> <li> <p>Install <code>kubeadm</code>, <code>kubelet</code>, and <code>kubectl</code> on all the nodes.</p> </li> <li> <p>Install container runtime on all nodes- you will be using <code>containerd</code>.</p> </li> <li> <p>Initiate <code>kubeadm</code> control plane configuration on the master node.</p> </li> <li> <p>Save the worker node join command with the token.</p> </li> <li> <p>Install CNI network plugin i.e. Flannel on master node.</p> </li> <li> <p>Join the worker node to the master node (control plane) using the join command.</p> </li> <li> <p>Validate all cluster components and nodes are visible on master node.</p> </li> <li> <p>Deploy a sample app and validate the app from master node.</p> </li> </ol>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-kubeadm-kubelet-and-containerd-on-master-and-worker-nodes","title":"Install kubeadm, kubelet and containerd on master and worker nodes","text":"<p><code>kubeadm</code> will not install or manage <code>kubelet</code> or <code>kubectl</code> for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines:</p> <p>\u2022 kubeadm: the command to bootstrap the cluster.</p> <p>\u2022 kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.</p> <p>\u2022 kubectl: the command line util to talk to your cluster.</p> <p>In this step, you will install kubelet and kubeadm on the below nodes</p> <ul> <li> <p>master</p> </li> <li> <p>worker1</p> </li> <li> <p>worker2</p> </li> </ul> <p>The below steps will be performed on all the above mentioned nodes:</p> <ul> <li> <p>SSH into all the 3 machines</p> </li> <li> <p>Update the repositories and packages:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>Turn off <code>swap</code></p> <pre><code>swapoff -a\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> </li> <li> <p>Install <code>curl</code> and <code>apt-transport-https</code></p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\n</code></pre> </li> <li> <p>Download the Google Cloud public signing key and add key to verify releases</p> <pre><code>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n</code></pre> </li> <li> <p>add kubernetes apt repo</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n</code></pre> </li> <li> <p>Install kubelet, kubeadm, and kubectl</p> <pre><code>sudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\n</code></pre> </li> <li> <p><code>apt-mark hold</code> is used so that these packages will not be updated/removed automatically</p> <pre><code>sudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-the-container-runtime-ie-containerd-on-master-and-worker-nodes","title":"Install the container runtime i.e. containerd on master and worker nodes","text":"<p>To run containers in Pods, Kubernetes uses a container runtime.</p> <p>By default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.</p> <ul> <li> <p>Install container runtime - containerd</p> <p>The first thing to do is configure the persistent loading of the necessary <code>containerd</code> modules. This forwarding IPv4 and letting iptables see bridged trafficis is done with the following command:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre> </li> <li> <p>Ensure <code>net.bridge.bridge-nf-call-iptables</code> is set to <code>1</code> in your sysctl config:</p> <pre><code># sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n</code></pre> </li> <li> <p>Apply sysctl params without reboot:</p> <pre><code>sudo sysctl --system\n</code></pre> </li> <li> <p>Install the necessary dependencies with:</p> <pre><code>sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates\n</code></pre> </li> <li> <p>The <code>containerd.io</code> packages in DEB and RPM formats are distributed by Docker.     Add the required GPG key with:</p> <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n</code></pre> <p>It's now time to Install and configure containerd:</p> <pre><code>sudo apt update -y\nsudo apt install -y containerd.io\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# Reload the systemd daemon with\nsudo systemctl daemon-reload\n\n# Start containerd\nsudo systemctl restart containerd\nsudo systemctl enable --now containerd\n</code></pre> <p>You can verify <code>containerd</code> is running with the command:</p> <pre><code>sudo systemctl status containerd\n</code></pre> <p>Configuring the kubelet cgroup driver</p> <p>From 1.22 onwards, if you do not set the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code>, <code>kubeadm</code> will default it to <code>systemd</code>. So you do not need to do anything here by default but if you want you change it you can refer to this documentation.</p> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#configure-kubeadm-to-bootstrap-the-cluster-on-master-node","title":"Configure kubeadm to bootstrap the cluster on master node","text":"<p>Run the below command on the master node i.e. <code>master</code> that you want to setup as control plane.</p> <ul> <li> <p>SSH into master machine</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Execute the below command to initialize the cluster:</p> <pre><code>export MASTER_IP=&lt;Master-Internal-IP&gt;\nkubeadm config images pull\nkubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>Important Note</p> <p>Please make sure you replace the correct IP of the node with <code>&lt;Master-Internal-IP&gt;</code> which is the Internal IP of master node. <code>--pod-network-cidr</code> value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: <code>--pod-network-cidr=10.244.0.0/16</code>. If you are opted to use Calico CNI network plugin then you need to use: <code>--pod-network-cidr=192.168.0.0/16</code> and if you are opted to use Weave Net no need to pass this parameter.</p> <p>For example, our <code>Flannel</code> CNI network plugin based kubeadm init command with master node with internal IP: <code>192.168.0.167</code> look like below:</p> <p>For example:</p> <pre><code>export MASTER_IP=192.168.0.167\nkubeadm config images pull\nkubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from <code>kubeadm init</code> should looks like below:</p> <pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following\ncommand on each worker nodes as root:\n\nkubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3\n    7ab9834567333b939458a5bfb5 \\\n    --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep\nit secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you\ncan use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as\nroot:\n\nkubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5\n</code></pre> <p>The output consists of 2 major tasks:</p> <p>A. Setup <code>kubeconfig</code> using on current master node: As you are running as <code>root</code> user so you need to run the following command:</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n</code></pre> <p>We need to run the below commands as a normal user to use the kubectl from terminal.</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Now the machine is initialized as master.</p> <p>Warning</p> <p>Kubeadm signs the certificate in the admin.conf to have <code>Subject: O = system:masters, CN = kubernetes-admin. system:masters</code> is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC). Do not share the admin.conf file with anyone and instead grant users custom permissions by generating them a kubeconfig file using the <code>kubeadm kubeconfig user</code> command.</p> <p>B. Join worker nodes running following command on individual worker nodes:</p> <pre><code>kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5\n</code></pre> <p>Important Note</p> <p>Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here.</p> <p>If you do not have the token, you can get it by running the following command on the control-plane node:</p> <pre><code>kubeadm token list\n</code></pre> <p>The output is similar to this:</p> <pre><code>TOKEN     TTL  EXPIRES      USAGES           DESCRIPTION            EXTRA GROUPS\n8ewj1p... 23h  2018-06-12   authentication,  The default bootstrap  system:\n                            signing          token generated by     bootstrappers:\n                                            'kubeadm init'.         kubeadm:\n                                                                    default-node-token\n</code></pre> <p>If you missed the join command, execute the following command <code>kubeadm token create --print-join-command</code> in the master node to recreate the token with the join command.</p> <pre><code>root@master:~$ kubeadm token create --print-join-command\n\nkubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\\n--discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa\n</code></pre> <p>By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node:</p> <pre><code>kubeadm token create\n</code></pre> <p>The output is similar to this: <code>5didvk.d09sbcov8ph2amjw</code></p> <p>We can use this new token to join:</p> <pre><code>kubeadm join &lt;master-ip&gt;:&lt;master-port&gt; --token &lt;token&gt; \\\n    --discovery-token-ca-cert-hash sha256:&lt;hash&gt;\n</code></pre> </li> </ul> <p>Now that you have initialized the master - you can now work on bootstrapping the worker nodes.</p> <ul> <li> <p>SSH into worker1 and worker2</p> </li> <li> <p>Switch to root user on both the machines: <code>sudo su</code></p> </li> <li> <p>Check the output given by the init command on master to join worker node:</p> <pre><code>kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\\n    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5\n</code></pre> </li> <li> <p>Execute the above command on both the nodes:</p> </li> <li> <p>Your output should look like:</p> <pre><code>This node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#validate-all-cluster-components-and-nodes-are-visible-on-all-nodes","title":"Validate all cluster components and nodes are visible on all nodes","text":"<ul> <li> <p>Verify the cluster</p> <pre><code>kubectl get nodes\n\nNAME      STATUS        ROLES                  AGE     VERSION\nmaster    NotReady      control-plane,master   21m     v1.26.1\nworker1   Ready         &lt;none&gt;                 9m17s   v1.26.1\nworker2   Ready         &lt;none&gt;                 9m25s   v1.26.1\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-cni-network-plugin","title":"Install CNI network plugin","text":""},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#cni-overview","title":"CNI overview","text":"<p>Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the <code>kubeadm</code> configuration file if applicable.</p> <p>You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: <code>kubectl get po -n kube-system</code>:</p> <p>You should see the following output. You will see the two <code>coredns-*</code> pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state.</p> <p>Output Example:</p> <pre><code>root@master:~$ kubectl get po -n kube-system\n NAME                               READY  STATUS   RESTARTS  AGE\ncoredns-558bd4d5db-5jktc             0/1   Pending   0        10m\ncoredns-558bd4d5db-xdc5x             0/1   Pending   0        10m\netcd-master1                         1/1   Running   0        11m\nkube-apiserver-master1               1/1   Running   0        11m\nkube-controller-manager-master1      1/1   Running   0        11m\nkube-proxy-5jfh5                     1/1   Running   0        10m\nkube-scheduler-master1               1/1   Running   0        11m\n</code></pre>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#supported-cni-options","title":"Supported CNI options","text":"<p>To read more about the currently supported base CNI solutions for Kubernetes read here and also read this.</p> <p>The below command can be run on the master node to install the CNI plugin:</p> <pre><code>kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml\n</code></pre> <p>As you had passed <code>--pod-network-cidr=10.244.0.0/16</code> with <code>kubeadm init</code> so this should work for Flannel CNI.</p> <p>Using Other CNI Options</p> <p>For Calico CNI plugin to work correctly, you need to pass <code>--pod-network-cidr=192.168.0.0/16</code> with <code>kubeadm init</code> and then you can run: <code>kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code></p> <p>For Weave Net CNI plugin to work correctly, you don't need to pass <code>--pod-network-cidr</code> with <code>kubeadm init</code> and then you can run: <code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl     version | base64 | tr -d '\\n')\"</code></p> <p>Dual Network:</p> <p>It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram:</p> <p></p> <p>To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme:</p> <pre><code>external_interface: eth0\ninternal_interface: eth1\n</code></pre> <p>Also you can decide here what CIDR should your cluster use</p> <pre><code>cluster_cidr: 10.43.0.0/16\nservice_cidr: 10.44.0.0/16\n</code></pre> <p>Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running:</p> <pre><code>kubectl get nodes\n\nNAME      STATUS   ROLES                    AGE   VERSION\nmaster    Ready    control-plane,master     22m   v1.26.1\nworker1   Ready    &lt;none&gt;                   10m   v1.26.1\nworker2   Ready    &lt;none&gt;                   10m   v1.26.1\n</code></pre>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#watch-recorded-video-showing-the-above-steps-on-setting-up-the-cluster","title":"Watch Recorded Video showing the above steps on setting up the cluster","text":"<p>Here's a quick recorded demo video upto this point where we successfully setup single master K8s cluster using Kubeadm.</p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#deploy-a-sample-nginx-application-from-the-master-node","title":"Deploy A Sample Nginx Application From the master node","text":"<p>Now that we have all the components to make the cluster and applications work, let's deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767.</p> <p>The below command can be run on:</p> <pre><code>kubectl run nginx --image=nginx --port=80\nkubectl expose pod nginx --port=80 --type=NodePort\n</code></pre> <p>To check which NodePort is opened and running the Nginx run:</p> <pre><code>kubectl get svc\n</code></pre> <p>The output will show:</p> <p></p> <p>Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP.</p> <p>To check which worker node is serving <code>nginx</code>, you can check NODE column running the following command:</p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> <p>This will show like below:</p> <p></p> <p>Go to browser, visit <code>http://&lt;Worker-Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.25.246:32713 to check the nginx default page. Here <code>Worker_Floating-IP</code> corresponds to the Floating IP of the <code>nginx</code> pod running worker node i.e. <code>worker2</code>.</p> <p>For your example,</p> <p></p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#deploy-a-k8s-dashboard","title":"Deploy A K8s Dashboard","text":"<p>You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components.</p> <ul> <li> <p>SSH into <code>master</code> node</p> </li> <li> <p>Switch to root user: <code>sudo su</code></p> </li> <li> <p>Apply available deployment by running the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml\n</code></pre> <p>This will map Skooner port 4654 to a randomly selected port from the master node. The assigned NodePort on the master node can be found running:</p> <pre><code>kubectl get svc --namespace=kube-system\n</code></pre> <p>OR,</p> <pre><code>kubectl get po,svc -n kube-system\n</code></pre> <p></p> <p>To check which worker node is serving <code>skooner-*</code>, you can check NODE column running the following command:</p> <pre><code>kubectl get pods --all-namespaces --output wide\n</code></pre> <p>OR,</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> <p>This will show like below:</p> <p></p> <p>Go to browser, visit <code>http://&lt;Worker-Floating-IP&gt;:&lt;NodePort&gt;</code> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Here <code>Worker_Floating-IP</code> corresponds to the Floating IP of the <code>skooner-*</code> pod running worker node i.e. <code>worker2</code>.</p> <p></p> </li> </ul> <p>Setup the Service Account Token to access the Skooner Dashboard:</p> <p>The first (and easiest) option is to create a dedicated service account. Run the following commands:</p> <ul> <li> <p>Create the service account in the current namespace (we assume default)</p> <pre><code>kubectl create serviceaccount skooner-sa\n</code></pre> </li> <li> <p>Give that service account root on the cluster</p> <pre><code>kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa\n</code></pre> </li> <li> <p>Create a secret that was created to hold the token for the SA:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n    name: skooner-sa-token\n    annotations:\n        kubernetes.io/service-account.name: skooner-sa\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre> <p>Information</p> <p>Since 1.22, this type of Secret is no longer used to mount credentials into Pods, and obtaining tokens via the TokenRequest API is recommended instead of using service account token Secret objects. Tokens obtained from the TokenRequest API are more secure than ones stored in Secret objects, because they have a bounded lifetime and are not readable by other API clients. You can use the <code>kubectl create token</code> command to obtain a token from the TokenRequest API. For example: <code>kubectl create token skooner-sa</code>, where <code>skooner-sa</code> is service account name.</p> </li> <li> <p>Find the secret that was created to hold the token for the SA</p> <pre><code>kubectl get secrets\n</code></pre> </li> <li> <p>Show the contents of the secret to extract the token</p> <pre><code>kubectl describe secret skooner-sa-token\n</code></pre> </li> </ul> <p>Copy the token value from the secret detail and enter it into the login screen to access the dashboard.</p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#watch-demo-video-showing-how-to-deploy-applications","title":"Watch Demo Video showing how to deploy applications","text":"<p>Here's a recorded demo video on how to deploy applications on top of setup single master K8s cluster as explained above.</p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#very-important-certificates-renewal","title":"Very Important: Certificates Renewal","text":"<p>Client certificates generated by <code>kubeadm</code> expire after one year unless the Kubernetes version is upgraded or the certificates are manually renewed.</p> <p>To renew certificates manually, you can use the kubeadm certs renew command with the appropriate command line options. After running the command, you should restart the control plane Pods.</p> <p><code>kubeadm certs renew</code> can renew any specific certificate or, with the subcommand all, it can renew all of them, as shown below:</p> <pre><code>kubeadm certs renew all\n</code></pre> <p>Once renewing certificates is done. You must restart the <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code> and <code>etcd</code>, so that they can use the new certificates by running:</p> <pre><code>systemctl restart kubelet\n</code></pre> <p>Then, update the new kube config file:</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n</code></pre> <p>Don't Forget to Update the older kube config file</p> <p>Update wherever you are using the older kube config to connect with the cluster.</p>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#clean-up","title":"Clean Up","text":"<ul> <li> <p>To view the Cluster info:</p> <pre><code>kubectl cluster-info\n</code></pre> </li> <li> <p>To delete your local references to the cluster:</p> <pre><code>kubectl config delete-cluster\n</code></pre> </li> </ul>"},{"location":"other-tools/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#how-to-remove-the-node","title":"How to Remove the node?","text":"<p>Talking to the control-plane node with the appropriate credentials, run:</p> <pre><code>kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets\n</code></pre> <ul> <li> <p>Before removing the node, reset the state installed by kubeadm:</p> <pre><code>kubeadm reset\n</code></pre> <p>The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:</p> <pre><code>iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X\n</code></pre> <p>If you want to reset the IPVS tables, you must run the following command:</p> <pre><code>ipvsadm -C\n</code></pre> </li> <li> <p>Now remove the node:</p> <pre><code>kubectl delete node &lt;node name&gt;\n</code></pre> </li> </ul> <p>If you wish to start over, run <code>kubeadm init</code> or <code>kubeadm join</code> with the appropriate arguments.</p>"},{"location":"other-tools/mlflow/mlflow-overview/","title":"MLflow Overview","text":""},{"location":"other-tools/mlflow/mlflow-overview/#mlflow-overview","title":"MLflow Overview","text":"<p>MLflow is an open-source platform that helps manage the entire machine learning lifecycle, including experimentation, reproducibility, deployment, and model management. It provides four core components:</p> <p></p> <ol> <li> <p>MLflow Tracking:</p> <p>Tracks experiments to log and query results such as parameters, metrics, and models.</p> </li> <li> <p>MLflow Projects:</p> <p>Defines and packages code in a reusable and reproducible format for running machine learning workflows.</p> </li> <li> <p>MLflow Models:</p> <p>Manages and stores machine learning models in various formats for deployment across different platforms.</p> </li> <li> <p>MLflow Registry:</p> <p>A central repository for managing the lifecycle of machine learning models, including versioning and stage transitions.</p> </li> </ol> <p>For more information, visit the MLflow website.</p>"},{"location":"other-tools/mlflow/mlflow-server-setup/","title":"MLflow Server Setup","text":""},{"location":"other-tools/mlflow/mlflow-server-setup/#mlflow-server-setup","title":"MLflow Server Setup","text":"<p>Our MLflow Server Setup has a straightforward architecture, as illustrated in the figure below:</p> <p></p> <p>It consists of three main components: a backend store that holds experiment metadata (such as hyperparameters and metrics), an artifact store for storing all artifacts (like model files), and the MLflow server, which provides an API and a UI for viewing and recording this information.</p> <p>In our setup, we are going to use:</p> <p>PostgreSQL database - for storing MLflow metadata</p> <p>MinIO S3 Storage - for storing the MLflow artifacts such as model training files (such as models, data, and visualizations, etc.). These artifacts are crucial for reproducing and understanding the results of a machine learning experiment. To manage and store these artifacts in a scalable, durable, and secure manner, an S3 bucket is required.</p>"},{"location":"other-tools/mlflow/mlflow-server-setup/#standalone-deployment-of-mlflow-server","title":"Standalone Deployment of <code>MLflow</code> Server","text":"<ul> <li> <p>Prerequisites:</p> <p>Setup the OpenShift CLI (<code>oc</code>) Tools locally and configure the OpenShift CLI to enable <code>oc</code> commands. Refer to this user guide.</p> </li> </ul>"},{"location":"other-tools/mlflow/mlflow-server-setup/#deployment-procedure","title":"Deployment procedure","text":"<ol> <li> <p>Clone or navigate to this repository.</p> <p>To get started, clone the repository using:</p> <pre><code>git clone https://github.com/nerc-project/llm-on-nerc.git\ncd llm-on-nerc/mlflow\n</code></pre> </li> <li> <p>In the <code>standalone</code> folder, you will find the following YAML files that allow     you to easily deploy a MLflow infrastructure:</p> <ul> <li> <p>01-mlflow-postgres.yml: Defines all objects required to setup a standalone     PostgreSQL database.</p> <p>This allow allow you to set your own database info via Secrets:</p> <ul> <li> <p>database-name: vectordb  # Change this with your own value</p> </li> <li> <p>database-password: vectordb  # Change this with your own value</p> </li> <li> <p>database-user: vectordb  # Change this with your own value</p> </li> </ul> </li> <li> <p>02-mlflow-minio.yml: Defines all objects required to setup a MinIO     object storage instance:</p> <ul> <li> <p>Deploys a MinIO instance in your project namespace.</p> </li> <li> <p>Creates one storage bucket within the MinIO instance named as <code>mlflow-bucket</code>.</p> </li> <li> <p>Generates a random Root User, which can also be used as the     Access Key, and a Root User Password, which serves as the     Secret Key for accessing both the MinIO API and the MinIO Console.</p> </li> <li> <p>Installs all required network policies.</p> </li> </ul> </li> <li> <p>03-mlflow-server.yml: Creates a MLflow Server that connects</p> </li> <li> <p>04-mlflow-others.yml: Creates the mlflow-test-connection pod to     test the MLflow server connection. And also creates the mlflow-test-training     pod to run a test experiment by submitting a sample training job to the     MLflow server.</p> <p>Note</p> <p>The mlflow-test-connection and mlflow-test-training pods will be removed after the initial run.</p> </li> </ul> </li> </ol> <p>You can run this <code>oc</code> command: <code>oc apply -f ./standalone/.</code> to execute all of the above described YAML files located in the standalone folder at once.</p> <pre><code>oc apply -f ./standalone/.\n\nsecret/mlflow-postgresql-secret created\npersistentvolumeclaim/mlflow-postgresql-pvc created\ndeployment.apps/mlflow-postgresql-deployment created\nservice/mlflow-postgresql-service created\nserviceaccount/mlflow-minio-setup created\nrolebinding.rbac.authorization.k8s.io/mlflow-minio-setup-edit created\npersistentvolumeclaim/mlflow-minio-pvc created\ndeployment.apps/mlflow-minio-deployment created\njob.batch/create-minio-buckets created\njob.batch/create-mlflow-minio-root-user created\nservice/mlflow-minio-service created\nroute.route.openshift.io/mlflow-minio-console created\nroute.route.openshift.io/mlflow-minio-s3 created\ndeployment.apps/mlflow-deployment created\nservice/mlflow-service created\nroute.route.openshift.io/mlflow-route created\npod/mlflow-test-connection created\npod/mlflow-test-training created\n</code></pre>"},{"location":"other-tools/mlflow/mlflow-server-setup/#clean-up","title":"Clean Up","text":"<p>To delete all resources if not necessary just run <code>oc delete -f ./standalone/.</code>.</p>"},{"location":"other-tools/mlflow/mlflow-server-setup/#usage","title":"Usage","text":"<ol> <li> <p>Go to the NERC's OpenShift Web Console.</p> </li> <li> <p>In the Navigation Menu, navigate to the Workloads -&gt; Topology menu     and make sure that you are on the MLflow project.</p> <p></p> </li> <li> <p>Check the <code>mlflow-deployment</code> pod circle that is in dark blue color (this means   it has finished deploying successfully and the pod is \"Running\").</p> <p>The API is now accessible at the endpoints:</p> <ul> <li> <p>defined by your Service, accessible internally on port 5000 using http.</p> <p>This is accessible within the cluster only, such as from the NERC RHOAI Workbench hosted Jupyter Notebooks or another pod within your project namespace.</p> <p>You can use either the service name or the fully qualified internal Hostname for service routing, as shown below:</p> <ul> <li> <p>Option 1: Using the service name i.e. <code>http://mlflow-service:5000</code></p> </li> <li> <p>Option 2: Using the full internal hostname i.e. <code>http://mlflow-service.&lt;your-namespace&gt;.svc.cluster.local:5000</code></p> </li> </ul> <p></p> </li> <li> <p>defined by your Route, accessible externally through https, e.g. <code>https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org</code>.</p> <p>Accessing MLflow GUI Dashboard:</p> <p>When the application has been deployed successfully, you can either open the application URL using the Open URL icon in the top right corner of the Pod's circle as shown below or you can naviate to the route URL by navigating to the \"Routes\" section under the Location path as shown below:</p> <p></p> <p>This show the MLflow GUI as shown below:</p> <p></p> </li> </ul> </li> </ol>"},{"location":"other-tools/mlflow/mlflow-server-setup/#adding-mlflow-to-training-code","title":"Adding MLflow to Training Code","text":"<pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\n\n# Set the tracking URI to your remote MLflow server\n\n# This Route endpoint is accessible externally over **HTTPS**:\nmlflow.set_tracking_uri(\"https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org\")  # Replace with your own remote server's route\n\n# Alternatively: Set the tracking URI to a local MLflow server's service endpoint. This is accessible internally over **HTTP** on the specified port.\n\n# This option is accessible within the cluster only, such as from:\n# -   NERC RHOAI Workbench (Jupyter Notebooks)\n# -   Another pod within your project namespace\n\n# You can use either the service name or the fully qualified internal Hostname:\n\n# Option 1: Using the service name\n# mlflow.set_tracking_uri(\"http://mlflow-service:5000\")\n\n# Option 2: Using the full internal Hostname\n# mlflow.set_tracking_uri(\"http://mlflow-service.&lt;your-namespace&gt;.svc.cluster.local:5000\")\n\n# Setting the experiment\nmlflow.set_experiment(\"my-experiment\")\n\nif __name__ == \"__main__\":\n    # Enabling automatic logging for scikit-learn runs\n    mlflow.sklearn.autolog()\n\n    # Starting a logging run\n    with mlflow.start_run():\n        # train\n</code></pre> <p>How to add MLflow to your Training Code?</p> <p>You can review this provided Credit Card Fraud Detection Application to understand how to integrate MLflow into your model training process. The detailed steps are outlined here.</p>"},{"location":"other-tools/mlflow/mlflow-server-setup/#examples","title":"Examples","text":"<p>Set up your Python Virtual environment and install all required packages by running: <code>pip install -r examples/requirements.txt</code> inside the activated virtual environment.</p> <p>Then you can run the following python experiment scripts:</p> <ul> <li> <p>01-test_remote.py: Please open and edit the Python file to set the tracking     URI to your own remote MLflow server and then run:</p> <pre><code>(venv)$ python examples/01-test_remote.py\n2025/05/15 18:36:13 INFO mlflow.tracking.fluent: Experiment with name 'test_experiment1' does not exist. Creating a new experiment.\nRun logged successfully!\n\ud83c\udfc3 View run nebulous-ant-535 at: https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org/#/experiments/1/runs/55f01904bd914be880fe9fd1fdbcc515\n\ud83e\uddea View experiment at: https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org/#/experiments/1\n</code></pre> </li> <li> <p>02-test_remote.py: Please open and edit the Python file to set the trackin     URI to your own remote MLflow server. Also, you need to set the MinIO S3 endpoint     URL to your MinIO server API and the MinIO S3 credentials i.e. <code>AWS_ACCESS_KEY_ID</code>     and <code>AWS_SECRET_ACCESS_KEY</code> and then run:</p> <pre><code>(venv)$ python examples/02-test_remote.py\nhttps://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org\n2025/05/15 18:36:34 INFO mlflow.tracking.fluent: Experiment with name 'test_experiment2' does not exist. Creating a new experiment.\nArtifact sample_plot.png logged successfully!\n\ud83c\udfc3 View run painted-cow-955 at: https://mlflow-route-&lt;your-namespace&gt;.apps.shift.nerc.mghpcc.org/#/experiments/2/runs/05ce7b095b5049e0ad72e0ef25cf48e3\n</code></pre> </li> </ul> <p></p> <p>By clicking on the test_experiment2 run in the MLflow GUI, you can verify that the experiment has successfully stored the artifact under the Artifacts tab as shown below:</p> <p></p> <p>Very Important</p> <p>There are ways we can improve this setup - for example, by adding basic authentication to the MLflow GUI to ensure that only authorized users can access it.</p> <p>For more details, refer to this documentation.</p>"},{"location":"other-tools/nfs/nfs-server-client-setup/","title":"Setup NFS Server and Client","text":""},{"location":"other-tools/nfs/nfs-server-client-setup/#network-file-system-nfs","title":"Network File System (NFS)","text":"<p>NFS enables a system to share directories and files with others over a network. With NFS, users and applications can access files on remote systems as if they were stored locally. Client systems mount a directory hosted on the NFS server, allowing them to access and work with the files it contains.</p>"},{"location":"other-tools/nfs/nfs-server-client-setup/#pre-requisite","title":"Pre-requisite","text":"<p>We are using the following configuration to set up the NFS server and client on Ubuntu-based NERC OpenStack VMs:</p> <ul> <li> <p>1 Linux machine for the NFS Server, <code>ubuntu-24.04-x86_64</code>, <code>cpu-su.1</code> flavor     with 1vCPU, 4GB RAM, 20GB storage - also assign Floating IP.     Please note the NFS Server's Internal IP i.e. <code>&lt;NFS_SERVER_INTERNAL_IP&gt;</code>     i.e. <code>192.168.0.73</code> in this example.</p> </li> <li> <p>1 Linux machine for the NFS Client, <code>ubuntu-24.04-x86_64</code>, <code>cpu-su.1</code> flavor     with 1vCPU, 4GB RAM, 20GB storage - also assign Floating IP.</p> </li> <li> <p>ssh access to both machines: Read more here     on how to set up SSH on your remote VMs.</p> </li> <li> <p>Create a security group with a rule that opens Port 2049 (the default     NFS port) for file sharing. Update Security Group to the NFS Server VM     only following this reference.</p> </li> </ul>"},{"location":"other-tools/nfs/nfs-server-client-setup/#installing-and-configuring-nfs-server","title":"Installing and configuring NFS Server","text":"<ol> <li> <p>Update System Packages:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>Install NFS Kernel Server:</p> <pre><code>sudo apt install nfs-kernel-server -y\n</code></pre> </li> <li> <p>Create and Attach an Empty Volume to the NFS Server:</p> <p>3.1. Create an Empty Volume.</p> <p>3.2. Attach the Volume to the NFS Server.</p> <p>Verify the 100GiB (adjust based on your Storage requirements) attached volume is available on the NFS server VM:</p> <pre><code>lsblk\n...\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nvda     253:0    0   20G  0 disk\n\u251c\u2500vda1  253:1    0   19G  0 part /\n\u251c\u2500vda14 253:14   0    4M  0 part\n\u251c\u2500vda15 253:15   0  106M  0 part /boot/efi\n\u2514\u2500vda16 259:0    0  913M  0 part /boot\nvdb     253:16   0    100G  0 disk\n</code></pre> <p>3.3. Format And Mount The Volume.</p> <p>Create a filesystem on the volume. In this example, we will create an <code>ext4</code> filesystem:</p> <pre><code>sudo mkfs.ext4 /dev/vdb\n</code></pre> <p>3.4. Create a directory you want to share over the network:</p> <pre><code>sudo mkdir -p /mnt/nfs_share\n</code></pre> <p>3.5. Mount it:</p> <pre><code>sudo mount /dev/vdb /mnt/nfs_share\n</code></pre> <p>Verify the mount path is set correctly:</p> <pre><code>df -H\n...\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           411M  1.1M  410M   1% /run\n/dev/vda1        20G  2.0G   18G  10% /\ntmpfs           2.1G     0  2.1G   0% /dev/shm\ntmpfs           5.3M     0  5.3M   0% /run/lock\n/dev/vda16      924M   65M  795M   8% /boot\n/dev/vda15      110M  6.4M  104M   6% /boot/efi\ntmpfs           411M   13k  411M   1% /run/user/1000\n/dev/vdb        106G   29k  100G   0% /mnt/nfs_share\n</code></pre> </li> <li> <p>Set the ownership and permissions to allow access (adjust based on requirements):</p> <p>Since we want all the client machines to access the shared directory, remove any restrictions in the directory permissions.</p> <pre><code>sudo chown -R nobody:nogroup /mnt/nfs_share/\n</code></pre> <p>You can also tweak the file permissions to your preference. Here we have given the read, write and execute privileges to all the contents inside the directory.</p> <pre><code>sudo chmod 777 /mnt/nfs_share/\n</code></pre> </li> <li> <p>Configure NFS Exports:</p> <p>Edit the <code>/etc/exports</code> file to define shared directories and permissions. Permissions for accessing the NFS server are defined in the <code>/etc/exports</code> file. So open the file using your favorite text editor i.e. nano editor:</p> <pre><code>sudo nano /etc/exports\n</code></pre> <p>Define Shared Directories and Permissions</p> <p>You can provide access to a single client, multiple clients, or specify an entire subnet.</p> <p>To grant access to a single client, use the syntax:</p> <pre><code>/mnt/nfs_share  Client_Internal_IP_1(rw,sync,no_subtree_check)\n</code></pre> <p>For multiple clients, specify each client on a separate file:</p> <pre><code>/mnt/nfs_share  Client_Internal_IP_1(rw,sync,no_subtree_check)\n/mnt/nfs_share  Client_Internal_IP_2(rw,sync,no_subtree_check)\n</code></pre> <p>Add a line like this to share the directory with read/write permissions for a subnet (e.g., 192.168.0.0/24):</p> <pre><code>/mnt/nfs_share 192.168.0.0/24(rw,sync,no_subtree_check)\n</code></pre> <p>Explanation:</p> <ul> <li>rw: Read and write access.</li> <li>sync: Changes are written to disk immediately.</li> <li>no_subtree_check: Avoid permission issues for subdirectories.</li> </ul> <p>Other Options for Directory Permissions for the NFS share directory</p> <p>You can configure the shared directories to be exported by adding them to the <code>/etc/exports</code> file. For example:</p> <pre><code>/srv     *(ro,sync,subtree_check)\n/home    *.hostname.com(rw,sync,no_subtree_check)\n/scratch *(rw,async,no_subtree_check,no_root_squash)\n</code></pre> <p>For more information read here.</p> </li> <li> <p>Apply Export Settings with the shared directories:</p> <pre><code>sudo exportfs -rav\n</code></pre> </li> <li> <p>Retart NFS Service:</p> <pre><code>sudo systemctl restart nfs-kernel-server\n</code></pre> </li> </ol>"},{"location":"other-tools/nfs/nfs-server-client-setup/#configure-the-nfs-client-on-the-client-vm","title":"Configure the NFS Client on the Client VM","text":"<ol> <li> <p>Update System Packages:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n</code></pre> </li> <li> <p>On the Client VM, install the required NFS package:</p> <pre><code>sudo apt install nfs-common -y\n</code></pre> </li> <li> <p>Create a Mount Point:</p> <p>Create a local directory where the shared NFS directory will be mounted:</p> <pre><code>sudo mkdir -p /mnt/nfs_clientshare\n</code></pre> </li> <li> <p>Testing connectivity between the Client and Server using the <code>showmount</code> command:</p> <pre><code>showmount --exports &lt;NFS_SERVER_INTERNAL_IP&gt;\n</code></pre> <p>For e.g.,</p> <pre><code>showmount --exports 192.168.0.73\n...\nExport list for 192.168.0.73:\n/mnt/nfs_share 192.168.0.0/24\n</code></pre> </li> <li> <p>Mount the Shared Directory:</p> <p>Use the <code>mount</code> command to connect to the NFS Server and mount the directory.</p> <pre><code>sudo mount -t nfs &lt;NFS_SERVER_INTERNAL_IP&gt;:/mnt/nfs_share /mnt/nfs_clientshare\n</code></pre> <p>For e.g.,</p> <pre><code>sudo mount -t nfs 192.168.0.73:/mnt/nfs_share /mnt/nfs_clientshare\n</code></pre> </li> <li> <p>Verify the Mount:</p> <p>Check if the directory is mounted successfully.</p> <p>Verify the mount path is set correctly:</p> <pre><code>df -H\n...\nFilesystem                  Size  Used Avail Use% Mounted on\ntmpfs                       411M  1.1M  410M   1% /run\n/dev/vda1                    20G  2.0G   18G  10% /\ntmpfs                       2.1G     0  2.1G   0% /dev/shm\ntmpfs                       5.3M     0  5.3M   0% /run/lock\n/dev/vda16                  924M   65M  795M   8% /boot\n/dev/vda15                  110M  6.4M  104M   6% /boot/efi\ntmpfs                       411M   13k  411M   1% /run/user/1000\n192.168.0.73:/mnt/nfs_share 106G     0  100G   0% /mnt/nfs_clientshare\n</code></pre> </li> </ol> <p>You should see the NFS share listed that is mounted and accessible.</p> <p>How to Unmount Shared Directory, if not needed!</p> <p>When you're finished with a mount, we can unmount it with the <code>umount</code> command.</p> <pre><code>sudo umount /mnt/nfs_clientshare\n</code></pre>"},{"location":"other-tools/nfs/nfs-server-client-setup/#make-the-mount-persistent-on-the-client-vm","title":"Make the Mount Persistent on the Client VM","text":"<p>An alternative method to mount an NFS share from another machine is by adding a line to the <code>/etc/fstab</code> file. This line should specify the floating IP of the NFS Server, the directory being exported on the NFS Server, and the directory on the local Client VM where the NFS share should be mounted. This setup ensures the NFS share is automatically mounted at boot time, even after the Client VM is rebooted.</p> <p>The general syntax for the line in <code>/etc/fstab</code> file is as follows:</p> <pre><code>example.hostname.com:/srv /opt/example nfs rsize=8192,wsize=8192,timeo=14,intr\n</code></pre> <ol> <li> <p>Edit <code>/etc/fstab</code> Open the file:</p> <pre><code>sudo nano /etc/fstab\n</code></pre> </li> <li> <p>Add an entry like this:</p> <pre><code>&lt;NFS_SERVER_INTERNAL_IP&gt;:/mnt/nfs_share /mnt/nfs_clientshare nfs defaults 0 0\n</code></pre> <p>For e.g.,</p> <pre><code>192.168.0.73:/mnt/nfs_share /mnt/nfs_clientshare nfs defaults 0 0\n</code></pre> </li> <li> <p>Test the Configuration Unmount and remount all filesystems listed in <code>/etc/fstab</code>:</p> <pre><code>sudo umount /mnt/nfs_clientshare\nsudo mount -a\n</code></pre> </li> <li> <p>Verify the Mount:</p> <p>Check if the directory is mounted successfully.</p> <pre><code>df -H\n</code></pre> </li> </ol>"},{"location":"other-tools/nfs/nfs-server-client-setup/#test-the-setup","title":"Test the Setup","text":"<ul> <li> <p>On the NFS Server, write a test file:</p> <pre><code>echo \"Hello from NFS Server\" | sudo tee /mnt/nfs_share/test.txt\n</code></pre> </li> <li> <p>On the NFS Client, verify the file is accessible:</p> <pre><code>cat /mnt/nfs_clientshare/test.txt\n</code></pre> </li> </ul>"},{"location":"other-tools/postgres/postgres-cluster-setup/","title":"PostgresSQL Cluster","text":""},{"location":"other-tools/postgres/postgres-cluster-setup/#create-a-postgres-cluster","title":"Create a Postgres Cluster","text":"<p>In the NERC OpenShift environment, the Crunchy Postgres for Kubernetes Operator is already installed, configured, and ready for general users to provision and manage PostgreSQL clusters.</p> <p></p> <p>Built on Postgres Operator from Crunchy Data (PGO), the Postgres Operator from Crunchy Data, Crunchy Postgres for Kubernetes gives you a declarative Postgres solution that automatically manages your PostgreSQL clusters.</p>"},{"location":"other-tools/postgres/postgres-cluster-setup/#steps","title":"Steps","text":""},{"location":"other-tools/postgres/postgres-cluster-setup/#using-the-cli-oc-command-on-your-local-terminal","title":"Using the CLI (oc command) on your local Terminal","text":"<p>Make sure you have the <code>oc</code> CLI tool installed and configured on your local machine following these steps.</p> <p>Information</p> <p>Some users may have access to multiple projects. Run the following command to switch to a specific project space: <code>oc project &lt;your-project-namespace&gt;</code>.</p>"},{"location":"other-tools/postgres/postgres-cluster-setup/#set-up-a-postgresql-cluster","title":"Set up a PostgreSQL cluster","text":"<p>Run the following command in your terminal to set up a PostgreSQL cluster named <code>hippo-pg</code>:</p> <p>Very Important: Changing the Server Name</p> <p>You can change the server name, e.g., <code>hippo-pg</code>, but make sure that all references to this name are updated in the subsequent steps.</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: postgres-operator.crunchydata.com/v1beta1\nkind: PostgresCluster\nmetadata:\n  name: hippo-pg\n#   finalizers:\n#     - postgres-operator.crunchydata.com/finalizer\n  labels:\n    app: hippo-pg\nspec:\n#   port: 5432\n  users:\n    - databases:\n        - postgres\n      name: postgres\n      options: SUPERUSER\n      password:\n        type: AlphaNumeric\n    - databases:\n        - postgres\n      name: admin\n      options: SUPERUSER\n      password:\n        type: AlphaNumeric\n#   proxy:\n#     pgBouncer:\n#       port: 5432\n#       replicas: 3\n#       service:\n#         type: ClusterIP\n#   monitoring:\n#     pgmonitor:\n#       exporter:\n#         image: 'registry.developers.crunchydata.com/crunchydata/crunchy-postgres-exporter:ubi8-5.6.1-0'\n#         resources:\n#           limits:\n#             cpu: 100m\n#             memory: 128Mi\n#           requests:\n#             cpu: 50m\n#             memory: 64Mi\n#   backups:\n#     pgbackrest:\n#       global:\n#         repo1-retention-full: '2'\n#         repo1-retention-diff: '3'\n#         archive-push-queue-max: 4GiB\n#         repo1-retention-archive-type: 'diff'\n#         repo1-retention-archive: '2'\n#       repos:\n#         - name: repo1\n#           schedules:\n#               full: \"0 2 * * 0\"           # Full backup every Sunday at 2 AM\n#               differential: \"0 2 * * 1-6\" # Differential backup Mon-Sat at 2 AM\n#               incremental: \"0 */8 * * *\"  # Incremental backup every 8 hours\n#           volume:\n#             volumeClaimSpec:\n#               accessModes:\n#                 - ReadWriteOnce\n#               resources:\n#                 requests:\n#                   storage: 100Gi\n  service:\n    type: ClusterIP\n  patroni:\n    dynamicConfiguration:\n      postgresql:\n        parameters:\n          max_connections: 100\n          shared_buffers: 256MB\n        pg_hba:\n          - \"host all all 0.0.0.0/0 md5\"\n          - \"host all all ::/0 md5\"\n          - \"local all all trust\"\n      synchronous_mode: false\n    leaderLeaseDurationSeconds: 30\n    port: 8008\n    switchover:\n      enabled: true\n      type: Switchover\n    syncPeriodSeconds: 10\n  instances:\n    - name: instance1\n      replicas: 1\n      dataVolumeClaimSpec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 20Gi   # Change this with your own value\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 1\n            memory: 1Gi\n      walVolumeClaimSpec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n            cpu: 200m\n            memory: 200Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n      resources:\n        requests:\n          cpu: 200m\n          memory: 200Mi\n        limits:\n          cpu: 1\n          memory: 1Gi\n  postgresVersion: 16\n  openshift: true\nEOF\n</code></pre> <p>PostgreSQL Cluster Configuration</p> <p>In this setup, we are using the basic settings to create a PostgreSQL cluster. However, you can customize your cluster to create a production-ready environment with backups, scaling, high availability, monitoring, and seamless upgrades.</p> <p>NOTE: Doing so will require additional resource quotas to support the enhanced features.</p> <p>For more details, refer to the Crunchy Data documentation: Creating a PostgreSQL Cluster \u2013 Crunchy Data.</p>"},{"location":"other-tools/postgres/postgres-cluster-setup/#background-pvc-services-and-secrets","title":"Background: PVC, Services and Secrets","text":"<p>Make sure the <code>pvc</code> are in a bound state. Run the following command:</p> <pre><code>oc get pvc --selector=postgres-operator.crunchydata.com/cluster=hippo-pg\n</code></pre> <p>The output should look similar to:</p> <pre><code>hippo-pg-instance1-wksp-pgdata                 Bound     pvc-ec32bb28-fa66-45ec-bb3b-5dd560200d2f   20Gi       RWO            ocs-external-storagecluster-ceph-rbd   &lt;unset&gt;                 38m\nhippo-pg-instance1-wksp-pgwal                  Bound     pvc-9c26989e-ad67-4de5-b39f-e82a233e85f3   10Gi       RWO            ocs-external-storagecluster-ceph-rbd   &lt;unset&gt;                 38m\n</code></pre> <p>When your PostgreSQL cluster is initialized using the script above to set up the cluster named <code>hippo-pg</code>, PGO creates a set of Kubernetes Services that provide stable endpoints for connecting to your PostgreSQL databases. These endpoints ensure a consistent and reliable way for your application to access and interact with your data. To see which services are available, you can run the following command:</p> <pre><code>oc get svc --selector=postgres-operator.crunchydata.com/cluster=hippo-pg\n</code></pre> <p>The output should look similar to:</p> <pre><code>NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nhippo-pg-ha          ClusterIP   172.30.243.135   &lt;none&gt;        5432/TCP   138m\nhippo-pg-ha-config   ClusterIP   None             &lt;none&gt;        &lt;none&gt;     138m\nhippo-pg-pods        ClusterIP   None             &lt;none&gt;        &lt;none&gt;     138m\nhippo-pg-primary     ClusterIP   None             &lt;none&gt;        5432/TCP   138m\nhippo-pg-replicas    ClusterIP   172.30.250.15    &lt;none&gt;        5432/TCP   138m\n</code></pre> <p>You don't need to worry about most of these Services, as they are primarily used to manage the overall health of your PostgreSQL cluster. For connecting your application to the database, the Service you should use is <code>hippo-pg-primary</code>.</p> <p>You can query the pod using:</p> <pre><code>oc get pod -o name -l postgres-operator.crunchydata.com/cluster=hippo-pg,postgres-operator.crunchydata.com/role=master\n</code></pre> <p>The PGO will also bootstrap a database and create PostgreSQL users (e.g. <code>admin</code> and <code>postgres</code>) as defined in <code>spec:users</code>. These accounts can be used by your application to access the database i.e. <code>postgres</code>.</p> <p>This information is stored in a Secret named with the pattern <code>&lt;clusterName&gt;-pguser-&lt;userName&gt;</code>. For our <code>hippo-pg</code> cluster, the Secrets are called <code>hippo-pg-pguser-admin</code> and <code>hippo-pg-pguser-postgres</code>, corresponding to the <code>admin</code> and <code>postgres</code> users.</p> <pre><code>oc get secrets --selector=postgres-operator.crunchydata.com/cluster=hippo-pg\n</code></pre> <p>The output should look similar to:</p> <pre><code>NAME                            TYPE     DATA   AGE\nhippo-pg-cluster-cert           Opaque   3      46m\nhippo-pg-instance1-wksp-certs   Opaque   4      46m\nhippo-pg-pguser-admin           Opaque   8      46m\nhippo-pg-pguser-postgres        Opaque   8      46m\nhippo-pg-replication-cert       Opaque   3      46m\n</code></pre> <p>These Secrets contain the information needed to connect your application to the PostgreSQL database:</p> <ul> <li> <p>user: The name of the user account.</p> </li> <li> <p>password: The password for the user account.</p> </li> <li> <p>dbname: The default database that the user has access to.</p> </li> <li> <p>host: The host name of the database, referencing the Service of the     primary PostgreSQL Cluster instance.</p> </li> <li> <p>port: The port on which the database is listening.</p> </li> <li> <p>uri: A PostgreSQL connection URI     containing all information for logging into the database.</p> </li> <li> <p>jdbc-uri: A PostgreSQL JDBC connection URI     for connecting via a JDBC driver.</p> </li> </ul>"},{"location":"other-tools/postgres/postgres-cluster-setup/#connect-via-psql-in-the-local-terminal","title":"Connect via <code>psql</code> in the local Terminal","text":"<p>If you are on the same network as your PostgreSQL cluster, you can connect directly to it using the following command:</p> <pre><code>psql $(oc get secrets hippo-pg-pguser-postgres -o go-template='{{.data.uri | base64decode}}')\n</code></pre>"},{"location":"other-tools/postgres/postgres-cluster-setup/#connect-using-a-port-forward","title":"Connect Using a Port-Forward","text":"<p>To access PostgreSQL Cluster without public exposure, you can use port forwarding to the cluster's primary pod - on port <code>5432</code>:</p> <pre><code>PG_CLUSTER_PRIMARY_POD=$(oc get pod -o name -l postgres-operator.crunchydata.com/cluster=hippo-pg,postgres-operator.crunchydata.com/role=master)\noc port-forward \"${PG_CLUSTER_PRIMARY_POD}\" 5432:5432\n</code></pre> <p>You should see output similar to:</p> <pre><code>Forwarding from 127.0.0.1:5432 -&gt; 5432\nForwarding from [::1]:5432 -&gt; 5432\n</code></pre> <p>Run the following commands to establish a connection to the PostgreSQL cluster:</p> <pre><code>PG_CLUSTER_USER_SECRET_NAME=hippo-pg-pguser-postgres\n\nPGUSER=$(oc get secrets \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.user | base64decode}}') \\\nPGPASSWORD=$(oc get secrets \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.password | base64decode}}') \\\nPGDATABASE=$(oc get secrets \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.dbname | base64decode}}') \\\npsql -h localhost -p 5432\n</code></pre>"},{"location":"other-tools/postgres/postgres-cluster-setup/#connecting-with-pgadmin","title":"Connecting With <code>pgAdmin</code>","text":"<p>Crunchy Postgres for Kubernetes also provides a <code>pgAdmin</code> image for users who prefer working with a graphical user interface.</p> <p>Let's create the <code>pgAdmin</code> database browser.</p> <ul> <li> <p>Create a secret for pgAdmin password:</p> <pre><code>export PGADMIN_PASSWORD='&lt;your-pgadmin-password&gt;'\n</code></pre> </li> <li> <p>Create a secret for pgAdmin password:</p> <pre><code>oc create secret generic pgadmin-password-secret \\\n--from-literal=pgadmin-admin-password=$PGADMIN_PASSWORD\n</code></pre> </li> <li> <p>Run the below command to create pgAdmin instance</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\napiVersion: postgres-operator.crunchydata.com/v1beta1\nkind: PGAdmin\nmetadata:\n  name: hippo-pgadmin\nspec:\n  dataVolumeClaimSpec:\n    accessModes:\n      - ReadWriteOnce\n    resources:\n      requests:\n        storage: 1Gi\n  serverGroups:\n    - name: Crunchy Postgres for Kubernetes\n      postgresClusterSelector: {}\n  serviceName: hippo-pgadmin\n  users:\n    - passwordRef:\n        key: pgadmin-admin-password\n        name: pgadmin-password-secret\n      role: Administrator\n      username: admin@example.com   # Change this with your own username\nEOF\n</code></pre> <p>To access pgAdmin without public exposure</p> <p>To access pgAdmin without creating a public Route (as described in the next step), you can use port forwarding to the pgAdmin service i.e. <code>hippo-pgadmin</code> - on port <code>5050</code>:</p> <pre><code>oc port-forward svc/hippo-pgadmin 5050:5050\n</code></pre> <p>You should see output similar to:</p> <pre><code>Forwarding from 127.0.0.1:5050 -&gt; 5050\nForwarding from [::1]:5050 -&gt; 5050\n</code></pre> <p>Once the port-forward is active, open your browser and navigate to:</p> <pre><code>http://localhost:5050\n</code></pre> <p>Use the following credentials to log in to the opened <code>pgAdmin</code> database explorer:</p> <p>username: admin@example.com</p> <p>password: $PGADMIN_PASSWORD</p> </li> <li> <p>To access pgAdmin publicly, create a route for the <code>pgAdmin</code> database explorer:</p> <pre><code>cat &lt;&lt;EOF | oc apply -f -\nkind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: route-pgadmin\nspec:\n  to:\n    kind: Service\n    name: hippo-pgadmin\n    weight: 100\n  port:\n    targetPort: pgadmin-port\n  tls:\n    termination: edge\n    insecureEdgeTerminationPolicy: Redirect\n  wildcardPolicy: None\nEOF\n</code></pre> </li> <li> <p>Get the <code>pgAdmin</code> URL to access the database explorer:</p> <pre><code>oc get routes route-pgadmin -o jsonpath='https://{.spec.host}'\n</code></pre> <p>Browse to the retrieved <code>pgAdmin</code> URL in your web browser to access the database explorer.</p> <p></p> <p>Use the following credentials to log in to the <code>pgAdmin</code> database explorer:</p> <p>username: admin@example.com</p> <p>password: $PGADMIN_PASSWORD</p> </li> </ul> <p>Once logged in, you can access the pgAdmin console as shown below:</p> <p></p> <p>Click the Crunchy Postgres for Kubernetes server group to view the existing server, e.g., <code>hippo-pg</code>.  </p> <p>To configure server connections, right-click on <code>hippo-pg</code> and select Properties to open the Server dialog.</p> <p>Use the Server dialog to define a connection to the server. Click the Connection tab to continue.</p> <p>In the Connection tab, configure the connection using the following fields:</p> <p></p> <ul> <li> <p>Maintenance database: Specify the name of the initial database to which     the client will connect.</p> </li> <li> <p>Username: Specify the role name used to authenticate with the server.</p> </li> <li> <p>Password: Enter the password used for authentication.</p> </li> <li> <p>Save password?: Check this box to save the password for future use. Use     Clear Saved Password to remove it if needed.</p> </li> </ul> <p>To retrieve these information, run the following commands:</p> <pre><code>PG_CLUSTER_USER_SECRET_NAME=hippo-pg-pguser-postgres\n\noc get secrets \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.dbname | base64decode}}'\noc get secrets \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.user | base64decode}}'\noc get secrets \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.password | base64decode}}'\n</code></pre> <p>These commands extract the database name, username, and password from the Kubernetes Secret associated with the specified user in your PostgreSQL cluster.</p> <p>Admin User Secret</p> <p>For the <code>admin</code> user, set the Secret name as follows:</p> <pre><code>PG_CLUSTER_USER_SECRET_NAME=hippo-pg-pguser-admin\n</code></pre> <p>To retrieve this information for the <code>admin</code> user, run the following commands:</p> <pre><code>oc get secret \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.dbname | base64decode}}'\noc get secret \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.user | base64decode}}'\noc get secret \"${PG_CLUSTER_USER_SECRET_NAME}\" -o go-template='{{.data.password | base64decode}}'\n</code></pre> <p>Once the database connection is successful, you will see the <code>pgAdmin</code> console as shown below:</p> <p></p>"},{"location":"other-tools/postgres/postgres-cluster-setup/#deleting-the-pgadmin","title":"Deleting the <code>pgAdmin</code>","text":"<p>If <code>pgAdmin</code> was created as a PGAdmin custom resource, you can delete it by running:</p> <pre><code>oc delete pgadmin hippo-pgadmin\n</code></pre> <p>If <code>pgAdmin</code> was exposed with a Route, you can also delete the route:</p> <pre><code>oc delete route route-pgadmin\n</code></pre>"},{"location":"other-tools/postgres/postgres-cluster-setup/#delete-the-postgres-cluster","title":"Delete the Postgres Cluster","text":"<p>There comes a time when it is necessary to delete your Postgres Cluster. If you have been following along with the example, you can delete your cluster by simply running:</p> <pre><code>oc delete postgrescluster hippo-pg\n</code></pre> <p>This removes the PostgresCluster resource and all associated components managed by the PGO.</p>"}]}